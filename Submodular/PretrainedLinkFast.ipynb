{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bffbb0c",
   "metadata": {},
   "source": [
    "# Test Dataset to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38a1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd880dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c466d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b785488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import get_data, generate_synthetic\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import copy\n",
    "import ipynb.fs.full.utils.MoonGraph as MoonGraph\n",
    "import logging\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch_geometric.utils import add_self_loops\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ebdc9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()    \n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--balance', type=bool, default=True)\n",
    "    parser.add_argument('--num_worker', type=int, default=0)\n",
    "    parser.add_argument('--dataset', type=str, default=\"karate\", choices=available_datasets)\n",
    "    parser.add_argument('--epochs', type=int, default=20)\n",
    "    parser.add_argument('--link_batch_size', type=int, default=4096*8) #8192\n",
    "    parser.add_argument('--link_num_steps', type=int, default=100)\n",
    "    parser.add_argument('--num_neurons', type=int, default=32)\n",
    "    parser.add_argument('--f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ae56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e61e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0d46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from multiprocessing.pool import ThreadPool, Pool\n",
    "import os.path as osp\n",
    "from typing import Optional, List, Dict\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.typing import EdgeType, InputNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54170485",
   "metadata": {},
   "source": [
    "# Link Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ebe5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1002ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNNlayer=GCNConv\n",
    "\n",
    "class LinkModel(nn.Module):\n",
    "    def __init__(self, input_rep, num_neurons=64):\n",
    "        super(LinkModel, self).__init__()\n",
    "        \n",
    "        self.MLP1 = nn.Linear(input_rep,num_neurons)        \n",
    "        #self.MLP2 = nn.Linear(num_neurons,num_neurons)\n",
    "        self.MLP3 = nn.Linear(num_neurons*2,1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "                            \n",
    "        x = self.MLP1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        y = self.MLP1(y)\n",
    "        y = y.relu()\n",
    "        y = F.dropout(y, p=0.2, training=self.training)\n",
    "        \n",
    "        z=torch.cat((x-y,x*y),dim=1)  #         xy=x+y        \n",
    "#         z = self.MLP2(z)\n",
    "#         z = z.relu()\n",
    "#         z = F.dropout(z, p=0.5, training=self.training)\n",
    "\n",
    "        z = self.MLP3(z)\n",
    "#         z = torch.sigmoid(z)\n",
    "#         z = z.relu()\n",
    "#         z = F.log_softmax(z,dim=1)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5b0e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinkModel(data.num_features, num_neurons=64).to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13a430",
   "metadata": {},
   "source": [
    "## Link Prediction Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c9d609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkSampler(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, data, input_nodes: InputNodes = None, batch_size: int=1, num_steps: int = 1, \n",
    "                 save_dir: Optional[str] = None, recompute = True,log: bool = True, balance=False, **kwargs):\n",
    "\n",
    "        if 'collate_fn' in kwargs:\n",
    "            del kwargs['collate_fn']\n",
    "            \n",
    "        self.num_steps = num_steps\n",
    "        self.__batch_size__ = batch_size        \n",
    "        self.save_dir = save_dir\n",
    "        self.recompute = recompute\n",
    "        self.log = log\n",
    "        self.balance = balance\n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        \n",
    "        self.input_nodeidx = torch.nonzero(input_nodes).flatten()\n",
    "        \n",
    "        #print(self.input_nodeidx)\n",
    "        \n",
    "        if balance:\n",
    "            #get an estimate of ratio\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            x = self.input_nodeidx[indices]\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            y = self.input_nodeidx[indices]\n",
    "            label = (self.data.y[x] == self.data.y[y]).type(torch.float)\n",
    "            self.ratio = torch.sum(label).item()/self.__batch_size__\n",
    "            \n",
    "            #print(self.ratio)\n",
    "                        \n",
    "            #######\n",
    "            self.num_class = torch.max(data.y)+1        \n",
    "            self.clusters = [[] for i in range(self.num_class)]\n",
    "        \n",
    "            for i in self.input_nodeidx:\n",
    "                self.clusters[data.y[i]].append(i.item())\n",
    "                \n",
    "            for i in range(self.num_class):\n",
    "                self.clusters[i] = torch.LongTensor(self.clusters[i])\n",
    "            \n",
    "            #print(self.clusters)\n",
    "        \n",
    "\n",
    "        super().__init__(self, batch_size=1, collate_fn=self.__collate__,\n",
    "                         **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def __filename__(self):\n",
    "        return f'{self.__class__.__name__.lower()}_{self.sample_coverage}.pt'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.balance and self.ratio<=0.40:\n",
    "            \n",
    "            per_class = math.ceil(self.__batch_size__*(0.5-self.ratio)/self.num_class)\n",
    "            \n",
    "            Xs = torch.LongTensor([])\n",
    "            Ys = torch.LongTensor([])\n",
    "                        \n",
    "            for i in range(self.num_class):\n",
    "                \n",
    "                if len(self.clusters[i])==0:\n",
    "                    continue\n",
    "                \n",
    "                indices = torch.randint(len(self.clusters[i]), (per_class, ))\n",
    "                x = self.clusters[i][indices]\n",
    "                indices = torch.randint(len(self.clusters[i]), (per_class, ))\n",
    "                y = self.clusters[i][indices]\n",
    "                Xs = torch.cat((Xs,x))\n",
    "                Ys = torch.cat((Ys,y))                  \n",
    "            \n",
    "            remaining = per_class*self.num_class\n",
    "            remaining = self.__batch_size__ - remaining\n",
    "            \n",
    "            indices = torch.randint(len(self.input_nodeidx), (remaining, ))\n",
    "            x = self.input_nodeidx[indices]\n",
    "            indices = torch.randint(len(self.input_nodeidx), (remaining, ))\n",
    "            y = self.input_nodeidx[indices]\n",
    "            \n",
    "            x = torch.cat((Xs,x))\n",
    "            y = torch.cat((Ys,y))\n",
    "                    \n",
    "            #print(x.shape)\n",
    "            #print(y.shape)\n",
    "            \n",
    "        else:\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            x = self.input_nodeidx[indices]\n",
    "            indices = torch.randint(len(self.input_nodeidx), (self.__batch_size__, ))\n",
    "            y = self.input_nodeidx[indices]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __collate__(self, data_list):\n",
    "        assert len(data_list) == 1\n",
    "        \n",
    "        x, y = data_list[0]\n",
    "        \n",
    "        label = (self.data.y[x] == self.data.y[y]).type(torch.float)\n",
    "        b_data = self.data.__class__()\n",
    "        b_data.x = x\n",
    "        b_data.y = y\n",
    "        b_data.label = label\n",
    "        b_data.x_feat = self.data.x[x]\n",
    "        b_data.y_feat = self.data.x[y]\n",
    "                \n",
    "        return b_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef05b10",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ca9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data, dataset = get_data('Reddit', log=False)\n",
    "\n",
    "# from torch_geometric.data import Data\n",
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# train_mask = torch.zeros(len(y)).type(torch.bool)\n",
    "# train_mask[[0,1,2]]=True\n",
    "# data = Data(x=x, y=y, edge_index = edge_index, train_mask = train_mask, val_mask = train_mask, test_mask = train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4649fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4096\n",
    "# train_sampler  = LinkSampler(data, input_nodes = data.train_mask, batch_size = batch_size, num_steps = 10, save_dir = None,\n",
    "#                              recompute = True,log = True, balance=True)\n",
    "# for batch in train_sampler:\n",
    "#     print(batch)\n",
    "#     print(sum(batch.label)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "089a539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinkModel(data.num_features, num_neurons=64).to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026939c",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e809c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, sampler, log = True):\n",
    "\n",
    "    y_pred=np.array([])\n",
    "    y_true=np.array([])\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=args.link_batch_size*args.link_num_steps)\n",
    "        pbar.set_description(f'Predicting: ')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b_data in sampler:\n",
    "            b_data = b_data.to(device)\n",
    "            out = model(b_data.x_feat, b_data.y_feat).view(-1)         \n",
    "            \n",
    "            pred = torch.zeros_like(out)\n",
    "            pred[out >= 0.5] = 1\n",
    "            \n",
    "            pred = pred.cpu().numpy()\n",
    "            test_target=b_data.label.cpu().numpy()\n",
    "        \n",
    "#             print(out)\n",
    "#             print(pred)\n",
    "#             print(test_target)\n",
    "        \n",
    "            y_pred = np.append(y_pred,pred)\n",
    "            y_true = np.append(y_true,test_target)\n",
    "            \n",
    "            if log:\n",
    "                pbar.update(args.link_batch_size)\n",
    "        if log:\n",
    "            pbar.close()\n",
    "    \n",
    "    micro=f1_score(y_true, y_pred, average='micro')\n",
    "    weighted=f1_score(y_true, y_pred, average='weighted')\n",
    "    acc=accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, micro, weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08c7daed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, log = True, epochs=1, worker = 0):    \n",
    "        \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()    \n",
    "#     criterion = nn.BCELoss()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     criterion = nn.NLLLoss()\n",
    "            \n",
    "    train_sampler  = LinkSampler(data, input_nodes = data.train_mask, batch_size = args.link_batch_size, num_steps = args.link_num_steps,\n",
    "                                 save_dir = None, recompute = False,log = log, balance=args.balance, num_workers = worker)\n",
    "    \n",
    "    val_sampler  = LinkSampler(data, input_nodes = data.val_mask, batch_size = args.link_batch_size, num_steps = args.link_num_steps,\n",
    "                                 save_dir = None, recompute = False,log = log, balance=args.balance, num_workers = worker)\n",
    "    \n",
    "    test_sampler  = LinkSampler(data, input_nodes = data.test_mask, batch_size = args.link_batch_size, num_steps = args.link_num_steps,\n",
    "                                 save_dir = None, recompute = False,log = log, balance=args.balance, num_workers = worker)\n",
    "    \n",
    "    show_pbar = log\n",
    "#     if log and data.num_nodes>100000:\n",
    "#         show_pbar = True\n",
    "    \n",
    "    #worker = 0     \n",
    "    train_losses=[]\n",
    "    \n",
    "    for epoch in range(1,epochs+1):        \n",
    "        total_loss = total_examples = 0\n",
    "        y_pred=[]\n",
    "        y_true=[]\n",
    "        \n",
    "        if show_pbar:\n",
    "            pbar = tqdm(total=args.link_batch_size*args.link_num_steps)\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        for b_data in train_sampler:            \n",
    "            b_data = b_data.to(device)\n",
    "            \n",
    "#             print(sum(b_data.label))\n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            out = model(b_data.x_feat,b_data.y_feat)\n",
    "            loss = criterion(out, b_data.label.view(-1,1))            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_examples += args.link_batch_size\n",
    "            \n",
    "            if show_pbar:\n",
    "                pbar.update(args.link_batch_size)\n",
    "        if show_pbar:\n",
    "            pbar.close()\n",
    "        \n",
    "        loss=total_loss / args.link_num_steps\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if log:            \n",
    "            print(f'Epoch {epoch:03d} Loss {loss:.4f}', end=' ')            \n",
    "            tr_a, tr_b, tr_c = predict(model, data, train_sampler, log = log)\n",
    "            print(f'\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')            \n",
    "    \n",
    "    if log:\n",
    "#         tr_a, tr_b, tr_c = predict(model, data, train_sampler, log = log) \n",
    "#         print(f'Train\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')\n",
    "        tr_a, tr_b, tr_c = predict(model, data, val_sampler, log = log) \n",
    "        print(f'Val\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')\n",
    "        tr_a, tr_b, tr_c = predict(model, data, test_sampler, log = log) \n",
    "        print(f'Test\\t{tr_a:.4f},\\t{tr_b:.4f},\\t{tr_c:.4f}')\n",
    "                \n",
    "    return model\n",
    "\n",
    "# train(model, data, log = True, epochs=100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23a4957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link(data, selfloop = True, log = True, worker = 0):\n",
    "    \n",
    "    args.balance = False\n",
    "    \n",
    "    if data.num_nodes<10000:\n",
    "        args.epochs = 20\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = min(4096, data.num_nodes*data.num_nodes)\n",
    "        args.link_num_steps = 200\n",
    "\n",
    "    elif data.num_nodes<100000:\n",
    "        args.epochs = 10\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096*2\n",
    "        args.link_num_steps = 200\n",
    "    else:\n",
    "        args.epochs = 5\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096*8\n",
    "        args.link_num_steps = 200\n",
    "        \n",
    "    #args.epochs = 1\n",
    "    \n",
    "        \n",
    "    model = LinkModel(data.num_features, num_neurons=args.num_neurons).to(device)\n",
    "    \n",
    "    if log:\n",
    "        print(model)\n",
    "    \n",
    "    train(model, data, log = log, epochs=args.epochs, worker = worker)        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1caad",
   "metadata": {},
   "source": [
    "## Edge Weight Computation Edge Wise (Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91971043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_weight(data, selfloop = True, log = True, worker = 0):\n",
    "    \n",
    "    #if data.num_nodes<10000:\n",
    "    worker = 0 ##worker 0 found to be fastest\n",
    "    \n",
    "    link_model = train_link(data, selfloop = selfloop, log = log, worker = worker)\n",
    "    \n",
    "    w = torch.Tensor([]).type(torch.float).to(device)\n",
    "    \n",
    "    indices = torch.arange(0, data.edge_index.shape[1])\n",
    "    batches = torch.split(indices, args.link_batch_size)\n",
    "    \n",
    "    link_model.eval()    \n",
    "    with torch.no_grad():    \n",
    "        for batch in batches:\n",
    "            \n",
    "            #print(batch)\n",
    "            \n",
    "            idx = data.edge_index[:,batch]\n",
    "            ew = link_model(data.x[idx[0]].to(device),data.x[idx[1]].to(device))\n",
    "            ew = ew.view(-1)\n",
    "            ew = torch.clamp(ew, min=1e-3, max=1.0)\n",
    "            w = torch.cat((w,ew))\n",
    "            \n",
    "    return w.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab51012",
   "metadata": {},
   "source": [
    "## Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95a70dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('Reddit', log=False, h_score=True, split_no = 0)\n",
    "# args.epochs = 1\n",
    "# args.num_neurons = 32\n",
    "# args.link_batch_size = 4096*8\n",
    "# args.link_num_steps = 100\n",
    "# args.balance = True\n",
    "\n",
    "# start = time.time()\n",
    "# link_model = train_link(data, selfloop = True, log = True)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfefd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = get_link_weight(data, selfloop = True, log = False, worker=0)\n",
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb89da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4230836",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Weight assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d20ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkNN():\n",
    "    \n",
    "    def __init__(self, data, value='min', log=True, worker=0, \n",
    "                 lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data        \n",
    "        self.value = value\n",
    "        self.log = log\n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "        \n",
    "        self.sign = 1\n",
    "        \n",
    "        if value=='min':\n",
    "            self.sign = -1\n",
    "            \n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        self.weight = get_link_weight(data, selfloop = True, log = log, worker=worker)\n",
    "        \n",
    "    def node_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()           \n",
    "        \n",
    "        target_class_sim = self.weight[edge_index]\n",
    "        ind = np.argsort(self.sign*target_class_sim) #-1*desending, normal will be ascending\n",
    "        \n",
    "#         print(u, row, col, edge_index)\n",
    "#         print(target_class_sim)\n",
    "#         print(ind)\n",
    "         \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5 \n",
    "        \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))        \n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "#         print(len(col),l1, l2, l3)\n",
    "        \n",
    "#         S_G = np.ones(l1, dtype=float)*1.0\n",
    "#         S_G = np.append(S_G, np.ones(l2, dtype=float)*0.5)\n",
    "#         if(l3>0):\n",
    "#             S_G = np.append(S_G, np.ones(l3, dtype=float)*0.1)\n",
    "\n",
    "        S_G = np.ones(l1, dtype=float)*self.w1\n",
    "        S_G = np.append(S_G, np.ones(l2, dtype=float)*self.w2)\n",
    "        \n",
    "        if(l3>0):\n",
    "            S_G = np.append(S_G, np.ones(l3, dtype=float)*self.w3)\n",
    "        \n",
    "        S_G = S_G.tolist()\n",
    "        \n",
    "#         S_G = list(range(1,len(col)+1))\n",
    "        S_edge = edge_index[ind].tolist()\n",
    "        \n",
    "        return S_G, S_edge\n",
    "\n",
    "    def get_nn_weight(self):\n",
    "        \n",
    "        if self.log:        \n",
    "            pbar = tqdm(total=self.N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "\n",
    "        for u in range(self.N):            \n",
    "            weight, e_index = self.node_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:        \n",
    "                pbar.update(1)\n",
    "        if self.log:        \n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.node_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_nn_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "\n",
    "        \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):\n",
    "        if self.data.num_nodes<10000:\n",
    "            weight = self.get_nn_weight()    \n",
    "        else:\n",
    "            weight = self.get_nn_weight_multiproces()\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c7e7c",
   "metadata": {},
   "source": [
    "## Submodular Weight assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9caee41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkSubOld():\n",
    "    \n",
    "    def __init__(self, data, value='max', selfloop = False, log = True, worker=0, lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        self.selfloop = selfloop\n",
    "        \n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "    \n",
    "        #self.X = data.x.to(device)\n",
    "        self.X = self.data.x\n",
    "        self.on_device=True        \n",
    "        \n",
    "        self.model = train_link(data, selfloop = selfloop, log= log, worker=worker)\n",
    "        self.model.eval()        \n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        if self.log:\n",
    "            print(\"value: \", value)\n",
    "        \n",
    "        self.value = value\n",
    "        self.sign = -1\n",
    "        \n",
    "        if self.value == 'max':\n",
    "            self.sign = 1 ##-1 select the nearest ones, 1 for the farthest        \n",
    "            \n",
    "        elif self.value == 'min':\n",
    "            self.sign = -1\n",
    "        else:\n",
    "            raise 'Not implemented error'\n",
    "    \n",
    "    def pairwise_link(self, x):  \n",
    "                \n",
    "        n, f = x.shape\n",
    "        \n",
    "        x_col1 = x.repeat_interleave(n, dim=0)\n",
    "        x_col2 = x.repeat(n,1)\n",
    "        # print(x_col1, x_col2)\n",
    "        \n",
    "        if self.on_device:\n",
    "            x_col1 = x_col1.to(device)\n",
    "            x_col2 = x_col2.to(device)            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(x_col1, x_col2).detach().cpu()\n",
    "            output = torch.clamp(output, min=1e-3, max=1.0)\n",
    "        #print(output.shape)\n",
    "#         output = output.softmax(dim=1)\n",
    "#         second_column = output[:,1].cpu()        \n",
    "        #print(second_column)\n",
    "        \n",
    "        similarity_matrix = output.view(n,n)\n",
    "        \n",
    "#         print(similarity_matrix)\n",
    "        \n",
    "        return similarity_matrix\n",
    "        \n",
    "    def lazy_greedy_weight(self,u):\n",
    "        \n",
    "        row, col, edge_index = self.adj[u,:].coo()\n",
    "        \n",
    "        if len(col)==0:\n",
    "            return [],[]\n",
    "        \n",
    "        \n",
    "        vertices = [u]+col.tolist()\n",
    "        \n",
    "        v2i={i:j for i,j in zip(vertices, range(len(vertices)))}\n",
    "        i2v={value:key for key, value in v2i.items()}\n",
    "        \n",
    "        kernel_dist = self.pairwise_link(self.X[vertices])\n",
    "        \n",
    "        gain_list=[(self.sign*kernel_dist[v2i[u],v2i[v.item()]],v.item(), e.item()) for v,e in zip(col,edge_index)] \n",
    "        #-1 selecting nearest\n",
    "        #1 selecting farthest\n",
    "\n",
    "        heapq.heapify(gain_list)\n",
    "        #print(gain_list)\n",
    "\n",
    "        S=[u]\n",
    "        S_G=[]\n",
    "        S_edge=[]\n",
    "        S_index=[v2i[u]]\n",
    "        \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5         \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))\n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "        #print(len(col),l1, l2, l3)\n",
    "        \n",
    "        rank=1 #rank weight instead gain weight\n",
    "        \n",
    "        while(gain_list):\n",
    "            (gain_v, v, e) = heapq.heappop(gain_list)\n",
    "            gain_v = self.sign*gain_v #make it positive\n",
    "            #print(gain_v, v)\n",
    "\n",
    "            if len(gain_list)==0:                                    \n",
    "                S.append(v)\n",
    "                #if gain_v<1e-6:gain_v=1e-6#S_G.append(gain_v)#S_G.append(rank)\n",
    "                if rank <= l1:S_G.append(self.w1)                \n",
    "                elif rank<=l1+l2:S_G.append(self.w2)\n",
    "                else:S_G.append(self.w3)\n",
    "\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                break\n",
    "            elif len(gain_list)<l3:\n",
    "                S.append(v)\n",
    "                S_G.append(self.w3)\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                continue\n",
    "            \n",
    "            gain_v_update = self.sign*min(kernel_dist[v2i[v],S_index])\n",
    "            \n",
    "            #print(\"updated: \", S,v,gain_v_update, gain_v)\n",
    "            (gain_v_second,v_second,_)=gain_list[0] #top\n",
    "            gain_v_second = gain_v_second #make it positive\n",
    "\n",
    "            if gain_v_update<=gain_v_second:\n",
    "                \n",
    "                gain_v_update = self.sign*gain_v_update\n",
    "                \n",
    "                if gain_v_update<1e-6:\n",
    "                    gain_v_update=1e-6\n",
    "                S.append(v)\n",
    "                #S_G.append(gain_v_update)\n",
    "                #S_G.append(rank)\n",
    "                \n",
    "                if rank<=l1:\n",
    "                    S_G.append(self.w1)\n",
    "                elif rank<=l1+l2:\n",
    "                    S_G.append(self.w2)\n",
    "                else:\n",
    "                    S_G.append(self.w3)\n",
    "                rank+=1\n",
    "                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "            else:\n",
    "                heapq.heappush(gain_list,(self.sign*gain_v_update,v, e))\n",
    "\n",
    "        return S_G, S_edge\n",
    "    \n",
    "    #serial\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "        \n",
    "        test = 0\n",
    "\n",
    "        for u in range(N):                \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "        \n",
    "            #test += sum((np.array(weight)>1.0).astype(int))\n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        #print(test)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "        \n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "        \n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "        \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "                \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)        \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):  \n",
    "        \n",
    "        if self.data.num_nodes<10000:\n",
    "            weight = self.get_submodular_weight()\n",
    "        else:      \n",
    "            self.on_device=False\n",
    "            self.X = self.data.x.to('cpu')        \n",
    "            self.model = self.model.to('cpu')            \n",
    "            self.model.eval()\n",
    "            #weight = self.get_submodular_weight_multiproces()\n",
    "            weight = self.get_submodular_weight()    \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "# data, dataset = get_data('karate', log=False)\n",
    "# submodular_weight = LinkSub(data, selfloop = False, log = True)\n",
    "# submodular_weight.lazy_greedy_weight(1)\n",
    "# #data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4baeed5",
   "metadata": {},
   "source": [
    "## Submodular Weight assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c3b1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkSub():\n",
    "    \n",
    "    def __init__(self, data, value='max', selfloop = False, log = True, worker=0, lambda1=0.25, lambda2=0.25, w1=1.0, w2=0.5, w3=0.1):\n",
    "        \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        self.log = log\n",
    "        self.selfloop = selfloop\n",
    "        \n",
    "        self.lambda1=lambda1\n",
    "        self.lambda2=lambda2\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "    \n",
    "        #self.X = data.x.to(device)\n",
    "        self.X = self.data.x\n",
    "        self.on_device=True        \n",
    "        \n",
    "        self.model = train_link(data, selfloop = selfloop, log= log, worker=worker)\n",
    "        self.model.eval()        \n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        if self.log:\n",
    "            print(\"value: \", value)\n",
    "        \n",
    "        self.value = value\n",
    "        \n",
    "    def pairwise_link(self, x):  \n",
    "                \n",
    "        n, f = x.shape\n",
    "        \n",
    "        x_col1 = x.repeat_interleave(n, dim=0)\n",
    "        x_col2 = x.repeat(n,1)\n",
    "        # print(x_col1, x_col2)\n",
    "        \n",
    "        if self.on_device:\n",
    "            x_col1 = x_col1.to(device)\n",
    "            x_col2 = x_col2.to(device)            \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(x_col1, x_col2).detach().cpu()\n",
    "            output = torch.clamp(output, min=1e-3, max=1.0)\n",
    "        #print(output.shape)\n",
    "#         output = output.softmax(dim=1)\n",
    "#         second_column = output[:,1].cpu()        \n",
    "        #print(second_column)\n",
    "        \n",
    "        similarity_matrix = output.view(n,n)\n",
    "        \n",
    "#         print(similarity_matrix)\n",
    "        \n",
    "        return similarity_matrix.numpy()\n",
    "    \n",
    "    \n",
    "    def lazy_greedy_weight(self,u):\n",
    "    \n",
    "        row, col, edge_index = self.adj[u,:].coo()\n",
    "        \n",
    "        if len(col)==0:\n",
    "            return [],[]\n",
    "                \n",
    "        vertices = [u]+col.tolist()\n",
    "        \n",
    "        v2i={i:j for i,j in zip(vertices, range(len(vertices)))}\n",
    "        i2v={value:key for key, value in v2i.items()}\n",
    "                    \n",
    "        kernel_dist = self.pairwise_link(self.X[vertices])\n",
    "                \n",
    "#         print(vertices)\n",
    "#         print(row,col,edge_index)\n",
    "#         print(self.data.x[vertices])\n",
    "#         print(kernel_dist) \n",
    "        \n",
    "        #convert to max heap by multiplying with -1\n",
    "        gain_of_u = np.sum(kernel_dist[v2i[u],:])\n",
    "        gain_list=[(-1*(sum(np.max(kernel_dist[[v2i[u],v2i[v.item()]],:],axis=0))-gain_of_u),v.item(), e.item()) for v,e in zip(col,edge_index)] \n",
    "        \n",
    "#         print(gain_of_u)\n",
    "#         print(gain_list)\n",
    "\n",
    "        heapq.heapify(gain_list)\n",
    "        #print(gain_list)\n",
    "        \n",
    "        S=[u]\n",
    "        S_G=[]\n",
    "        S_edge=[]\n",
    "        S_index=[v2i[u]]\n",
    "                        \n",
    "        lambda1 = self.lambda1 #top 25% with probability 1\n",
    "        lambda2 = self.lambda2 #second 25% with probability 0.5         \n",
    "        l1=math.ceil(len(col)*lambda1)\n",
    "        l2=min(len(col)-l1,math.ceil(len(col)*lambda2))\n",
    "        l3=max(0,int(len(col)-l1-l2))\n",
    "        \n",
    "        #print(len(col),l1, l2, l3)\n",
    "        \n",
    "        rank=1 #rank weight instead gain weight\n",
    "        \n",
    "        S_index_gain=gain_of_u\n",
    "        \n",
    "        while(gain_list):\n",
    "            (gain_v, v, e) = heapq.heappop(gain_list)\n",
    "            gain_v = -1*gain_v #make it positive\n",
    "            \n",
    "            #print(\"popped: \",gain_v, v)                        \n",
    "\n",
    "            if len(gain_list)==0:                                    \n",
    "                S.append(v)\n",
    "                if gain_v<1e-6:\n",
    "                    gain_v=1e-6#S_G.append(gain_v)#S_G.append(rank)\n",
    "                                        \n",
    "                if rank <= l1:S_G.append(self.w1)                \n",
    "                elif rank<=l1+l2:S_G.append(self.w2)\n",
    "                else:S_G.append(self.w3)\n",
    "\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                break\n",
    "                \n",
    "            elif len(gain_list)<l3:\n",
    "                S.append(v)\n",
    "                S_G.append(self.w3)\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                continue\n",
    "                \n",
    "            gain_v_update = sum(np.max(kernel_dist[np.append(S_index,v2i[v]),:],axis=0))-S_index_gain\n",
    "            \n",
    "            #print(\"updated: \",gain_v_update, S_index_gain)\n",
    "                    \n",
    "            #print(\"updated: \", S,v,gain_v_update, gain_v)\n",
    "            (gain_v_second,v_second,_)=gain_list[0] #top\n",
    "            gain_v_second = -1*gain_v_second #make it positive\n",
    "\n",
    "            if gain_v_update>=gain_v_second:                \n",
    "                if gain_v_update<1e-6:\n",
    "                    gain_v_update=1e-6\n",
    "                    \n",
    "                gain_v_update = -1*gain_v_update    \n",
    "                \n",
    "                S.append(v)\n",
    "                #S_G.append(gain_v_update)\n",
    "                #S_G.append(rank)\n",
    "                S_index_gain = sum(np.max(kernel_dist[np.append(S_index,v2i[v]),:],axis=0))\n",
    "                                \n",
    "                \n",
    "                    \n",
    "                if rank<=l1:S_G.append(self.w1)\n",
    "                elif rank<=l1+l2:S_G.append(self.w2)\n",
    "                else:S_G.append(self.w3)\n",
    "                rank+=1                \n",
    "                S_edge.append(e)\n",
    "                S_index.append(v2i[v])\n",
    "                \n",
    "                #print(\"Taken: \", S_index, S_index_gain)\n",
    "                \n",
    "            else:\n",
    "                heapq.heappush(gain_list,(-1*gain_v_update,v, e))            \n",
    "    \n",
    "        return S_G, S_edge\n",
    "            \n",
    "    #serial\n",
    "    def get_submodular_weight(self):\n",
    "        \n",
    "        N = self.N\n",
    "        #N = 1000\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')\n",
    "\n",
    "        edge_weight=[]\n",
    "        edge_index=[]\n",
    "        \n",
    "        test = 0\n",
    "\n",
    "        for u in range(N):                \n",
    "            weight, e_index = self.lazy_greedy_weight(u)\n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "        \n",
    "            #test += sum((np.array(weight)>1.0).astype(int))\n",
    "            if self.log:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        #print(test)\n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E        \n",
    "        \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)\n",
    "        \n",
    "        return weight\n",
    "        \n",
    "    \n",
    "    def process_block(self, list_u):\n",
    "        \n",
    "        #print(\"Processing :\",len(list_u), list_u[0], list_u[-1])\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_index = []\n",
    "        \n",
    "        for u in list_u:        \n",
    "            weight, e_index = self.lazy_greedy_weight(u)            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "        #print(\"Done :\",len(list_u), list_u[0], list_u[-1])\n",
    "            \n",
    "        return edge_weight, edge_index, len(list_u)\n",
    "        \n",
    "    \n",
    "    #multiprocessing\n",
    "    def get_submodular_weight_multiproces(self):\n",
    "        \n",
    "        edge_weight=[]\n",
    "        edge_index=[]        \n",
    "        \n",
    "        N = self.N\n",
    "        num_blocks = NUM_PROCESSORS\n",
    "        elem_size = int(N/num_blocks)\n",
    "        \n",
    "        nodes = np.arange(num_blocks*elem_size).reshape(num_blocks,-1).tolist()\n",
    "        if num_blocks*elem_size<N:\n",
    "            nodes.append(list(range(num_blocks*elem_size,N)))        \n",
    "        \n",
    "        pool_size = NUM_PROCESSORS        \n",
    "        if self.log:\n",
    "            print(\"Pool Size: \", pool_size)        \n",
    "        pool = Pool(pool_size)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar = tqdm(total=N)\n",
    "            pbar.set_description(f'Nodes')  \n",
    "                \n",
    "        for (weight, e_index, num_el) in pool.imap_unordered(self.process_block, nodes):            \n",
    "            edge_weight.extend(weight)\n",
    "            edge_index.extend(e_index)\n",
    "            \n",
    "            if self.log:\n",
    "                pbar.update(num_el)\n",
    "        \n",
    "        if self.log:\n",
    "            pbar.close()\n",
    "        \n",
    "        assert len(edge_index)==self.E\n",
    "                \n",
    "        weight=torch.zeros(len(edge_index))        \n",
    "        weight[edge_index]=torch.Tensor(edge_weight)        \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "    \n",
    "    def compute_weights(self):  \n",
    "        \n",
    "        if self.data.num_nodes<10000:\n",
    "            weight = self.get_submodular_weight()\n",
    "        else:      \n",
    "            self.on_device=False\n",
    "            self.X = self.data.x.to('cpu')        \n",
    "            self.model = self.model.to('cpu')            \n",
    "            self.model.eval()\n",
    "            #weight = self.get_submodular_weight_multiproces()\n",
    "            weight = self.get_submodular_weight()    \n",
    "        \n",
    "        return weight\n",
    "    \n",
    "# data, dataset = get_data('karate', log=False)\n",
    "# submodular_weight = LinkSub(data, selfloop = False, log = True)\n",
    "# submodular_weight.lazy_greedy_weight(1)\n",
    "# data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c8e1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('karate', log = False)\n",
    "# submodular_weight = LinkSub(data, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d24d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submodular_weight.lazy_greedy_weight(0)\n",
    "#data.weight = submodular_weight.compute_weights()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbb922",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec49c506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  ./Dataset/\n",
      "Result directory: ./Dataset/RESULTS/\n",
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "===========================================================================================================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 LinkModel(\n",
      "  (MLP1): Linear(in_features=34, out_features=32, bias=True)\n",
      "  (MLP3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 231200/231200 [00:00<00:00, 351522.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Loss 0.1713 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 820732.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.8744,\t0.8744,\t0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 231200/231200 [00:00<00:00, 330570.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 Loss 0.0517 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 359883.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 231200/231200 [00:00<00:00, 428052.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 Loss 0.0322 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 870894.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 231200/231200 [00:00<00:00, 423541.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 Loss 0.0290 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 748440.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 231200/231200 [00:00<00:00, 407108.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Loss 0.0270 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 757610.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 231200/231200 [00:00<00:00, 386223.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 Loss 0.0257 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 622238.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 231200/231200 [00:00<00:00, 420031.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Loss 0.0238 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 899188.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 231200/231200 [00:00<00:00, 370645.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 Loss 0.0226 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 861798.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 231200/231200 [00:00<00:00, 424572.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 Loss 0.0219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 751748.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 231200/231200 [00:00<00:00, 425088.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 780933.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 231200/231200 [00:00<00:00, 380179.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 Loss 0.0222 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 503538.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 231200/231200 [00:00<00:00, 319929.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 730994.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 231200/231200 [00:00<00:00, 248543.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 537363.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 231200/231200 [00:00<00:00, 412108.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 685536.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 231200/231200 [00:00<00:00, 322162.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 Loss 0.0216 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 869871.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 231200/231200 [00:00<00:00, 351399.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 333308.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 231200/231200 [00:00<00:00, 442283.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 818125.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 231200/231200 [00:00<00:00, 297990.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 Loss 0.0221 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 855198.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 231200/231200 [00:00<00:00, 245173.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 Loss 0.0221 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 753946.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 231200/231200 [00:00<00:00, 440339.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 Loss 0.0216 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 623340.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 481133.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\t0.6772,\t0.6772,\t0.5469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 735793.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\t0.6779,\t0.6779,\t0.5477\n",
      "Execution time:  32.23678493499756\n",
      "LinkModel(\n",
      "  (MLP1): Linear(in_features=34, out_features=32, bias=True)\n",
      "  (MLP3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 231200/231200 [00:00<00:00, 284711.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Loss 0.1562 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 409271.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.9369,\t0.9369,\t0.9336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 231200/231200 [00:00<00:00, 440110.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 Loss 0.0522 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 802357.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 231200/231200 [00:01<00:00, 212251.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 Loss 0.0270 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:01<00:00, 169082.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 231200/231200 [00:00<00:00, 260868.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 Loss 0.0246 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 737262.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 231200/231200 [00:00<00:00, 241761.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Loss 0.0233 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 741888.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 231200/231200 [00:00<00:00, 243977.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 Loss 0.0220 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 647117.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 231200/231200 [00:00<00:00, 274620.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 373288.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 231200/231200 [00:00<00:00, 411488.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 265533.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 231200/231200 [00:00<00:00, 234719.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 Loss 0.0216 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 746525.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 231200/231200 [00:01<00:00, 215682.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 747259.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 231200/231200 [00:01<00:00, 163920.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 Loss 0.0215 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:01<00:00, 175109.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 231200/231200 [00:00<00:00, 341800.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 554380.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 231200/231200 [00:00<00:00, 290024.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 Loss 0.0219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 568638.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 231200/231200 [00:00<00:00, 276736.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 369727.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 231200/231200 [00:00<00:00, 382735.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 Loss 0.0220 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 284172.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 231200/231200 [00:00<00:00, 387494.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 296465.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 231200/231200 [00:00<00:00, 240000.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 Loss 0.0219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 848791.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 231200/231200 [00:01<00:00, 218945.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 Loss 0.0215 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 878043.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 231200/231200 [00:01<00:00, 187309.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 Loss 0.0219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:01<00:00, 179508.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 231200/231200 [00:00<00:00, 289469.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 527835.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 384176.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\t0.6785,\t0.6785,\t0.5486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 282522.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\t0.6805,\t0.6805,\t0.5511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 34/34 [00:00<00:00, 2272.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  35.98017597198486\n",
      "LinkModel(\n",
      "  (MLP1): Linear(in_features=34, out_features=32, bias=True)\n",
      "  (MLP3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 231200/231200 [00:00<00:00, 295239.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 Loss 0.1575 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 378563.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0.9374,\t0.9374,\t0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 231200/231200 [00:00<00:00, 235803.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 Loss 0.0518 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 870624.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 231200/231200 [00:00<00:00, 232672.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 Loss 0.0319 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 742471.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 231200/231200 [00:00<00:00, 254693.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 Loss 0.0274 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 513163.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 231200/231200 [00:00<00:00, 261325.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Loss 0.0259 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 363262.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 231200/231200 [00:01<00:00, 140488.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 Loss 0.0244 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 265453.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 231200/231200 [00:00<00:00, 292809.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Loss 0.0238 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 362539.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 231200/231200 [00:00<00:00, 235087.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 Loss 0.0235 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 703205.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 231200/231200 [00:01<00:00, 207263.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 Loss 0.0238 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 730626.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 231200/231200 [00:00<00:00, 254029.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 Loss 0.0237 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 750475.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 231200/231200 [00:00<00:00, 282936.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 Loss 0.0234 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 739499.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 231200/231200 [00:00<00:00, 257585.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 Loss 0.0230 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 624512.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 231200/231200 [00:00<00:00, 239161.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 Loss 0.0216 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 734353.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 231200/231200 [00:01<00:00, 159642.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:01<00:00, 170531.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 231200/231200 [00:00<00:00, 431468.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 Loss 0.0219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 268639.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 231200/231200 [00:00<00:00, 451794.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 Loss 0.0219 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 294640.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 231200/231200 [00:01<00:00, 221573.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 Loss 0.0218 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 880154.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 231200/231200 [00:00<00:00, 234337.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 Loss 0.0222 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 867563.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 231200/231200 [00:01<00:00, 230966.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 Loss 0.0217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 880020.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 231200/231200 [00:00<00:00, 238043.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 Loss 0.0215 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 739034.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1.0000,\t1.0000,\t1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 287044.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val\t0.6790,\t0.6790,\t0.5492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: : 100%|██████████| 231200/231200 [00:00<00:00, 880282.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\t0.6768,\t0.6768,\t0.5464\n",
      "value:  max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 34/34 [00:00<00:00, 502.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  35.348976373672485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    log = True\n",
    "    datasetname = 'karate'\n",
    "    #args.link_batch_size = 32\n",
    "    args.epochs = 2\n",
    "    \n",
    "    data, dataset = get_data(datasetname, log=log, h_score=True)\n",
    "    #data = generate_synthetic(data, d=100, h=0.25, train=0.1, random_state=1, log=log)\n",
    "    #--------------------------#\n",
    "    start = time.time() \n",
    "    data.weight = get_link_weight(data, selfloop = True, log = log, worker = 0)\n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "    #--------------------------#\n",
    "    start = time.time()     \n",
    "    nn_weight = LinkNN(data, value ='min', log = log) \n",
    "    data.weight = nn_weight.compute_weights()    \n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "    #--------------------------#\n",
    "    start = time.time()    \n",
    "    submodular_weight = LinkSub(data, value ='max', selfloop = True, log = log)    \n",
    "    data.weight = submodular_weight.compute_weights()    \n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)\n",
    "    #--------------------------#    \n",
    "#     if 'weight' in data:\n",
    "#         cp_data= copy.deepcopy(data)\n",
    "#         G = to_networkx(cp_data, to_undirected=True, edge_attrs=['weight'])\n",
    "#         to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] <1.0 ]\n",
    "#         G.remove_edges_from(to_remove)\n",
    "#         updated_data = from_networkx(G)\n",
    "        \n",
    "#         print(updated_data)\n",
    "        \n",
    "#         updated_data = from_networkx(G, group_edge_attrs=['weight'])\n",
    "#         updated_data.weight = updated_data.edge_attr.view(-1)\n",
    "        \n",
    "#         row, col = updated_data.edge_index\n",
    "#         updated_data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "#         updated_data.weight = torch.cat((updated_data.weight, updated_data.weight),dim=0)\n",
    "        \n",
    "#         print(\"Node Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='node'))\n",
    "#         print(\"Edge Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge'))\n",
    "#         print(\"Edge_insensitive Homophily:\", homophily(updated_data.edge_index, cp_data.y, method='edge_insensitive'))    \n",
    "        \n",
    "    None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
