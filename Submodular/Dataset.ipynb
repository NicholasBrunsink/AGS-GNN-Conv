{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74d868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "kernel_name = os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))\n",
    "\n",
    "if kernel_name == 'py38cu11':\n",
    "    import ctypes\n",
    "    ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcusparse.so.11\");\n",
    "    ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcublas.so.11\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ec2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon, Coauthor\n",
    "\n",
    "import ipynb.fs.full.utils.MoonGraph as MoonGraph\n",
    "# import utils.MoonGraph as MoonGraph\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import torch_geometric.utils.subgraph as subgraph\n",
    "\n",
    "#import torch_geometric.utils.assortativity as assortativity #pytorch geometric's latest version has this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523668ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gamma, uniform, expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70095f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.typing import Adj, SparseTensor\n",
    "from torch_geometric.utils import coalesce, degree\n",
    "from torch_geometric.utils.to_dense_adj import to_dense_adj\n",
    "\n",
    "\n",
    "def assortativity(edge_index: Adj) -> float:\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        adj: SparseTensor = edge_index\n",
    "        row, col, _ = adj.coo()\n",
    "    else:\n",
    "        assert isinstance(edge_index, Tensor)\n",
    "        row, col = edge_index\n",
    "\n",
    "    device = row.device\n",
    "    out_deg = degree(row, dtype=torch.long)\n",
    "    in_deg = degree(col, dtype=torch.long)\n",
    "    degrees = torch.unique(torch.cat([out_deg, in_deg]))\n",
    "    mapping = row.new_zeros(degrees.max().item() + 1)\n",
    "    mapping[degrees] = torch.arange(degrees.size(0), device=device)\n",
    "\n",
    "    # Compute degree mixing matrix (joint probability distribution) `M`\n",
    "    num_degrees = degrees.size(0)\n",
    "    src_deg = mapping[out_deg[row]]\n",
    "    dst_deg = mapping[in_deg[col]]\n",
    "\n",
    "    pairs = torch.stack([src_deg, dst_deg], dim=0)\n",
    "    occurrence = torch.ones(pairs.size(1), device=device)\n",
    "    pairs, occurrence = coalesce(pairs, occurrence)\n",
    "    M = to_dense_adj(pairs, edge_attr=occurrence, max_num_nodes=num_degrees)[0]\n",
    "    # normalization\n",
    "    M /= M.sum()\n",
    "\n",
    "    # numeric assortativity coefficient, computed by\n",
    "    # Pearson correlation coefficient of the node degrees\n",
    "    x = y = degrees.float()\n",
    "    a, b = M.sum(0), M.sum(1)\n",
    "\n",
    "    vara = (a * x**2).sum() - ((a * x).sum())**2\n",
    "    varb = (b * x**2).sum() - ((b * x).sum())**2\n",
    "    xy = torch.outer(x, y)\n",
    "    ab = torch.outer(a, b)\n",
    "    out = (xy * (M - ab)).sum() / (vara * varb).sqrt()\n",
    "    return out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42962da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/SitaoLuan/ACM-GNN/tree/main/synthetic-experiments\n",
    "#https://github.com/KAIDI3270/Geom_GCN_pytorch_implementation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def func(feature):\n",
    "\n",
    "    f = list(map(int, feature.split(',')))\n",
    "    \n",
    "    return f\n",
    "\n",
    "def get_heterophily(root, DATASET_NAME='texas', train=0.6, val=0.2, test=0.2):\n",
    "    \n",
    "    edge_file = root+'/'+DATASET_NAME+'/out1_graph_edges.txt'\n",
    "    id_feature_label_file = root+'/'+DATASET_NAME+'/out1_node_feature_label.txt'\n",
    "    \n",
    "    edges = pd.read_csv(edge_file, sep='\\t', header=0)\n",
    "    id_feature_label = pd.read_csv(id_feature_label_file, sep='\\t', header=0)\n",
    "    \n",
    "#     print(edges)\n",
    "#     print(id_feature_label)\n",
    "    \n",
    "    edge_index = torch.LongTensor(edges.values.tolist()).T\n",
    "    node_id  = torch.LongTensor(id_feature_label['node_id'].values.tolist())\n",
    "    y = torch.LongTensor(id_feature_label['label'].values.tolist())\n",
    "    x = id_feature_label['feature'].apply(func)\n",
    "    x = torch.Tensor(x.values.tolist())\n",
    "    \n",
    "    N = len(node_id)\n",
    "    indexs = list(range(N))\n",
    "    \n",
    "    train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "    val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    \n",
    "    data = Data(edge_index=edge_index, \n",
    "                x=x, node_id=node_id, \n",
    "                y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#get_heterophily('/scratch/gilbreth/das90/Dataset/heterophily/','squirrel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8f65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_film(root, DATASET_NAME='film', train=0.6, val=0.2, test=0.2):\n",
    "    \n",
    "    file = root+DATASET_NAME+'/'    \n",
    "    f = open(file+'class_map.json')\n",
    "    class_map = json.load(f)\n",
    "    class_map = {int(key):int(value) for key, value in class_map.items()}\n",
    "    #print(class_map)\n",
    "    f.close()    \n",
    "    \n",
    "    y = list(class_map.values())\n",
    "    x = np.load(file+'feats.npy')    \n",
    "    #print(x.shape)\n",
    "    \n",
    "#     f = open(file+'id_map.json')\n",
    "#     id_map = json.load(f)\n",
    "#     id_map = {int(key):int(value) for key, value in id_map.items()}    \n",
    "#     #print(id_map)\n",
    "#     f.close()\n",
    "    \n",
    "    #target = pd.read_csv(file+'film_target.csv', sep=',', header=0)\n",
    "    #print(target)\n",
    "    #target['new_id']=target['id'].apply(lambda x: id_map[x])\n",
    "    \n",
    "    \n",
    "    edges = pd.read_csv(file+'film_edges.csv', sep=',', header=0)\n",
    "    \n",
    "    u = edges['id1'].values.tolist()\n",
    "    v = edges['id2'].values.tolist()\n",
    "    \n",
    "    edge_index=[u,v]\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "    edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    indexs = list(range(N))\n",
    "    train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "    val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "#     train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "#     val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    \n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    \n",
    "    data = Data(edge_index=edge_index, \n",
    "                x=x,\n",
    "                y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#get_film('/scratch/gilbreth/das90/Dataset/heterophily/','film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6acefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=False):\n",
    "    \n",
    "    if isinstance(data.x, SparseTensor):\n",
    "        N = data.x.size(0)\n",
    "        data.num_nodes = N\n",
    "    else:\n",
    "        N = data.x.shape[0]\n",
    "    \n",
    "    indexs = list(range(N))\n",
    "    \n",
    "    if random_state:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    else:        \n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba35d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataset(Dataset):\n",
    "    def __init__(self, root, dataset_name, train=0.6, val=0.2, test=0.2,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.root = root\n",
    "        self.dataset_name=dataset_name\n",
    "        self.degree=degree\n",
    "        self.train=train\n",
    "        self.val=val\n",
    "        self.test=test\n",
    "        \n",
    "        if dataset_name == 'film':\n",
    "            self.data = get_film(root,dataset_name, train, val, test)\n",
    "        else:\n",
    "            self.data = get_heterophily(root,dataset_name, train, val, test)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "# dataset = HeteroDataset('/scratch/gilbreth/das90/Dataset/heterophily/','texas')\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501b6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINKXpyg2(Dataset):\n",
    "    def __init__(self, root, dataset_name, train=0.6, val=0.2, test=0.2,\n",
    "                 transform=None, pre_transform=None, pre_filter=None, random_state=False):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.root = root\n",
    "        self.dataset_name=dataset_name\n",
    "        self.degree=degree\n",
    "        self.train=train\n",
    "        self.val=val\n",
    "        self.test=test\n",
    "        \n",
    "        FolderName = root+'/LINKXdataset/'+dataset_name+'/'\n",
    "\n",
    "        data = Data()\n",
    "\n",
    "        data.x = torch.load(FolderName+'x.pt')\n",
    "        data.edge_index =torch.load(FolderName+'edge_index.pt')\n",
    "        data.y = torch.load(FolderName+'y.pt')\n",
    "    \n",
    "        self.data = train_val_test_mask(data, train=train, val=val, test=test, random_state=random_state)\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root+'/LINKXdataset/'+self.dataset_name\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "# #'pokec', 'arxiv-year', 'snap-patents', 'twitch-gamer'\n",
    "# dataset = LINKXpyg2('/scratch/gilbreth/das90/Dataset/','pokec', random_state=0)\n",
    "# data = dataset[0]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013ed21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGB_MAGcustom(Dataset):\n",
    "    def __init__(self, root, dataset_name, data, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.dataset_name=dataset_name\n",
    "        \n",
    "        self.FolderName = root\n",
    "        self.data = data\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root+self.dataset_name\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):        \n",
    "        return self.data\n",
    "    \n",
    "# #'pokec', 'arxiv-year', 'snap-patents', 'twitch-gamer'\n",
    "# dataset = LINKXpyg2('/scratch/gilbreth/das90/Dataset/','pokec', random_state=0)\n",
    "# data = dataset[0]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57e8e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "# dataset = Reddit(root=DIR+'Reddit')\n",
    "# data = dataset[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc434e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RedditSynthetic(DIR, h=0.5, k=25, log=False, recompute=False):\n",
    "    \n",
    "    file_path = DIR+'RedditSynthetic/Reddit'+str(h)+str(k)    \n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    dataset = Reddit(root=DIR+'Reddit')\n",
    "    \n",
    "    if os.path.isfile(file_path) and recompute==False:\n",
    "        data = torch.load(file_path)  \n",
    "        \n",
    "        if log:\n",
    "            print(\"loaded from: \",file_path)\n",
    "        \n",
    "        return dataset, data\n",
    "    \n",
    "    data = dataset[0]\n",
    "    \n",
    "    if log:\n",
    "        print(data)\n",
    "        \n",
    "    num_class = max(data.y)+1\n",
    "    \n",
    "    E = data.num_edges\n",
    "    N = data.num_nodes\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    edge_numbers = []\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            if log:\n",
    "                pbar.update(1)\n",
    "            continue\n",
    "        y_current = data.y[i].item()\n",
    "        \n",
    "#         print(y_current)\n",
    "#         print(row, col, edge)\n",
    "        \n",
    "        \n",
    "        match_indexes = ((data.y[col] == y_current).nonzero()).view(-1).numpy()\n",
    "        other_indexes = ((data.y[col] != y_current).nonzero()).view(-1).numpy()\n",
    "        \n",
    "#         print(len(match_indexes),match_indexes)\n",
    "#         print(len(other_indexes),other_indexes)\n",
    "        \n",
    "    \n",
    "        select=int(len(col)*k/100) #select k percent of nodes\n",
    "        h_select = min(int(select*h),len(match_indexes)) #select h homophilic nodes\n",
    "        o_select = min(select-h_select, len(other_indexes))\n",
    "        \n",
    "#         print(h_select, len(match_indexes))\n",
    "#         print(o_select, len(other_indexes))\n",
    "#         print(\"*\"*100)\n",
    "        \n",
    "        samples1 = np.random.choice(match_indexes, h_select, replace=False)\n",
    "        samples2 = np.random.choice(other_indexes, o_select, replace=False)\n",
    "                            \n",
    "        edge_numbers.append(edge[samples1])\n",
    "        edge_numbers.append(edge[samples2])\n",
    "        \n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:        \n",
    "        pbar.close()\n",
    "            \n",
    "    edge_numbers = torch.cat(edge_numbers)    \n",
    "    edge_index = data.edge_index[:,edge_numbers]\n",
    "    \n",
    "    if log:\n",
    "        print(f'Average node degree: {edge_index.shape[1] / N:.2f}')    \n",
    "        print(homophily(edge_index, data.y, method='node'),homophily(edge_index, data.y, method='edge'), end=' ')\n",
    "    \n",
    "    data.edge_index = edge_index\n",
    "    \n",
    "    torch.save(data, file_path)\n",
    "\n",
    "    if log:\n",
    "        print(\"Data saved to file: \",file_path)\n",
    "        \n",
    "    return dataset, data\n",
    "\n",
    "# RedditSynthetic(DIR, h=0.5, k=25, log=True, recompute=False)\n",
    "\n",
    "# for h in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#     RedditSynthetic(DIR, h=h, k=25, log=True, recompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e79d3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(DATASET_NAME='Cora', DIR=None, params=None, train=None, random_state=False, log=True, h_score=False, split_no=0):\n",
    "    \n",
    "    if DIR is not None:\n",
    "        if log: print('Looking at: ',DIR)    \n",
    "    elif os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "        DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "    elif os.uname()[1].find('unimodular')==0:\n",
    "        DIR='/scratch2/das90/Dataset/'\n",
    "    else:\n",
    "        DIR='./Dataset/'\n",
    "\n",
    "    Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    RESULTS_DIR=DIR+'RESULTS/'\n",
    "    Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if log:\n",
    "        print(\"Data directory: \", DIR)\n",
    "        print(\"Result directory:\", RESULTS_DIR)\n",
    "    \n",
    "    from torch_geometric.datasets import Planetoid,  KarateClub, CitationFull\n",
    "    from torch_geometric.transforms import NormalizeFeatures\n",
    "    from torch_geometric.datasets import Reddit, Reddit2\n",
    "    \n",
    "    #DATASET_NAME='Cora' #\"Cora\", \"CiteSeer\", \"PubMed\"\n",
    "\n",
    "    if DATASET_NAME in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "        dataset = Planetoid(root=DIR+'Planetoid', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME in ['cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed']:\n",
    "        #['cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed']\n",
    "        dataset = CitationFull(root=DIR+'Citation', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"Reddit2\":\n",
    "        #dataset = Reddit2(root=DIR+'Reddit2', transform=NormalizeFeatures())\n",
    "        dataset = Reddit2(root=DIR+'Reddit2')\n",
    "        \n",
    "    elif DATASET_NAME == \"Reddit\":\n",
    "        #dataset = Reddit(root=DIR+'Reddit', transform=NormalizeFeatures())\n",
    "        dataset = Reddit(root=DIR+'Reddit')    \n",
    "        \n",
    "    elif DATASET_NAME in [\"RedditSynthetic\", \"Reddit0.125\",\"Reddit0.225\",\"Reddit0.325\",\"Reddit0.425\",\"Reddit0.525\",\n",
    "                          \"Reddit0.625\",\"Reddit0.725\",\"Reddit0.825\",\"Reddit0.925\"]:\n",
    "        \n",
    "        h = k = 0\n",
    "        \n",
    "        if DATASET_NAME == \"RedditSynthetic\":\n",
    "            h = params['h']\n",
    "            k = params['k']\n",
    "        \n",
    "        else:\n",
    "            k = int(DATASET_NAME[-2:])\n",
    "            h = float(DATASET_NAME[-5:-2])\n",
    "            \n",
    "            #print(h,k)\n",
    "        \n",
    "        dataset, data = RedditSynthetic(DIR, h=h, k=k, log=True, recompute=False)\n",
    "                \n",
    "    \n",
    "    elif DATASET_NAME in ['ego-Gplus', 'gemsec-Facebook']:\n",
    "        from torch_geometric.datasets import SNAPDataset        \n",
    "        dataset = SNAPDataset(root=DIR+'SNAPDataset', name=DATASET_NAME, transform=NormalizeFeatures())        \n",
    "        print(dataset)\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "        \n",
    "    \n",
    "    elif DATASET_NAME in [\"BlogCatalog\", \"PPI\", \"Facebook\", \"Twitter\", \"TWeibo\", \"MAG\"]:\n",
    "        from torch_geometric.datasets import AttributedGraphDataset\n",
    "        \n",
    "#         dataset = AttributedGraphDataset(root=DIR+'/AttributedGraphDataset', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "        dataset = AttributedGraphDataset(root=DIR+'/AttributedGraphDataset', name=DATASET_NAME)\n",
    "        \n",
    "        print(dataset)\n",
    "        print(dataset[0])        \n",
    "        \n",
    "    elif DATASET_NAME == \"AmazonProducts\":\n",
    "        #dataset = AmazonProducts(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "        dataset = AmazonProducts(root=DIR+'AmazonProducts')\n",
    "        \n",
    "    elif DATASET_NAME in ['Computers', 'Photo']:\n",
    "        #dataset = Amazon(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "        dataset = Amazon(root=DIR+'Amazon/', name = DATASET_NAME)        \n",
    "        \n",
    "\n",
    "    elif DATASET_NAME in ['CS', 'Physics']:\n",
    "        dataset = Coauthor(root=DIR+'Coauthor/', name = DATASET_NAME)        \n",
    "\n",
    "        \n",
    "    elif DATASET_NAME == \"Moon\":\n",
    "        dataset = MoonGraph.MoonDataset(n_samples=100, degree=5, train=0.5)    \n",
    "        G, data =dataset[0]\n",
    "    \n",
    "    elif DATASET_NAME == \"karate\":\n",
    "        dataset = KarateClub()        \n",
    "        data = dataset[0]\n",
    "        data.val_mask = ~data.train_mask\n",
    "        data.test_mask = data.val_mask\n",
    "    \n",
    "    elif DATASET_NAME == \"Fake\":\n",
    "        dataset = FakeDataset(num_graphs = 1, \n",
    "                              avg_num_nodes = 2000, \n",
    "                              avg_degree = 10, \n",
    "                              num_channels = 64, \n",
    "                              edge_dim = 0, \n",
    "                              num_classes = 10, \n",
    "                              task = 'auto', \n",
    "                              is_undirected = True,                               \n",
    "                              transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME == \"OGB_MAG\":\n",
    "        #dataset = OGB_MAG(root=DIR+'OGB_MAG', preprocess='metapath2vec', transform=NormalizeFeatures())\n",
    "        dataset = OGB_MAG(root=DIR+'OGB_MAG3', preprocess='metapath2vec')\n",
    "        \n",
    "        data = dataset[0]                \n",
    "        #print(dataset, data, data['paper'])        \n",
    "        data = Data(x=data['paper'].x, edge_index=data['paper', 'cites', 'paper'].edge_index,\n",
    "                   train_mask=data['paper'].train_mask,val_mask = data['paper'].val_mask,test_mask = data['paper'].test_mask,y= data['paper'].y,)\n",
    "        \n",
    "        dataset = OGB_MAGcustom(DIR+'OGB_MAG3','OGB_MAG',data)\n",
    "        \n",
    "        #return dataset\n",
    "        \n",
    "    elif DATASET_NAME == \"Flickr\":\n",
    "        dataset = Flickr(root=DIR+'Flickr')\n",
    "    \n",
    "    elif DATASET_NAME == \"Yelp\":\n",
    "        dataset = Yelp(root=DIR+'Yelp')\n",
    "    \n",
    "    elif DATASET_NAME == \"PPI\":\n",
    "        \n",
    "        dataset = PPI(root=DIR+'PPI')\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    ###heterophilic dataset\n",
    "    #https://github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py\n",
    "    elif DATASET_NAME in ['pokec', 'arxiv-year', 'snap-patents', 'twitch-gamer','wiki']:\n",
    "        dataset = LINKXpyg2(DIR, DATASET_NAME, random_state=random_state)\n",
    "    \n",
    "    elif DATASET_NAME in [\"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import LINKXDataset\n",
    "        \n",
    "        dataset = LINKXDataset(root = DIR+'/Heterophilic/', name = DATASET_NAME)    \n",
    "        #transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME in [\"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import HeterophilousGraphDataset\n",
    "        \n",
    "        dataset = HeterophilousGraphDataset(DIR+'/Heterophilic/', DATASET_NAME)    \n",
    "        #transform=NormalizeFeatures())\n",
    "    elif DATASET_NAME == 'Actor':\n",
    "        from ipynb.fs.full.HeterophilousDataset import Actor\n",
    "        \n",
    "        dataset = Actor(root=DIR+'/Heterophilic/Actor')\n",
    "        #transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME in [\"Cornell\", \"Texas\", \"Wisconsin\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import WebKB\n",
    "        \n",
    "        dataset = WebKB(root = DIR+'/Heterophilic/', name = DATASET_NAME)\n",
    "        #transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME in [\"Chameleon\", \"Crocodile\", \"Squirrel\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import WikipediaNetwork\n",
    "        \n",
    "        if DATASET_NAME == 'Crocodile':\n",
    "            dataset = WikipediaNetwork(DIR+'/Heterophilic/', DATASET_NAME.lower(), geom_gcn_preprocess= False)\n",
    "        else:        \n",
    "            dataset = WikipediaNetwork(DIR+'/Heterophilic/', DATASET_NAME.lower())\n",
    "            #transform=NormalizeFeatures())\n",
    "    \n",
    "    #implemented heterophily\n",
    "    elif DATASET_NAME in ['chameleon','cornell','film', 'squirrel', 'texas','wisconsin']:\n",
    "        dataset = HeteroDataset(DIR+'/heterophily/', DATASET_NAME)       \n",
    "    \n",
    "    else: \n",
    "        return None, None\n",
    "        raise Exception('dataset not found')\n",
    "\n",
    "    if DATASET_NAME in ['Moon', 'karate','OGB_MAG'] or DATASET_NAME[:7]=='Reddit0' or DATASET_NAME in ['RedditSynthetic']:\n",
    "        #MoonGraph.draw_blobs_data(G, data)        \n",
    "        None\n",
    "    else:\n",
    "        data = dataset[0]  # Get the first graph object.\n",
    "        if 'train_mask' not in data:\n",
    "            data = train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=random_state)\n",
    "            \n",
    "        elif data.train_mask.dim()>1:            \n",
    "            \n",
    "            if data.train_mask.shape[1]>split_no:            \n",
    "                data.train_mask = data.train_mask[:,split_no]\n",
    "                data.val_mask = data.val_mask[:,split_no]\n",
    "                data.test_mask = data.test_mask[:,split_no]\n",
    "            else:\n",
    "                data = train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=random_state)\n",
    "            \n",
    "        \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "        \n",
    "    if log:\n",
    "        print()\n",
    "        print(f'Dataset: {dataset}:')\n",
    "        print('======================')\n",
    "        print(f'Number of graphs: {len(dataset)}')\n",
    "        print(f'Number of features: {dataset.num_features}')\n",
    "        print(f'Number of classes: {dataset.num_classes}')\n",
    "        print()\n",
    "        print(data)\n",
    "        print('===========================================================================================================')\n",
    "\n",
    "        # Gather some statistics about the graph.\n",
    "        print(f'Number of nodes: {data.num_nodes}')\n",
    "        print(f'Number of edges: {data.num_edges}')\n",
    "        print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "        print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "        print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "        print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "        print(f'Has self-loops: {data.has_self_loops()}')\n",
    "        print(f'Is undirected: {data.is_undirected()}')\n",
    "    \n",
    "    if len(data.y.shape) == 1:\n",
    "        labels = data.y\n",
    "    else:\n",
    "        if log: print(\"Testing homophily by converting multi-label to one-label\")\n",
    "        labels = data.y.argmax(dim=1)\n",
    "        data.y = labels\n",
    "    \n",
    "    if torch.min(data.y)<0:\n",
    "        if log: print(\"Shifting label to non-negative\")\n",
    "        data.y = data.y-torch.min(data.y)\n",
    "    \n",
    "    \n",
    "    if h_score:\n",
    "        print(\"N \",data.num_nodes, \" E \",data.num_edges,\" d \",data.num_edges / data.num_nodes, end=' ')\n",
    "        \n",
    "        print(homophily(data.edge_index, labels, method='node'),homophily(data.edge_index, labels, method='edge'), end=' ')\n",
    "        \n",
    "        try:\n",
    "            esen = homophily(data.edge_index, labels, method='edge_insensitive')\n",
    "        except:\n",
    "            esen = -1        \n",
    "        print(esen, end=' ')            \n",
    "        print(assortativity(data.edge_index), end=' ')\n",
    "        \n",
    "        \n",
    "    return data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34d7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params={'h':0.5, 'k':25}\n",
    "# dataset = get_data('Reddit0.525', params=None, log=True, h_score = True, split_no = 0)\n",
    "# dataset = get_data('RedditSynthetic', params=params, log=True, h_score = True, split_no = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b34b4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import SNAPDataset\n",
    "# DIR='/scratch/gilbreth/das90/Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10cde7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ego-facebook': ['facebook.tar.gz'],\n",
    "# 'ego-gplus': ['gplus.tar.gz'],\n",
    "# 'ego-twitter': ['twitter.tar.gz'],\n",
    "# 'soc-ca-astroph': ['ca-AstroPh.txt.gz'],\n",
    "# 'soc-ca-grqc': ['ca-GrQc.txt.gz'],\n",
    "# 'soc-epinions1': ['soc-Epinions1.txt.gz'],\n",
    "# 'soc-livejournal1': ['soc-LiveJournal1.txt.gz'],\n",
    "# 'soc-pokec': ['soc-pokec-relationships.txt.gz'],\n",
    "# 'soc-slashdot0811': ['soc-Slashdot0811.txt.gz'],\n",
    "# 'soc-slashdot0922': ['soc-Slashdot0902.txt.gz'],\n",
    "# 'wiki-vote': ['wiki-Vote.txt.gz'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dd6bb4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "===========================================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "N  2708  E  10556  d  3.8980797636632203 0.825157880783081 0.8099659085273743 0.7657181620597839 -0.06587088108062744 "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':   \n",
    "#     data, dataset = get_data('karate', log=True, h_score = True)\n",
    "#     print(sum(torch.where(data.train_mask==True)))\n",
    "#     data, dataset = get_data('Cora', train=0.2, random_state=True)\n",
    "#     print(sum(torch.where(data.train_mask==True)))\n",
    "    \n",
    "#     data, dataset = get_data('karate', log=True, h_score = True, split_no = 0)\n",
    "\n",
    "    data, dataset = get_data('Cora', log=True, h_score = True)\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0ffb4",
   "metadata": {},
   "source": [
    "## Dataset Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ffb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"Cornell\",\n",
    "    \"Texas\",\n",
    "    \"Wisconsin\",\n",
    "    \"reed98\",\n",
    "    \"amherst41\",\n",
    "    \"penn94\",\n",
    "    \"Roman-empire\",\n",
    "    \"cornell5\",\n",
    "    \"Squirrel\",\n",
    "    \"johnshopkins55\",\n",
    "    \"AmazonProducts\",\n",
    "    \"Actor\",\n",
    "    \"Minesweeper\",\n",
    "    \"Questions\",\n",
    "    \"Chameleon\",\n",
    "    \"Tolokers\",\n",
    "    \"Flickr\",\n",
    "    \"Yelp\",\n",
    "    \"Amazon-ratings\",\n",
    "    \"genius\",\n",
    "    \"cora\",\n",
    "    \"CiteSeer\",\n",
    "    \"dblp\",\n",
    "    \"Computers\",\n",
    "    \"pubmed\",\n",
    "    \"Reddit\",\n",
    "    \"cora_ml\",\n",
    "    \"Cora\",\n",
    "    \"Reddit2\",\n",
    "    \"CS\",\n",
    "    \"Photo\",\n",
    "    \"Physics\",\n",
    "    \"citeseer\",    \n",
    "    'pokec',\n",
    "    'arxiv-year',\n",
    "    'snap-patents',\n",
    "    'twitch-gamer',\n",
    "    'wiki',\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fac98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaa0c1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataset_properties():\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        #print(dataset, end='\\t')\n",
    "        print(i,\"\\t\",dataset)\n",
    "        data, dataset = get_data(dataset, log=False, h_score = False)\n",
    "                \n",
    "        if len(data.y.shape) > 1:\n",
    "            data.y = data.y.argmax(dim=1)        \n",
    "            num_classes = torch.max(data.y).item()+1\n",
    "        else:\n",
    "            num_classes = dataset.num_classes        \n",
    "        \n",
    "        \n",
    "#         tr = int(data.train_mask.sum()) / data.num_nodes\n",
    "#         va = int(data.val_mask.sum()) / data.num_nodes\n",
    "#         te = int(data.test_mask.sum()) / data.num_nodes\n",
    "        \n",
    "#         print(f'tr/va/te {tr:0.2f}/{va:0.2f}/{te:0.2f}')\n",
    "\n",
    "\n",
    "#         f = dataset.num_features\n",
    "#         c = dataset.num_classes \n",
    "#         i = \"Yes\" if data.has_isolated_nodes() else \"No\"\n",
    "#         sl = \"Yes\" if data.has_self_loops() else \"No\"\n",
    "#         direc = \"Yes\" if data.is_undirected() else \"No\"\n",
    "#         print(f, \" \", c, \" \", i, \" \",sl, \" \", direc)\n",
    "        \n",
    "#         h_adj = adj_homophily(data, num_classes, log=False)        \n",
    "#         print('h_adj ',h_adj)\n",
    "        \n",
    "        #print(data.y)        \n",
    "#         try:\n",
    "#             count, score = test_uniformity(data, dataset.num_classes, log=False) #print(count, score)            \n",
    "#         except:\n",
    "#             score = -1        \n",
    "#         print(' h_uni ', score, end = ' ')\n",
    "#         try:\n",
    "#             total_en, en_score = total_entropy(data, dataset.num_classes, log=False) #print(total_en, en_score)\n",
    "#         except:\n",
    "#             en_score = -1\n",
    "#         print(' h_ent ', en_score)\n",
    "        \n",
    "#         if data.num_nodes>10001:\n",
    "#             print(\"Hagg_aff \",-1, \" Hagg_lap \", -1)\n",
    "#         else:\n",
    "#             Hagg_aff = agg_homophily(data, matrix_tu = 'affinity')\n",
    "#             Hagg_lap = agg_homophily(data, matrix_tu = 'laplacian')\n",
    "#             print(\"Hagg_aff \",Hagg_aff, \" Hagg_lap \", Hagg_lap)\n",
    "    \n",
    "    return \n",
    "    \n",
    "    \n",
    "# dataset_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd51b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "# dataset = PygNodePropPredDataset(name = \"ogbn-papers100M\", root = '/scratch/gilbreth/das90/Dataset/') \n",
    "\n",
    "# # split_idx = dataset.get_idx_split()\n",
    "# # train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "# # graph = dataset[0] # pyg graph object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ebc5c",
   "metadata": {},
   "source": [
    "From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]. These metrics support sparse matrix inputs. [‘nan_euclidean’] but it does not yet support sparse matrices.\n",
    "\n",
    "From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c29b0",
   "metadata": {},
   "source": [
    "# Create Synthetic Graphs of different homophily scores and degree of different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09ce9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_class(data):\n",
    "    \n",
    "    unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "#     print(unique_elements)\n",
    "#     print(counts)\n",
    "    mincount = min(counts).item()\n",
    "#     print(mincount)\n",
    "\n",
    "    subset = []\n",
    "\n",
    "    for i in unique_elements:\n",
    "        indexes = ((data.y == i).nonzero()).view(-1).numpy()\n",
    "#         print(len(indexes))\n",
    "#         print(indexes)\n",
    "        samples = np.random.choice(indexes, mincount, replace=False)\n",
    "        subset.extend(samples)\n",
    "\n",
    "#     print(len(subset), mincount*num_classes)\n",
    "\n",
    "    node_idx = torch.tensor(subset)\n",
    "    edge_index = subgraph(node_idx, data.edge_index)[0]\n",
    "    \n",
    "#     print(node_idx, edge_index)\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "\n",
    "    data.num_nodes = node_idx.size(0)\n",
    "    data.edge_index = edge_index\n",
    "\n",
    "    for key, item in data:\n",
    "        if key in ['edge_index', 'num_nodes']:\n",
    "            continue\n",
    "        if isinstance(item, torch.Tensor) and item.size(0) == N:\n",
    "            data[key] = item[node_idx]\n",
    "        elif isinstance(item, torch.Tensor) and item.size(0) == E:\n",
    "            data[key] = item[edge_idx]\n",
    "        else:\n",
    "            data[key] = item\n",
    "    \n",
    "    return data\n",
    "\n",
    "# print(data.edge_index)\n",
    "# data = balance_class(data)\n",
    "# print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad160506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic(data, d=5, h=0.8, train=0.6, random_state=None, log=True, balance = False):\n",
    "    \n",
    "    if balance:\n",
    "        data = balance_class(data)\n",
    "        \n",
    "    num_class = max(data.y)+1\n",
    "    cluster_vertices = {}\n",
    "    for c in range(num_class):\n",
    "        indices = torch.where(data.y == c)[0]\n",
    "        cluster_vertices[c]=indices\n",
    "    \n",
    "    n = data.num_nodes\n",
    "    \n",
    "#     intra_d = np.random.multinomial(n*d*h, np.ones(n)/n, size=1)[0]\n",
    "#     inter_d = np.random.multinomial(n*d*(1-h), np.ones(n)/n, size=1)[0]\n",
    "    \n",
    "    intra_d = np.round(np.ones(n)*(d*h)).astype(int)\n",
    "    inter_d = np.round(np.ones(n)*(d*(1-h))).astype(int)\n",
    "    \n",
    "#     print(intra_d, inter_d)\n",
    "    \n",
    "    edge_index = [[],[]]\n",
    "    \n",
    "    for c in range(num_class):\n",
    "        intra_vertices = cluster_vertices[c]\n",
    "        inter_vertices = torch.cat([value for key, value in cluster_vertices.items() if key!=c])\n",
    "        \n",
    "        intra_vertices = intra_vertices.numpy()\n",
    "        inter_vertices = inter_vertices.numpy()\n",
    "        \n",
    "#         print('Class:', c)\n",
    "#         print(intra_vertices)\n",
    "#         print(inter_vertices)\n",
    "        \n",
    "        for u in intra_vertices:\n",
    "            \n",
    "            ## remove self-loop\n",
    "            #intra_vertices_u = \n",
    "            \n",
    "            intra_v = np.random.choice(intra_vertices, min(len(intra_vertices),intra_d[u]), replace=False)\n",
    "            inter_v = np.random.choice(inter_vertices, min(len(inter_vertices),inter_d[u]), replace=False)\n",
    "            \n",
    "            Vs = np.append(intra_v,inter_v)\n",
    "            Us = np.repeat(u,len(Vs))\n",
    "            \n",
    "            unique_elements, counts = np.unique(inter_v, return_counts=True)\n",
    "            \n",
    "#             print(\"-\"*50)\n",
    "#             print(u)\n",
    "#             print(Vs)\n",
    "#             print(unique_elements)\n",
    "#             print(counts)\n",
    "#             print(\"-\"*50)\n",
    "            \n",
    "#             if len(unique_elements)< (num_class-1):\n",
    "#                 print('un du toa:')\n",
    "#                 print(unique_elements)\n",
    "#                 print(counts)\n",
    "            \n",
    "            edge_index[0].extend(Us)\n",
    "            edge_index[1].extend(Vs)\n",
    "             \n",
    "#             edge_index[1].extend(Us)\n",
    "#             edge_index[0].extend(Vs)\n",
    "    \n",
    "    data.edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "    \n",
    "    if log:\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='node'):0.4f}\",end=' ')\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='edge'):0.4f}\",end=' ')\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='edge_insensitive'):0.4f}\",end=' ')\n",
    "        print(f\"{assortativity(data.edge_index):0.4f}\", end=' ')\n",
    "        \n",
    "        unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "        print(unique_elements.tolist(), counts.tolist(), end=' ')\n",
    "\n",
    "        print(f'{int(data.train_mask.sum()) / data.num_nodes:.4f}', end=' ')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# data = generate_synthetic(data, d=10, h=0.0, train=0.6, random_state=None, log=True, balance = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24e2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e71a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic2homophily(data, d=5, h1=0.2, h2=0.8, ratio=0.5, train=0.6, random_state=None, log=True, balance = False):\n",
    "    \n",
    "    if balance:\n",
    "        data = balance_class(data)\n",
    "        \n",
    "    num_class = max(data.y)+1\n",
    "    cluster_vertices = {}\n",
    "    for c in range(num_class):\n",
    "        indices = torch.where(data.y == c)[0]\n",
    "        cluster_vertices[c]=indices\n",
    "    \n",
    "    n = data.num_nodes\n",
    "    \n",
    "#     intra_d = np.random.multinomial(n*d*h, np.ones(n)/n, size=1)[0]\n",
    "#     inter_d = np.random.multinomial(n*d*(1-h), np.ones(n)/n, size=1)[0]\n",
    "    \n",
    "    intra_d1 = np.round(np.ones(n)*(d*h1)).astype(int)\n",
    "    inter_d1 = np.round(np.ones(n)*(d*(1-h1))).astype(int)\n",
    "    \n",
    "    intra_d2 = np.round(np.ones(n)*(d*h2)).astype(int)\n",
    "    inter_d2 = np.round(np.ones(n)*(d*(1-h2))).astype(int)\n",
    "        \n",
    "    \n",
    "#     print(intra_d, inter_d)\n",
    "    \n",
    "    edge_index = [[],[]]\n",
    "    \n",
    "    for c in range(num_class):\n",
    "        intra_vertices = cluster_vertices[c]\n",
    "        inter_vertices = torch.cat([value for key, value in cluster_vertices.items() if key!=c])\n",
    "        \n",
    "        intra_vertices = intra_vertices.numpy()\n",
    "        inter_vertices = inter_vertices.numpy()\n",
    "        \n",
    "#         print('Class:', c)\n",
    "#         print(intra_vertices)\n",
    "#         print(inter_vertices)\n",
    "        \n",
    "        for u in intra_vertices:        \n",
    "            \n",
    "            ## remove self-loop\n",
    "            #intra_vertices_u = \n",
    "            \n",
    "            if random.random()<ratio:            \n",
    "                intra_v = np.random.choice(intra_vertices, min(len(intra_vertices),intra_d1[u]), replace=False)\n",
    "                inter_v = np.random.choice(inter_vertices, min(len(inter_vertices),inter_d1[u]), replace=False)\n",
    "            else:\n",
    "                intra_v = np.random.choice(intra_vertices, min(len(intra_vertices),intra_d2[u]), replace=False)\n",
    "                inter_v = np.random.choice(inter_vertices, min(len(inter_vertices),inter_d2[u]), replace=False)\n",
    "            \n",
    "            Vs = np.append(intra_v,inter_v)\n",
    "            Us = np.repeat(u,len(Vs))\n",
    "            \n",
    "            unique_elements, counts = np.unique(inter_v, return_counts=True)\n",
    "            \n",
    "#             print(\"-\"*50)\n",
    "#             print(u)\n",
    "#             print(Vs)\n",
    "#             print(unique_elements)\n",
    "#             print(counts)\n",
    "#             print(\"-\"*50)\n",
    "            \n",
    "#             if len(unique_elements)< (num_class-1):\n",
    "#                 print('un du toa:')\n",
    "#                 print(unique_elements)\n",
    "#                 print(counts)\n",
    "            \n",
    "            edge_index[0].extend(Us)\n",
    "            edge_index[1].extend(Vs)\n",
    "             \n",
    "#             edge_index[1].extend(Us)\n",
    "#             edge_index[0].extend(Vs)\n",
    "    \n",
    "    data.edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "    \n",
    "    if log:\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='node'):0.4f}\",end=' ')\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='edge'):0.4f}\",end=' ')\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='edge_insensitive'):0.4f}\",end=' ')\n",
    "        print(f\"{assortativity(data.edge_index):0.4f}\", end=' ')\n",
    "        \n",
    "        unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "        print(unique_elements.tolist(), counts.tolist(), end=' ')\n",
    "\n",
    "        print(f'{int(data.train_mask.sum()) / data.num_nodes:.4f}', end=' ')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# data, dataset = get_data('karate', log=False, h_score = False, split_no = 0)\n",
    "# data = generate_synthetic2homophily(data, d=10, h1=0.05, h2=0.25, ratio=0.5, train=0.6, random_state=None, log=True, balance = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4961d5",
   "metadata": {},
   "source": [
    "## Adjusted Homophily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91b7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_homophily(data,num_classes,log=True):\n",
    "    h_edge = homophily(data.edge_index, data.y, method='edge')\n",
    "    h_adj = -1\n",
    "    E = data.num_edges\n",
    "    N = data.num_nodes\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    class_degree=np.zeros(num_classes)\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "        \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            if log:\n",
    "                pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        y_current = data.y[i].item()\n",
    "        class_degree[y_current] += len(col)        \n",
    "        \n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:        \n",
    "        pbar.close()\n",
    "        \n",
    "    D_k = np.sum(class_degree**2)/E**2\n",
    "    \n",
    "    #print(D_k)\n",
    "    \n",
    "    h_adj = (h_edge - D_k)/(1-D_k)\n",
    "    \n",
    "    return h_adj\n",
    "\n",
    "# data, dataset = get_data('Tolokers', log=False, h_score = False)\n",
    "# adj_homophily(data, dataset.num_classes, log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ff61e",
   "metadata": {},
   "source": [
    "## Homophily plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "187afc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_compute(data):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    hp_data=np.zeros(N)\n",
    "    \n",
    "    pbar = tqdm(total=N)\n",
    "    pbar.set_description(f'Nodes')\n",
    "        \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        y_current = data.y[i]\n",
    "        y_neighbors = data.y[col]\n",
    "        \n",
    "        match  = (y_neighbors==y_current).type(torch.int).sum()\n",
    "        \n",
    "        hp_data[i] = match.item()/len(y_neighbors)\n",
    "        \n",
    "        #print(y_current, y_neighbors, match, hp_data[i])\n",
    "        \n",
    "        pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    return hp_data\n",
    "\n",
    "#hp_data = hp_compute(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef941fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def custom_formatter(value, _):\n",
    "    if value == 0:\n",
    "        return \"0\"  # or any other representation for zero\n",
    "    # Format the tick label as \"1x10^exponent\"\n",
    "    return f\"1x10^{int(np.log10(value))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c7d5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rcParams[\"font.family\"] = 'DeJavu Serif'\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b428401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter, StrMethodFormatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16e102dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# https://github.com/matplotlib/matplotlib/issues/5862#issuecomment-197330145\n",
    "def fix_eps(fpath):\n",
    "    \"\"\"Fix carriage returns in EPS files caused by Arial font.\"\"\"\n",
    "    txt = b\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        for line in f:\n",
    "            if b\"\\r\\rHebrew\" in line:\n",
    "                line = line.replace(b\"\\r\\rHebrew\", b\"Hebrew\")\n",
    "            txt += line\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        f.write(txt)\n",
    "            \n",
    "def pd_hist(data, DATASET_NAME=''):\n",
    "    \n",
    "#     plt.rcParams[\"font.family\"] = \"serif\"\n",
    "#     plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "    \n",
    "    width = 5\n",
    "    font_size = 16\n",
    "    \n",
    "    \n",
    "    plt.rc('font', size=font_size)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=font_size)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=font_size)     # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=font_size)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=font_size)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=font_size)    # legend fontsize\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize = (5, 5), dpi=150);\n",
    "    ax = plt.gca();\n",
    "    ax.set_aspect('auto')\n",
    "    fig.canvas.draw();      \n",
    "    \n",
    "    # Generate some random data\n",
    "    #data = np.random.normal(size=1000)\n",
    "    # Calculate the probability density function\n",
    "    density, bins, _ = plt.hist(data, density=False, bins=25)\n",
    "\n",
    "    # Plot the probability density function\n",
    "    plt.plot(bins[:-1], density)\n",
    "    \n",
    "    font = {'fontname':'Times New Roman', 'size':font_size}\n",
    "    \n",
    "    #ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Local Node homophily')\n",
    "    plt.ylabel('Number of Nodes')\n",
    "    #plt.title('Histogram of node homophily values'+': '+DATASET_NAME)\n",
    "#     plt.ticklabel_format(axis='y', style='sci', scilimits=(0,1),useMathText=True)\n",
    "\n",
    "    class MathTextSciFormatter(ticker.Formatter):\n",
    "        def __init__(self, fmt=\"%1.1e\"):\n",
    "            self.fmt = fmt\n",
    "        def __call__(self, x, pos=None):\n",
    "            s = self.fmt % x\n",
    "            decimal_point = '.'\n",
    "            positive_sign = '+'\n",
    "            tup = s.split('e')\n",
    "            significand = tup[0].rstrip(decimal_point)\n",
    "            sign = tup[1][0].replace(positive_sign, '')\n",
    "            exponent = tup[1][1:].lstrip('0')\n",
    "            if exponent:\n",
    "                exponent = '10^{%s%s}' % (sign, exponent)\n",
    "            if significand and exponent:\n",
    "                s =  r'%s{\\times}%s' % (significand, exponent)\n",
    "            else:\n",
    "                s =  r'%s%s' % (significand, exponent)\n",
    "            return \"${}$\".format(s)\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(MathTextSciFormatter(\"%1.1e\"))\n",
    "\n",
    "\n",
    "    filename=\"Plots/homophily_\"+DATASET_NAME\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show();\n",
    "    fig.savefig(filename + '.pdf', format = 'pdf', bbox_inches='tight');\n",
    "    fig.savefig(filename + '.eps', format = 'eps', bbox_inches='tight', dpi = fig.dpi);\n",
    "    fix_eps(filename + '.eps');\n",
    "    \n",
    "    return \n",
    "\n",
    "# hp_data = [0.01, 0.1,0.6]\n",
    "# pd_hist(hp_data, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59ad726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # datasets = ['Cora', 'karate']\n",
    "# for DATASET_NAME in datasets:\n",
    "#     data, dataset = get_data(DATASET_NAME, log=False, h_score = False)\n",
    "#     hp_data = hp_compute(data)\n",
    "#     pd_hist(hp_data, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ff0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ed600e8",
   "metadata": {},
   "source": [
    "## Gephi Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d58fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "# from ipynb.fs.full.utils.GNNutils import save_gephi_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d54f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gephi_graph(data, DATASET_NAME):\n",
    "    G_fillename = '/scratch/gilbreth/das90/Dataset/GephiGraphs/'+DATASET_NAME\n",
    "\n",
    "    if os.path.exists(G_fillename)==False:\n",
    "        print(\"Graph is not found, creating it....\")\n",
    "        G = to_networkx(data, to_undirected=True)\n",
    "        nx.write_gpickle(G, G_fillename)\n",
    "        print(\"Done\")\n",
    "    else:\n",
    "        print(\"Loading Saved graph...\")\n",
    "        G = nx.read_gpickle(G_fillename)\n",
    "        print(\"Done\")\n",
    "    \n",
    "    graph_name = '/scratch/gilbreth/das90/Dataset/GephiGraphs/'+DATASET_NAME+'_original'\n",
    "    save_gephi_graph(G, data.y, graph_name)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24b9bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for DATASET_NAME in datasets:\n",
    "#     #DATASET_NAME = 'karate'\n",
    "#     print('-'*20,DATASET_NAME,'-'*20)\n",
    "#     data, dataset = get_data(DATASET_NAME)\n",
    "#     if data.y.ndim >1:\n",
    "#         data.y = data.y.argmax(dim=1)\n",
    "#     create_gephi_graph(data,DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "183f4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_gephi_graph(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e465dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# data = Data(x=x, y=y, edge_index = edge_index)\n",
    "# draw_graph(edge_index, y, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b4ffa",
   "metadata": {},
   "source": [
    "## Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6453e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c99739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'chameleon'\n",
    "# data, dataset = get_data(DATASET_NAME, log = False)\n",
    "# data = generate_synthetic(data, d=10, h=0.0, train=0.6, random_state=1, log=True, balance = False)\n",
    "# num_classes = dataset.num_classes\n",
    "# print(num_classes)\n",
    "# N = data.num_nodes\n",
    "# E = data.num_edges\n",
    "\n",
    "# adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "#     value=torch.arange(E, device=data.edge_index.device),\n",
    "#     sparse_sizes=(N, N))\n",
    "\n",
    "# # if len(col)==0:\n",
    "# #     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5243975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_uniformity(col_labels, num_classes, cur_y, log=True, ):    \n",
    "    if log:\n",
    "        print(col_labels)\n",
    "        print(cur_y)\n",
    "        \n",
    "    unique_elements, counts = np.unique(col_labels, return_counts=True)\n",
    "    \n",
    "    if log:\n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "    \n",
    "    index = np.where(unique_elements == cur_y)[0]\n",
    "    unique_elements = np.delete(unique_elements, index)\n",
    "    counts = np.delete(counts, index)\n",
    "\n",
    "    if log: \n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "        \n",
    "    if len(unique_elements)<(num_classes-1):\n",
    "        if log:\n",
    "            print('unexpted: ')\n",
    "            print(unique_elements)\n",
    "            print(counts)\n",
    "        return -1\n",
    "    \n",
    "    expected_frequency = sum(counts) / (num_classes-1)\n",
    "    observed_frequency = np.array(counts)\n",
    "\n",
    "    if log:\n",
    "        print(expected_frequency)\n",
    "        print(observed_frequency)\n",
    "\n",
    "    chi2, p_value = chisquare(observed_frequency, f_exp=expected_frequency)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Chi-squared statistic:\", chi2)\n",
    "        print(\"p-value:\", p_value)\n",
    "\n",
    "    significance_level = 0.05\n",
    "    if p_value < significance_level:\n",
    "        if log:print(\"The distribution significantly deviates from the uniform distribution.\")\n",
    "        return 0\n",
    "    else:\n",
    "        if log:print(\"The distribution is similar to the uniform distribution.\")\n",
    "        return 1\n",
    "\n",
    "    \n",
    "# u = 0\n",
    "# row, col, ed = adj[u,:].coo()   \n",
    "# col_labels = data.y[col]\n",
    "\n",
    "# print(row)\n",
    "# print(col)\n",
    "# print(col_labels)\n",
    "# print(len(col_labels))\n",
    "\n",
    "# cur_y = data.y[u].item()\n",
    "# print(cur_y)\n",
    "# given_uniformity(col_labels, num_classes, cur_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db6980b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uniformity(data, num_classes, log=True):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for u in range(N):            \n",
    "        row, col, edge_index = adj[u,:].coo()   \n",
    "        col_labels = data.y[col]\n",
    "        cur_y = data.y[u].item()\n",
    "        is_diverse = given_uniformity(col_labels, num_classes, cur_y, log = False)\n",
    "        \n",
    "#         if is_diverse ==-1:\n",
    "#             print(u)\n",
    "        \n",
    "        count+=max(0,is_diverse)\n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:\n",
    "        pbar.close()\n",
    "    \n",
    "    return count, count/N\n",
    "    \n",
    "# test_uniformity(data, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62059285",
   "metadata": {},
   "source": [
    "## Aggregated Homophily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ec8450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_homophily(data, matrix_tu = 'affinity'):\n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "\n",
    "    adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "\n",
    "    A = torch.zeros((N,N))\n",
    "    edges = data.edge_index.t()\n",
    "    A[edges[:,0], edges[:,1]] = 1\n",
    "    A[edges[:,1], edges[:,0]] = 1    \n",
    "    \n",
    "    I = torch.eye(N)\n",
    "    D = torch.diag(torch.Tensor(torch.sum(A,dim=1)))\n",
    "#     print(A)\n",
    "#     print(D)\n",
    "\n",
    "    Ai = A + I\n",
    "    DiInv = torch.diag(torch.Tensor(1/torch.sum(Ai,dim=1)))\n",
    "    Arw = torch.mm(DiInv, Ai)\n",
    "    \n",
    "    M = Arw\n",
    "    \n",
    "    if matrix_tu == 'affinity':\n",
    "        M = Arw\n",
    "    elif matrix_tu == 'laplacian':\n",
    "        M = I - Arw\n",
    "    \n",
    "    AX = torch.mm(M,data.x)\n",
    "    #print(AX)\n",
    "    SAX = torch.mm(AX,AX.T)    \n",
    "    #print(SAX)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for u in range(N):            \n",
    "        row, col, edge_index = adj[u,:].coo()\n",
    "        \n",
    "        col_labels = data.y[col]\n",
    "        cur_y = data.y[u]\n",
    "        \n",
    "#         print(col_labels)\n",
    "#         print(cur_y)\n",
    "        \n",
    "        zu_eq = col[torch.where(col_labels == cur_y)[0]]\n",
    "        zu_neq = col[torch.where(col_labels != cur_y)[0]]\n",
    "        #print(zu_eq, zu_neq)\n",
    "        \n",
    "        left_u = torch.mean(SAX[u,zu_eq])\n",
    "        right_u = torch.mean(SAX[u,zu_neq])\n",
    "        #print(left_u, right_u)\n",
    "        \n",
    "        if torch.isnan(right_u):\n",
    "            count+=1        \n",
    "                            \n",
    "        elif (left_u >= right_u):\n",
    "            count+=1\n",
    "        \n",
    "    return count/N\n",
    "    \n",
    "    \n",
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME, log=False)\n",
    "# data = generate_synthetic(data, d=10, h = 0.2, train=0.6, random_state=1, log=False, balance = True)\n",
    "# data.x = F.one_hot(data.y).float()\n",
    "\n",
    "# print(agg_homophily(data, 'affinity'))\n",
    "# print(agg_homophily(data, 'laplacian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d60ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME, log=False)\n",
    "# data = generate_synthetic(data, d=10, h = 0.2, train=0.6, random_state=1, log=False, balance = True)\n",
    "\n",
    "# num_classes = dataset.num_classes\n",
    "# print(num_classes)\n",
    "# N = data.num_nodes\n",
    "# E = data.num_edges\n",
    "\n",
    "# adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "#     value=torch.arange(E, device=data.edge_index.device),\n",
    "#     sparse_sizes=(N, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6a7ed",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "714d0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def scipyentropy(labels, num_class = 10):\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_probs = class_counts / len(labels)\n",
    "    \n",
    "    print(class_probs)\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy_value = entropy(class_probs, base=2)\n",
    "    print(\"Entropy:\", entropy_value)\n",
    "    \n",
    "    return entropy_value\n",
    "\n",
    "# labels = [0, 1, 2, 0, 1, 1, 3, 2, 2, 4, 4, 5, 5, 6, 6, 6, 6]\n",
    "# scipyentropy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd252a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_entropy(class_probabilities, num_classes):\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for probability in class_probabilities:\n",
    "        if probability != 0.0:\n",
    "            entropy += -probability * math.log2(probability)\n",
    "\n",
    "    max_entropy = -math.log2(1/num_classes)\n",
    "    \n",
    "    #print(max_entropy)    \n",
    "    normalized_entropy = entropy / max_entropy\n",
    "    \n",
    "    return normalized_entropy\n",
    "\n",
    "# Example usage\n",
    "# class_probabilities = [0.9, 0.0, 0.0, 0.0, 0.1]\n",
    "# entropy_value = compute_entropy(class_probabilities, len(class_probabilities))\n",
    "# print(\"Normalized Entropy:\", entropy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da55313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_entropy(col_labels, num_classes, cur_y, log=True):    \n",
    "    if log:\n",
    "        print(col_labels)\n",
    "        print(cur_y)\n",
    "        \n",
    "    unique_elements, counts = np.unique(col_labels, return_counts=True)\n",
    "    \n",
    "    if log:\n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "    \n",
    "    index = np.where(unique_elements == cur_y)[0]\n",
    "    unique_elements = np.delete(unique_elements, index)\n",
    "    counts = np.delete(counts, index)\n",
    "\n",
    "    if log: \n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "        \n",
    "    \n",
    "    prob = counts/sum(counts)\n",
    "    if log: print(prob)\n",
    "    \n",
    "    return compute_entropy(prob, num_classes-1)\n",
    "\n",
    "\n",
    "# u = 0\n",
    "# row, col, ed = adj[u,:].coo()   \n",
    "# col_labels = data.y[col]\n",
    "\n",
    "# print(row)\n",
    "# print(col)\n",
    "# print(col_labels)\n",
    "# print(len(col_labels))\n",
    "\n",
    "# cur_y = data.y[u].item()\n",
    "# print(cur_y)\n",
    "# node_entropy(col_labels, num_classes, cur_y, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b30d900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_entropy(data, num_classes, log=True):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for u in range(N):            \n",
    "        row, col, edge_index = adj[u,:].coo()   \n",
    "        col_labels = data.y[col]\n",
    "        cur_y = data.y[u].item()\n",
    "        is_diverse = node_entropy(col_labels, num_classes, cur_y, log = False)\n",
    "        \n",
    "#         if is_diverse ==-1:\n",
    "#             print(u)\n",
    "        \n",
    "        count+= is_diverse\n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:\n",
    "        pbar.close()\n",
    "    \n",
    "    return count, count/N\n",
    "\n",
    "# total_entropy(data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ada4684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hetero():\n",
    "    d = 20\n",
    "    for h in np.array(range(0,21))/20:\n",
    "        DATASET_NAME = 'Cora'\n",
    "        data, dataset = get_data(DATASET_NAME, log=False)\n",
    "        data = generate_synthetic(data, d=d, h = h, train=0.6, random_state=1, log=False, balance = True)\n",
    "        num_classes = dataset.num_classes\n",
    "        print('d ', d, ' h', h, end=' ')\n",
    "        count, score = test_uniformity(data, num_classes, log=False)\n",
    "        print(count, score, end = ' ')\n",
    "#         Hagg_aff = agg_homophily(data, matrix_tu = 'affinity')\n",
    "#         Hagg_lap = agg_homophily(data, matrix_tu = 'laplacian')\n",
    "#         print(Hagg_aff, Hagg_lap)\n",
    "        total_en, en_score = total_entropy(data, num_classes, log=False)\n",
    "        print(total_en, en_score)\n",
    "    \n",
    "# test_hetero()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
