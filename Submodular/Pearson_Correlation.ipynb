{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c10580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29f49e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import torch_geometric.utils.subgraph as subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebd26850",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datsets = [\n",
    "    \"Cornell\",\n",
    "    \"Texas\",\n",
    "    \"Wisconsin\",\n",
    "    \"reed98\",\n",
    "    \"amherst41\",\n",
    "    \"penn94\",\n",
    "    \"Roman-empire\",\n",
    "    \"cornell5\",\n",
    "    \"Squirrel\",\n",
    "    \"johnshopkins55\",\n",
    "    \"AmazonProducts\",\n",
    "    \"Actor\",\n",
    "    \"Minesweeper\",\n",
    "    \"Questions\",\n",
    "    \"Chameleon\",\n",
    "    \"Tolokers\",\n",
    "    \"Flickr\",\n",
    "    \"Yelp\",\n",
    "    \"Amazon-ratings\",\n",
    "    \"genius\",\n",
    "    \"cora\",\n",
    "    \"CiteSeer\",\n",
    "    \"dblp\",\n",
    "    \"Computers\",\n",
    "    \"pubmed\",\n",
    "    \"Reddit\",\n",
    "    \"cora_ml\",\n",
    "    \"Cora\",\n",
    "    \"Reddit2\",\n",
    "    \"CS\",\n",
    "    \"Photo\",\n",
    "    \"Physics\",\n",
    "    \"citeseer\"\n",
    "]\n",
    "# test_datsets = ['Texas','Cornell','Wisconsin','Squirrel','Chameleon','Cora']\n",
    "# test_datsets = ['Texas']\n",
    "test_datasets = ['pokec','arxiv-year','snap-patents','twitch-gamer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0e51d",
   "metadata": {},
   "source": [
    "# Pearson coffeicient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4acd3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import time\n",
    "import math\n",
    "\n",
    "def pearson_coff(features, labels, sim_score='cosine'):\n",
    "   \n",
    "    # Simulated data (replace with your actual data)\n",
    "    # features = torch.rand((100, 128))  # 100 data points with 128-dimensional features\n",
    "    # labels = torch.randint(0, 2, (100,))  # Binary labels\n",
    "\n",
    "#     features = data.x\n",
    "#     labels = data.y\n",
    "\n",
    "    # Compute similarity scores (using cosine similarity in this example)\n",
    "    \n",
    "    if sim_score == 'cosine':\n",
    "        similarity_scores = cosine_similarity(features)\n",
    "    elif sim_score == 'euclidean':\n",
    "        similarity_scores = -1*euclidean_distances(features)\n",
    "    else:\n",
    "        similarity_scores = sim_score\n",
    "        \n",
    "        \n",
    "#     print(similarity_scores.shape)\n",
    "    \n",
    "    \n",
    "    # Initialize lists to store similarity scores and label matches\n",
    "    similarity_values = []\n",
    "    label_matches = []\n",
    "\n",
    "    # Compute similarity values and label matches\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1, len(features)):\n",
    "            similarity_values.append(similarity_scores[i, j])\n",
    "            label_matches.append(int(labels[i] == labels[j]))\n",
    "\n",
    "    # Calculate Pearson's correlation coefficient\n",
    "    correlation, _ = pearsonr(similarity_values, label_matches)\n",
    "\n",
    "#     # Plot the correlation\n",
    "#     plt.scatter(similarity_values, label_matches, alpha=0.5)\n",
    "#     plt.title(f'Pearson Correlation: {correlation:.2f}')\n",
    "#     plt.xlabel('Feature Similarity')\n",
    "#     plt.ylabel('Label Match (1 if same, 0 if different)')\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "\n",
    "# test_datsets = ['Texas','Cornell','Wisconsin','Squirrel','Chameleon','Cora']\n",
    "# test_datsets = ['Texas']\n",
    "test_datasets = ['pokec','arxiv-year','snap-patents','twitch-gamer']\n",
    "\n",
    "\n",
    "def node_pearson():\n",
    "\n",
    "    max_select = 5000\n",
    "    balance = True\n",
    "\n",
    "    for dataset_name in test_datsets:    \n",
    "        data, dataset = get_data(dataset_name, log=False, h_score = False, split_no = 0)\n",
    "\n",
    "\n",
    "        if data.num_nodes>max_select:\n",
    "\n",
    "            if balance:\n",
    "                num_class = torch.max(data.y)+1\n",
    "\n",
    "                input_nodeidx = torch.arange(data.num_nodes)\n",
    "\n",
    "                clusters = [[] for i in range(num_class)]\n",
    "\n",
    "                for i in input_nodeidx:\n",
    "                    clusters[data.y[i]].append(i.item())\n",
    "\n",
    "                for i in range(num_class):\n",
    "                    clusters[i] = torch.LongTensor(clusters[i])\n",
    "\n",
    "                indices = torch.LongTensor([])\n",
    "                per_class = int(math.ceil((max_select/num_class)))\n",
    "\n",
    "                for i in range(num_class):           \n",
    "\n",
    "                    if len(clusters[i])== 0:\n",
    "                        continue                \n",
    "                    indx = torch.randint(len(clusters[i]), (per_class, ))\n",
    "                    indx= clusters[i][indx]\n",
    "                    indices = torch.cat((indices,indx))                \n",
    "            else:\n",
    "                indices = torch.randint(data.num_nodes, (max_select, ))\n",
    "\n",
    "            features = data.x[indices]\n",
    "            labels = data.y[indices]\n",
    "        else:\n",
    "            features = data.x\n",
    "            labels = data.y\n",
    "\n",
    "        start = time.time()\n",
    "        print(dataset_name,\"\\t\",pearson_coff(features, labels, sim_score='euclidean'), end='\\t')\n",
    "        end = time.time()\n",
    "        print(\"computation time:\", end-start)\n",
    "        \n",
    "# node_pearson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d65bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp\n",
    "\n",
    "\n",
    "def pearson_coff_edges(data, link_batch_size=4096, sim_score='cosine', log = True):\n",
    "    \n",
    "    similarity_values = []\n",
    "    label_matches = []\n",
    "    \n",
    "    indices = torch.arange(0, data.edge_index.shape[1])\n",
    "    batches = torch.split(indices, link_batch_size)\n",
    "    \n",
    "    func = cosine_similarity\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=len(indices))\n",
    "        pbar.set_description(f'Batch')\n",
    "    \n",
    "    for batch in batches:\n",
    "        idx = data.edge_index[:,batch]\n",
    "        x = data.x[idx[0]]\n",
    "        y = data.x[idx[1]]\n",
    "        \n",
    "        #print(x.shape, y.shape)\n",
    "                \n",
    "        if sim_score=='cosine':\n",
    "            sim = torch.cosine_similarity(x,y, dim=1).tolist()                        \n",
    "        elif sim_score == 'euclidean':\n",
    "            sim = -1*torch.nn.PairwiseDistance(p=2)(x, y)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        \n",
    "        #print(sim)\n",
    "        similarity_values.extend(sim)        \n",
    "                \n",
    "        l_sim = (data.y[idx[0]] == data.y[idx[1]]).type(torch.int).tolist()\n",
    "        \n",
    "        #print(l_sim)\n",
    "        \n",
    "        label_matches.extend(l_sim)\n",
    "        \n",
    "        if log:\n",
    "            pbar.update(len(batch))\n",
    "    \n",
    "    if log:\n",
    "        pbar.close()\n",
    "\n",
    "    correlation = 0\n",
    "    \n",
    "    #print(similarity_values, label_matches)\n",
    "        \n",
    "    correlation, _ = pearsonr(similarity_values, label_matches)\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "\n",
    "\n",
    "# test_datsets = ['karate']\n",
    "\n",
    "# for dataset_name in test_datsets:\n",
    "    \n",
    "#     data, dataset = get_data(dataset_name, log=False, h_score = False, split_no = 0)\n",
    "#     start = time.time()\n",
    "#     print(dataset_name,\"\\t\",pearson_coff_edges(data, link_batch_size =4096*8, sim_score='euclidean', log = False), end='\\t')\n",
    "#     end = time.time()\n",
    "#     print(\"computation time:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcda34",
   "metadata": {},
   "source": [
    "# Link prediction homophily vs label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14aafd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.PretrainedLinkFast import train_link, args, device, DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cba31a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.Dataset import pearson_coff\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def normalize_rows(arr):\n",
    "    row_min = arr.min(axis=1, keepdims=True)\n",
    "    row_max = arr.max(axis=1, keepdims=True)\n",
    "    return (arr - row_min) / (row_max - row_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "129f0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(DATASET_NAME, log = False):\n",
    "    data, dataset = get_data(DATASET_NAME, log=False, h_score=log, split_no = 0)\n",
    "    #data.train_mask = data.train_mask|data.val_mask|data.test_mask\n",
    "    #data.train_mask = data.train_mask|data.val_mask\n",
    "    \n",
    "    args.balance = True\n",
    "    \n",
    "    if data.num_nodes<10000:\n",
    "        args.epochs = 20\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096\n",
    "        args.link_num_steps = 200\n",
    "\n",
    "    elif data.num_nodes<100000:\n",
    "        args.epochs = 10\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096*2\n",
    "        args.link_num_steps = 200\n",
    "    else:\n",
    "        args.epochs = 5\n",
    "        args.num_neurons = 32\n",
    "        args.link_batch_size = 4096*8\n",
    "        args.link_num_steps = 200\n",
    "        \n",
    "    worker = 0\n",
    "#     if data.x.shape[1]>1000:\n",
    "#         worker = 8\n",
    "    \n",
    "    link_model = train_link(data, selfloop = True, log = log, worker = worker)\n",
    "    \n",
    "    return data, dataset, link_model\n",
    "\n",
    "def link_correlation(DATASET_NAME, log = False):\n",
    "    \n",
    "    data, dataset, link_model = get_model(DATASET_NAME, log = False)\n",
    "    \n",
    "    max_select = 5000\n",
    "    \n",
    "    if data.num_nodes>max_select:\n",
    "        \n",
    "        if args.balance:\n",
    "            num_class = torch.max(data.y)+1\n",
    "            \n",
    "            input_nodeidx = torch.arange(data.num_nodes)\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 input_nodeidx = torch.nonzero(data.train_mask).flatten()            \n",
    "#             else:\n",
    "#                 input_nodeidx = torch.arange(data.num_nodes)\n",
    "            \n",
    "            clusters = [[] for i in range(num_class)]\n",
    "\n",
    "            for i in input_nodeidx:\n",
    "                clusters[data.y[i]].append(i.item())\n",
    "\n",
    "            for i in range(num_class):\n",
    "                clusters[i] = torch.LongTensor(clusters[i])\n",
    "                \n",
    "#             if log:\n",
    "#                 print(clusters)\n",
    "                            \n",
    "            indices = torch.LongTensor([])\n",
    "            per_class = int(math.ceil((max_select/num_class)))\n",
    "            \n",
    "            for i in range(num_class): \n",
    "                \n",
    "                if len(clusters[i])== 0:\n",
    "                    continue\n",
    "                    \n",
    "                indx = torch.randint(len(clusters[i]), (per_class, ))\n",
    "                indx= clusters[i][indx]\n",
    "                indices = torch.cat((indices,indx))                \n",
    "        else:\n",
    "            indices = torch.randint(data.num_nodes, (max_select, ))\n",
    "            \n",
    "#         if log:\n",
    "#             print(indices)\n",
    "        \n",
    "        features = data.x[indices]\n",
    "        labels = data.y[indices]\n",
    "    else:\n",
    "        features = data.x\n",
    "        labels = data.y\n",
    "        \n",
    "    \n",
    "#     print(\"features: \", features)\n",
    "#     print(\"labels: \",labels)\n",
    "    \n",
    "    similarity_values = []\n",
    "    label_matches = []\n",
    "    \n",
    "    feature_matrix = features.to(device)\n",
    "    N = feature_matrix.shape[0]\n",
    "    A = np.zeros((N,N))\n",
    "\n",
    "    #print(N)\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "\n",
    "    link_model.eval()\n",
    "    with torch.no_grad():    \n",
    "        for i in range(N):\n",
    "            x = feature_matrix[i].repeat(N, 1)\n",
    "            sim = link_model(x, feature_matrix)            \n",
    "            \n",
    "            pred = sim                        \n",
    "#             pred = torch.zeros_like(sim)\n",
    "#             pred[sim >= 0.5] = 1            \n",
    "            pred = pred.cpu().numpy().reshape(-1)            \n",
    "            A[i] = pred\n",
    "            if log:\n",
    "                pbar.update(1)\n",
    "    if log:\n",
    "        pbar.close()\n",
    "    \n",
    "    if log:\n",
    "        print(A)\n",
    "#     A = normalize_rows(A)\n",
    "#     print(A)\n",
    "    \n",
    "#     for i in range(N):\n",
    "#         for j in range(N):\n",
    "#             print(f'{A[i,j]:0.2f}',end=' ')\n",
    "#         print(\"\")\n",
    "    \n",
    "    similarity_scores = A\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "    # Compute similarity values and label matches\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1, len(features)):\n",
    "            similarity_values.append(similarity_scores[i, j])\n",
    "            label_matches.append(int(labels[i] == labels[j]))\n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:\n",
    "        pbar.close()\n",
    "    \n",
    "    \n",
    "    # Calculate Pearson's correlation coefficient\n",
    "    correlation, _ = pearsonr(similarity_values, label_matches)\n",
    "    \n",
    "    if log:\n",
    "        print(correlation)\n",
    "    \n",
    "    return correlation\n",
    "    \n",
    "#     # Plot the correlation\n",
    "#     plt.scatter(similarity_values, label_matches, alpha=0.5)\n",
    "#     plt.title(f'Pearson Correlation: {correlation:.2f}')\n",
    "#     plt.xlabel('Feature Similarity')\n",
    "#     plt.ylabel('Label Match (1 if same, 0 if different)')\n",
    "#     plt.show()\n",
    "\n",
    "# link_correlation('Reddit', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af2dca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datsets = [\n",
    "    \"karate\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d079557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all():\n",
    "    \n",
    "    max_select = 5000\n",
    "\n",
    "    for dataset_name in test_datsets:    \n",
    "        start = time.time()\n",
    "        print(dataset_name,\"\\t\",link_correlation(dataset_name, log=False), end='\\t')\n",
    "        end = time.time()\n",
    "        print(\"computation time:\", end-start)\n",
    "        \n",
    "# compute_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6880943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_coff_edges(DATASET_NAME, link_batch_size=4096, log = True):\n",
    "    \n",
    "    data, dataset, link_model = get_model(DATASET_NAME, log = log)\n",
    "    \n",
    "    similarity_values = []\n",
    "    label_matches = []\n",
    "    \n",
    "    indices = torch.arange(0, data.edge_index.shape[1])\n",
    "    batches = torch.split(indices, link_batch_size)\n",
    "    \n",
    "    func = cosine_similarity\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=len(indices))\n",
    "        pbar.set_description(f'Batch')\n",
    "    \n",
    "    link_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in batches:\n",
    "            idx = data.edge_index[:,batch]\n",
    "            x = data.x[idx[0]].to(device)\n",
    "            y = data.x[idx[1]].to(device)\n",
    "\n",
    "            sim = link_model(x,y)\n",
    "            sim = sim.view(-1).cpu().tolist()\n",
    "\n",
    "            similarity_values.extend(sim)        \n",
    "\n",
    "            l_sim = (data.y[idx[0]] == data.y[idx[1]]).type(torch.int).tolist()\n",
    "\n",
    "            #print(l_sim)\n",
    "\n",
    "            label_matches.extend(l_sim)\n",
    "\n",
    "            if log:\n",
    "                pbar.update(len(batch))\n",
    "\n",
    "        if log:\n",
    "            pbar.close()\n",
    "\n",
    "    correlation = 0\n",
    "    \n",
    "    #print(similarity_values, label_matches)\n",
    "        \n",
    "    correlation, _ = pearsonr(similarity_values, label_matches)\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "test_datsets = ['karate']\n",
    "\n",
    "def compute_edge_wise():\n",
    "    \n",
    "    for dataset_name in test_datsets:\n",
    "\n",
    "        data, dataset = get_data(dataset_name, log=False, h_score = False, split_no = 0)\n",
    "        start = time.time()\n",
    "        print(dataset_name,\"\\t\",pearson_coff_edges(dataset_name, link_batch_size =4096*8, log = True), end='\\t')\n",
    "        end = time.time()\n",
    "        print(\"computation time:\", end-start)\n",
    "    \n",
    "# compute_edge_wise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
