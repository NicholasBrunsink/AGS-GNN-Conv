{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "949d8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell\n",
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import get_data\n",
    "from torch_geometric.utils import degree\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98f191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import torch\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.typing import EdgeType, InputNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e71ff2",
   "metadata": {},
   "source": [
    "# Testing and Weighted Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68a1f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Data\n",
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# # edge_index = torch.LongTensor([[1,2]]).T\n",
    "\n",
    "\n",
    "# edge_index = edge_index-1\n",
    "# data = Data(x=x, y=y, edge_index = edge_index)\n",
    "\n",
    "\n",
    "\n",
    "# # loader = AGSNodeSampler(data, log = False, batch_size=2, num_steps=4, sample_coverage=1, save_dir='',num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c5638b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N = data.num_nodes\n",
    "# E = data.num_edges\n",
    "\n",
    "# adj = SparseTensor(\n",
    "#     row=data.edge_index[0], col=data.edge_index[1],\n",
    "#     value=torch.arange(E, device=data.edge_index.device),\n",
    "#     sparse_sizes=(N, N))\n",
    "\n",
    "# adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2afafd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj.saint_subgraph(torch.LongTensor([0,4,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "406d0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import torch\n",
    "#sys.path.append(\"/home/sferdou/CPPSamplerNew/build/src\")\n",
    "\n",
    "sys.path.append(\"/home/das90/GNNcodes/CVE2020/GNN-NC/Graph-Sparsification/CPPsamplerPy/build/src\")\n",
    "\n",
    "import sampling_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7600a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch_sparse.tensor import SparseTensor\n",
    " \n",
    "def w_random_walk_cpp(src: SparseTensor, start: torch.Tensor, prob: torch.Tensor, walk_length: int) -> torch.Tensor:\n",
    "    \n",
    "    rowptr, col, _ = src.csr()    \n",
    "    return sampling_module.weighted_random_walk(rowptr, col, start, walk_length, prob)\n",
    "\n",
    "SparseTensor.w_random_walk_cpp = w_random_walk_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20fbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7fb2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_cpu(rowptr, col, start, walk_length):\n",
    "    assert rowptr.device.type == 'cpu'\n",
    "    assert col.device.type == 'cpu'\n",
    "    assert start.device.type == 'cpu'\n",
    "\n",
    "    assert rowptr.dim() == 1\n",
    "    assert col.dim() == 1\n",
    "    assert start.dim() == 1\n",
    "\n",
    "    rand = torch.rand((start.size(0), walk_length), dtype=torch.float32, device=start.device)\n",
    "\n",
    "    L = walk_length + 1\n",
    "    out = torch.full((start.size(0), L), -1, dtype=torch.int64, device=start.device)\n",
    "\n",
    "    for n in range(start.size(0)):\n",
    "        cur = start[n]\n",
    "        out[n, 0] = cur\n",
    "\n",
    "        for l in range(walk_length):\n",
    "            row_start = rowptr[cur]\n",
    "            row_end = rowptr[cur + 1]\n",
    "            \n",
    "            random_val = rand[n, l].item()\n",
    "            selected_col_idx = int(random_val * (row_end - row_start))\n",
    "            cur = col[row_start + selected_col_idx]\n",
    "            out[n, l + 1] = cur\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c90873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_random_walk_cpu(rowptr, col, start, walk_length, prob):\n",
    "#     assert rowptr.device.type == 'cpu'\n",
    "#     assert col.device.type == 'cpu'\n",
    "#     assert start.device.type == 'cpu'\n",
    "#     assert prob.device.type == 'cpu'\n",
    "\n",
    "    assert rowptr.dim() == 1\n",
    "    assert col.dim() == 1\n",
    "    assert start.dim() == 1\n",
    "    assert prob.dim() == 1\n",
    "\n",
    "    #rand = torch.rand((start.size(0), walk_length), dtype=torch.float32, device=start.device)\n",
    "\n",
    "    L = walk_length + 1\n",
    "    out = torch.full((start.size(0), L), -1, dtype=torch.int64, device=start.device)\n",
    "\n",
    "    for n in range(start.size(0)):\n",
    "        cur = start[n]\n",
    "        out[n, 0] = cur\n",
    "\n",
    "        for l in range(walk_length):\n",
    "            row_start = rowptr[cur]\n",
    "            row_end = rowptr[cur + 1]\n",
    "            \n",
    "            if int(row_end-row_start)<1:\n",
    "                out[n, l + 1] = cur\n",
    "                continue\n",
    "                \n",
    "            #random_val = rand[n, l].item()            \n",
    "            col_probs = prob[row_start:row_end]\n",
    "            col_probs = torch.clamp(col_probs, min=0.0001)\n",
    "            col_probs = col_probs/col_probs.sum()\n",
    "            #print(col_probs)            \n",
    "            selected_col_idx = torch.multinomial(col_probs, num_samples=1, replacement=True)            \n",
    "            #selected_col_idx = int(random_val * (row_end - row_start))\n",
    "            cur = col[row_start + selected_col_idx]\n",
    "            out[n, l + 1] = cur\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e34a6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch_sparse.tensor import SparseTensor\n",
    " \n",
    "def w_random_walk(src: SparseTensor, start: torch.Tensor, prob: torch.Tensor, walk_length: int) -> torch.Tensor:\n",
    "    \n",
    "    rowptr, col, _ = src.csr()    \n",
    "    return weighted_random_walk_cpu(rowptr, col, start, walk_length, prob)\n",
    "#     return random_walk_cpu(rowptr, col, start, walk_length)\n",
    "\n",
    "SparseTensor.w_random_walk = w_random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0b4ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #adj.storage.row()[[1,7]]\n",
    "# deg_in = 1/adj.storage.rowcount()\n",
    "# print(deg_in)\n",
    "# deg_out =  1/adj.storage.colcount()\n",
    "# print(deg_out)\n",
    "# row, col, _ = adj.coo()\n",
    "\n",
    "# print(row)\n",
    "# print(col)\n",
    "# prob = (1. / deg_in[row]) + (1. / deg_out[col])\n",
    "# print(prob)\n",
    "\n",
    "\n",
    "# # prob[1]=0\n",
    "# # prob[6]=0\n",
    "\n",
    "# # prob = prob/prob.sum()\n",
    "# # print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9155920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = torch.LongTensor([0])\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(adj.w_random_walk_cpp(start, prob, 2))    \n",
    "# #     print(adj.random_walk(start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd07a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca272a",
   "metadata": {},
   "source": [
    "## Save and load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cabce350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_weight(method,save_dir,weights):\n",
    "    filename= save_dir+method+\".pt\"\n",
    "    \n",
    "    directory = osp.dirname(filename)    \n",
    "    if not osp.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    torch.save(weights, filename)\n",
    "    \n",
    "def load_weight(method, save_dir):\n",
    "    filename= save_dir+method+\".pt\"\n",
    "    if not osp.exists(filename):\n",
    "        return None\n",
    "    else:\n",
    "        return torch.load(filename)\n",
    "    \n",
    "def is_compute(recompute, method, save_dir):\n",
    "    compute=False\n",
    "    w = None\n",
    "    if recompute == True:                         \n",
    "        compute=True\n",
    "    else:\n",
    "        w = load_weight(method, save_dir)\n",
    "        if w is None:\n",
    "            compute=True            \n",
    "    return compute, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a37b6",
   "metadata": {},
   "source": [
    "# Graph Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6313a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.DisjointSparsifier import get_random_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6e8d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSSampler(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, data, batch_size: int, num_steps: int = 1,\n",
    "                 sample_coverage: int = 0, save_dir: Optional[str] = None, \n",
    "                 recompute = True,\n",
    "                 log: bool = True, \n",
    "                 sample_func: List = None,\n",
    "                 weight_func: Dict = None,\n",
    "                 params=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        if 'collate_fn' in kwargs:\n",
    "            del kwargs['collate_fn']\n",
    "\n",
    "        assert data.edge_index is not None\n",
    "        assert 'node_norm' not in data\n",
    "        assert 'edge_norm' not in data\n",
    "        assert not data.edge_index.is_cuda\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.__batch_size__ = batch_size\n",
    "        self.sample_coverage = sample_coverage\n",
    "        self.save_dir = save_dir\n",
    "        self.recompute = recompute\n",
    "        self.log = log\n",
    "\n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = data.num_edges\n",
    "\n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(self.E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        super().__init__(self, batch_size=1, collate_fn=self.__collate__,\n",
    "                         **kwargs)\n",
    "    \n",
    "        ### default norm computation\n",
    "        ### edge_weight computation\n",
    "        self.sample_func = sample_func\n",
    "        self.weight_func=weight_func\n",
    "        self.params=params        \n",
    "        self.recompute = recompute\n",
    "        \n",
    "        self.norm_computation = False        \n",
    "        self.norm_compute()\n",
    "        \n",
    "        if sample_func is not None:            \n",
    "            weights_dict = {}                    \n",
    "            for i,method in enumerate(self.sample_func):\n",
    "                weights_dict[self.weight_func[i]['weight']]=0\n",
    "                \n",
    "            #print(weights_dict)\n",
    "            \n",
    "            self.computed_weights = {}\n",
    "            \n",
    "            for key, value in weights_dict.items():                \n",
    "                if key == 'knn':\n",
    "                    w = self.knn_weights() \n",
    "                    self.computed_weights[key] = w\n",
    "                elif key == 'submodular':\n",
    "                    w = self.submodular_weights() \n",
    "                    self.computed_weights[key] = w\n",
    "                elif key == 'random':\n",
    "                    w = self.__compute_degree_weight()\n",
    "                    self.computed_weights[key] = w \n",
    "                \n",
    "                elif key == 'link':\n",
    "                    w = self.link_weights()\n",
    "                    self.computed_weights[key] = w \n",
    "                \n",
    "                elif key == 'fastlink':\n",
    "                    w = self.fastlink_weights(**kwargs)\n",
    "                    self.computed_weights[key] = w                 \n",
    "                \n",
    "                elif key == 'link-nn':\n",
    "                    w = self.link_nn_weights()\n",
    "                    self.computed_weights[key] = w\n",
    "                elif key == 'link-sub':\n",
    "                    w = self.link_sub_weights()\n",
    "                    self.computed_weights[key] = w\n",
    "                    \n",
    "                elif key == 'apricot':\n",
    "                    w = self.apricot_sub_weights()\n",
    "                    self.computed_weights[key] = w\n",
    "                    \n",
    "                elif key == 'disjoint':\n",
    "                    w = self.__compute_degree_weight()\n",
    "                    self.computed_weights[key] = w\n",
    "                    self.epoch_count = 0\n",
    "                    self.copy_data = copy.deepcopy(data)                    \n",
    "                    self.get_sparse()\n",
    "    \n",
    "    def get_sparse(self, sel_K=1):\n",
    "        sparse_data, sel_mst_index = get_random_sparse(\n",
    "            self.copy_data, sel_K = sel_K, max_K = self.params['disjoint']['max_K'], metric = self.params['disjoint']['metric'], \n",
    "            minimum=self.params['disjoint']['minimum'], draw=False, log=self.log, \n",
    "            dataset_name=self.save_dir, recompute=self.recompute)\n",
    "        \n",
    "        \n",
    "        self.sparse_adj = SparseTensor(\n",
    "            row=sparse_data.edge_index[0], col=sparse_data.edge_index[1],\n",
    "            value=torch.arange(sparse_data.num_edges, device=sparse_data.edge_index.device),\n",
    "            sparse_sizes=(sparse_data.num_nodes, sparse_data.num_nodes)\n",
    "        )\n",
    "        \n",
    "        if self.log:\n",
    "            print(sparse_data, sel_mst_index)\n",
    "        \n",
    "        return self.sparse_adj, sel_mst_index   \n",
    "        \n",
    "    \n",
    "    def norm_compute(self):        \n",
    "        if self.sample_coverage > 0:                \n",
    "            path = osp.join(self.save_dir or '', self.__filename__)\n",
    "            directory = osp.dirname(path)    \n",
    "            if not osp.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            \n",
    "            if self.recompute == False and self.save_dir is not None and osp.exists(path):  # pragma: no cover\n",
    "                if self.log:\n",
    "                    print('loading saved norm')\n",
    "                \n",
    "                self.node_norm, self.edge_norm = torch.load(path)\n",
    "            else:\n",
    "                self.norm_computation = True\n",
    "                self.node_norm, self.edge_norm = self.__compute_norm__()\n",
    "                self.norm_computation = False\n",
    "                \n",
    "                if self.log:\n",
    "                    print('Saving norm')\n",
    "                \n",
    "                if self.save_dir is not None:  # pragma: no cover\n",
    "                    torch.save((self.node_norm, self.edge_norm), path)\n",
    "            \n",
    "    def knn_weights(self):        \n",
    "        from ipynb.fs.full.KNNWeights import KNNWeight\n",
    "\n",
    "        method = 'knn'\n",
    "        m_name = method+self.params[method]['metric']\n",
    "        \n",
    "        compute, w = is_compute(self.recompute, m_name, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "            knn = KNNWeight(self.data, metric=self.params[method]['metric'], log=self.log)                \n",
    "            w = knn.compute_weights()\n",
    "            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",m_name)\n",
    "            save_weight(m_name, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",m_name)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    def submodular_weights(self):\n",
    "        from ipynb.fs.full.SubmodularWeights import SubModularWeightFacilityFaster\n",
    "        \n",
    "        method = 'submodular'\n",
    "        m_name = method+self.params[method]['metric']\n",
    "        \n",
    "        compute, w = is_compute(self.recompute, m_name, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "            sub = SubModularWeightFacilityFaster(self.data, metric=self.params[method]['metric'], log=self.log)\n",
    "            w = sub.compute_weights()\n",
    "            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",m_name)\n",
    "            save_weight(m_name, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",m_name)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    def apricot_sub_weights(self):\n",
    "        from ipynb.fs.full.SubmodularWeightsApricot import SubModularWeightApricot\n",
    "        \n",
    "        method = 'apricot'\n",
    "        m_name=method+self.params[method]['sub_func']+self.params[method]['metric']\n",
    "        \n",
    "        compute, w = is_compute(self.recompute, m_name, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "            sub = SubModularWeightApricot(self.data, metric=self.params[method]['metric'], sub_func=self.params[method]['sub_func'], log=self.log)\n",
    "            w = sub.compute_weights()\n",
    "            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",m_name)\n",
    "            save_weight(m_name, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",m_name)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    def link_weights(self):\n",
    "        method = 'link'        \n",
    "        compute, w = is_compute(self.recompute, method, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "            from ipynb.fs.full.PretrainedLink import LinkPred\n",
    "            \n",
    "            linkpred = LinkPred(self.data, selfloop = True, log=self.log)\n",
    "            w = linkpred.compute_weights()   \n",
    "            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",method)\n",
    "            save_weight(method, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",method)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    def fastlink_weights(self, **kwargs):\n",
    "        method = 'fastlink'        \n",
    "        compute, w = is_compute(self.recompute, method, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "            from ipynb.fs.full.PretrainedLinkFast import get_link_weight\n",
    "            \n",
    "            w = get_link_weight(self.data, selfloop = True, log = self.log, worker=kwargs['num_workers'])\n",
    "            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",method)\n",
    "            save_weight(method, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",method)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    def link_nn_weights(self):\n",
    "        method = 'link-nn'        \n",
    "        compute, w = is_compute(self.recompute, method, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "            #from ipynb.fs.full.PretrainedLink import LinkPred, LinkNN\n",
    "            from ipynb.fs.full.PretrainedLinkFast import LinkNN\n",
    "#             default value = 'min'            \n",
    "#             linkpred = LinkPred(self.data, selfloop = True, log=self.log)\n",
    "#             self.data.weight = linkpred.compute_weights()   \n",
    "            \n",
    "            nn_weight = LinkNN(self.data, value=self.params[method]['value'], log=self.log) #min favor similar ones, max disimilar\n",
    "            w = nn_weight.compute_weights()                            \n",
    "            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",method)\n",
    "            save_weight(method, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",method)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    def link_sub_weights(self):\n",
    "        method = 'link-sub'\n",
    "        compute, w = is_compute(self.recompute, method, self.save_dir)\n",
    "                        \n",
    "        if compute:\n",
    "#             from ipynb.fs.full.PretrainedLink import LinkSub\n",
    "            from ipynb.fs.full.PretrainedLinkFast import LinkSub\n",
    "            \n",
    "            linksub = LinkSub(self.data, value=self.params[method]['value'], selfloop = True, log=self.log) #min favor similar ones, max disimilar    \n",
    "            w = linksub.compute_weights()                        \n",
    "                            \n",
    "            if self.log:\n",
    "                print(\"saving weights \",method)\n",
    "            save_weight(method, self.save_dir, w)\n",
    "        else:\n",
    "            if self.log:\n",
    "                print(\"Loading weights \",method)\n",
    "                \n",
    "        return w\n",
    "    \n",
    "    @property\n",
    "    def __filename__(self):\n",
    "        return f'{self.__class__.__name__.lower()}_{self.sample_coverage}.pt'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "\n",
    "    def __sample_nodes__(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def __getitem__original(self, idx):\n",
    "        node_idx = self.__sample_nodes__(self.__batch_size__).unique()  \n",
    "        node_idx = node_idx.clamp(max=self.N-1)\n",
    "        adj, _ = self.adj.saint_subgraph(node_idx)\n",
    "        return node_idx, adj\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        if self.norm_computation:\n",
    "            return self.__getitem__original(idx)\n",
    "        \n",
    "#         print(\"-\"*100)        \n",
    "#         print(idx)    \n",
    "        \n",
    "        seed_node, subgraph_node_idx = self.ags_sample_nodes__(idx, self.__batch_size__) #make sure .unique()\n",
    "        \n",
    "#         print(seed_node,subgraph_node_idx)\n",
    "        \n",
    "        adj_subgraph = []\n",
    "        for node_idx in subgraph_node_idx:             \n",
    "            #node_idx = node_idx.clamp(max=self.N-1)            \n",
    "            adj, _ = self.adj.saint_subgraph(node_idx)            \n",
    "            adj_subgraph.append(adj)\n",
    "        \n",
    "#         print(adj_subgraph)\n",
    "        \n",
    "#         print(\"-\"*100)\n",
    "        \n",
    "        return seed_node, subgraph_node_idx, adj_subgraph\n",
    "\n",
    "    def __collate__(self, data_list):\n",
    "        assert len(data_list) == 1\n",
    "        \n",
    "        seed_node, subgraph_node_idx, adj_subgraph = data_list[0]\n",
    "        \n",
    "        #node_idx, adj = data_list[0]\n",
    "        \n",
    "        batch_data = []\n",
    "        \n",
    "        for i in range(len(subgraph_node_idx)):\n",
    "            \n",
    "            node_idx = subgraph_node_idx[i]\n",
    "            adj = adj_subgraph[i]\n",
    "        \n",
    "            data = self.data.__class__()\n",
    "            \n",
    "            data.seed_node = seed_node\n",
    "            data.batch_size = seed_node.shape[0]                \n",
    "            \n",
    "            data.num_nodes = node_idx.size(0)\n",
    "            row, col, edge_idx = adj.coo()\n",
    "            data.edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "            for key, item in self.data:\n",
    "                if key in ['edge_index', 'num_nodes']:\n",
    "                    continue\n",
    "                if isinstance(item, torch.Tensor) and item.size(0) == self.N:\n",
    "                    data[key] = item[node_idx]\n",
    "                elif isinstance(item, torch.Tensor) and item.size(0) == self.E:\n",
    "                    data[key] = item[edge_idx]\n",
    "                else:\n",
    "                    data[key] = item\n",
    "                     \n",
    "            if self.sample_coverage > 0:\n",
    "                data.node_norm = self.node_norm[node_idx]\n",
    "                data.edge_norm = self.edge_norm[edge_idx]\n",
    "\n",
    "            batch_data.append(data)\n",
    "            \n",
    "        if len(batch_data) == 1:\n",
    "            return batch_data[0]\n",
    "        \n",
    "        return batch_data\n",
    "    \n",
    "    \n",
    "    def __compute_degree_weight(self):\n",
    "        row, col, _ = self.adj.coo()\n",
    "\n",
    "        deg_in = 1. / self.adj.storage.colcount()\n",
    "        deg_out = 1. / self.adj.storage.rowcount()\n",
    "        weight = (1. / deg_in[row]) + (1. / deg_out[col])\n",
    "        \n",
    "        return weight\n",
    "    \n",
    "\n",
    "    def __compute_norm__(self):\n",
    "        \n",
    "        \n",
    "        node_count = torch.zeros(self.N, dtype=torch.float)\n",
    "        edge_count = torch.zeros(self.E, dtype=torch.float)\n",
    "                \n",
    "        loader = torch.utils.data.DataLoader(self, batch_size=200,\n",
    "                                             collate_fn=lambda x: x,\n",
    "                                             num_workers=self.num_workers)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar = tqdm(total=self.N * self.sample_coverage)\n",
    "            pbar.set_description('Compute AGS normalization')\n",
    "\n",
    "        num_samples = total_sampled_nodes = 0\n",
    "        \n",
    "        while total_sampled_nodes < self.N * self.sample_coverage:\n",
    "            \n",
    "            for data in loader:\n",
    "                #print(data)                \n",
    "                for node_idx, adj in data:\n",
    "                    edge_idx = adj.storage.value()\n",
    "                    node_count[node_idx] += 1\n",
    "                    edge_count[edge_idx] += 1\n",
    "                    total_sampled_nodes += node_idx.size(0)\n",
    "\n",
    "                    if self.log:  # pragma: no cover\n",
    "                        pbar.update(node_idx.size(0))\n",
    "            num_samples += self.num_steps\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar.close()\n",
    "\n",
    "        row, _, edge_idx = self.adj.coo()\n",
    "        t = torch.empty_like(edge_count).scatter_(0, edge_idx, node_count[row])\n",
    "        edge_norm = (t / edge_count).clamp_(0, 1e4)\n",
    "        edge_norm[torch.isnan(edge_norm)] = 0.1\n",
    "\n",
    "        node_count[node_count == 0] = 0.1\n",
    "        node_norm = num_samples / node_count / self.N\n",
    "\n",
    "        return node_norm, edge_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffe60653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# a = torch.LongTensor([1,2,3,4,5])\n",
    "# a.clamp(max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb8d0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSNodeSampler(AGSSampler):    \n",
    "    def __sample_nodes__(self, batch_size):\n",
    "        edge_sample = torch.randint(0, self.E, (batch_size, self.batch_size),\n",
    "                                    dtype=torch.long)\n",
    "        \n",
    "        print(edge_sample.shape)\n",
    "        print(edge_sample)\n",
    "        print(self.adj.storage.row()[edge_sample])\n",
    "\n",
    "        return self.adj.storage.row()[edge_sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac855954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSEdgeSampler(AGSSampler):\n",
    "    \n",
    "    def __sample_nodes__(self, batch_size):\n",
    "        row, col, _ = self.adj.coo()\n",
    "\n",
    "        deg_in = 1. / self.adj.storage.colcount()\n",
    "        deg_out = 1. / self.adj.storage.rowcount()\n",
    "        prob = (1. / deg_in[row]) + (1. / deg_out[col])\n",
    "\n",
    "        # Parallel multinomial sampling (without replacement)\n",
    "        # https://github.com/pytorch/pytorch/issues/11931#issuecomment-625882503\n",
    "        rand = torch.rand(batch_size, self.E).log() / (prob + 1e-10)\n",
    "        edge_sample = rand.topk(self.batch_size, dim=-1).indices\n",
    "\n",
    "        source_node_sample = col[edge_sample]\n",
    "        target_node_sample = row[edge_sample]\n",
    "\n",
    "        return torch.cat([source_node_sample, target_node_sample], -1)\n",
    "    \n",
    "    #modify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98415edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSRandomWalkSampler(AGSSampler):\n",
    "    def __init__(self, data, batch_size: int, walk_length: int,\n",
    "                 num_steps: int = 1, sample_coverage: int = 0,\n",
    "                 save_dir: Optional[str] = None, log: bool = True, **kwargs):\n",
    "        self.walk_length = walk_length\n",
    "        super().__init__(data, batch_size, num_steps, sample_coverage,\n",
    "                         save_dir, log, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def __filename__(self):\n",
    "        return (f'{self.__class__.__name__.lower()}_{self.walk_length}_'\n",
    "                f'{self.sample_coverage}.pt')\n",
    "\n",
    "    def __sample_nodes__(self, batch_size):\n",
    "        start = torch.randint(0, self.N, (batch_size, ), dtype=torch.long)\n",
    "        node_idx = self.adj.random_walk(start.flatten(), self.walk_length)\n",
    "        return node_idx.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad0f22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSGraphSampler(AGSSampler):\n",
    "    def __init__(self, data, batch_size: int, walk_length: int =2,\n",
    "                 num_steps: int = 1, sample_coverage: int = 100,\n",
    "                 save_dir: Optional[str] = None, log: bool = True, \n",
    "                 input_nodes: InputNodes = None,\n",
    "                 sample_func: List = None,\n",
    "                 weight_func: Dict = None,\n",
    "                 params=None,                 \n",
    "                 recompute = False,                 \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.log = log\n",
    "        self.sample_func = sample_func\n",
    "        self.weight_func=weight_func\n",
    "        self.params=params        \n",
    "        self.recompute = recompute,\n",
    "        self.walk_length = walk_length\n",
    "        self.input_nodes = None\n",
    "        if 'shuffle' in kwargs:\n",
    "            self.shfl = kwargs['shuffle']\n",
    "        else:\n",
    "            self.shfl = False\n",
    "        \n",
    "        if input_nodes is not None:\n",
    "            self.input_nodes = torch.nonzero(input_nodes).flatten()\n",
    "            \n",
    "            \n",
    "        super().__init__(data, batch_size, num_steps, sample_coverage,\n",
    "                         save_dir, recompute,log, \n",
    "                         sample_func, weight_func,params,\n",
    "                         **kwargs)\n",
    "        \n",
    "    @property\n",
    "    def __filename__(self):\n",
    "        return (f'{self.__class__.__name__.lower()}_{self.walk_length}_'\n",
    "                f'{self.sample_coverage}.pt')\n",
    "\n",
    "    def ags_sample_nodes__(self, idx, batch_size):                \n",
    "        \n",
    "        ## Seed not for everyone\n",
    "        \n",
    "        if self.input_nodes is not None:\n",
    "            if self.shfl:\n",
    "                rand_node = torch.randint(0, len(self.input_nodes), (batch_size, ), dtype=torch.long).flatten()        \n",
    "                seed_node = self.input_nodes[rand_node]\n",
    "            else:                \n",
    "                seed_node=self.input_nodes[idx*batch_size:(idx+1)*batch_size]\n",
    "        else:                    \n",
    "            seed_node = torch.randint(0, self.N, (batch_size, ), dtype=torch.long).flatten()        \n",
    "        \n",
    "        seed_unique = seed_node.unique()\n",
    "        \n",
    "        #print(\"start:\",seed_node)\n",
    "        #print(\"startunique:\",seed_unique)\n",
    "        \n",
    "        subgraph_node_idx = []   \n",
    "        \n",
    "        for i, method in enumerate(self.sample_func):\n",
    "                                            \n",
    "            if method in [\"rw\"]:\n",
    "                node_idx = self.adj.random_walk(seed_node, self.walk_length)        \n",
    "                                \n",
    "            elif method in ['wrw']:          \n",
    "                weight_name = self.weight_func[i]['weight']\n",
    "                prob = self.computed_weights[weight_name]                \n",
    "                #print(prob.shape)                    \n",
    "                #node_idx = self.adj.w_random_walk(seed_node, prob, self.walk_length)            \n",
    "                node_idx = self.adj.w_random_walk_cpp(seed_node, prob, self.walk_length)            \n",
    "            \n",
    "            elif method in ['disjoint']:                \n",
    "                node_idx = self.adj.random_walk(seed_node, self.walk_length)     \n",
    "                \n",
    "                #print(idx, self.epoch_count)\n",
    "\n",
    "                if idx == 0 and self.epoch_count > 0:                \n",
    "                    if self.log:\n",
    "                        print(\"Computing sparse graph\")                    \n",
    "                    self.sparse_adj, s_idx = self.get_sparse(self.params['disjoint']['sel_K'])\n",
    "                    if self.log:\n",
    "                        print(s_idx)\n",
    "                    \n",
    "                    self.epoch_count = self.epoch_count+1\n",
    "                \n",
    "                if idx == 0 and self.epoch_count == 0:\n",
    "                    self.epoch_count+=1\n",
    "                \n",
    "                node_idx1 = self.sparse_adj.random_walk(seed_node, self.walk_length)\n",
    "                node_idx = torch.cat((node_idx,node_idx1))\n",
    "                \n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "    \n",
    "            sampled_nodes = node_idx.view(-1)\n",
    "            \n",
    "            #print(sampled_nodes)         \n",
    "            sampled_nodes, inverse_indices = torch.cat((seed_unique, sampled_nodes)).unique(return_inverse=True)            \n",
    "            n_seed_idx = inverse_indices[:len(seed_unique)]\n",
    "            to_swap = sampled_nodes[:len(seed_unique)]\n",
    "\n",
    "            #print(n_seed_idx)\n",
    "            #print(to_swap)\n",
    "            #Use scatter to replace elements efficiently\n",
    "            #sampled_nodes.scatter_(0, n_seed_idx, to_swap.clone())\n",
    "            #sampled_nodes[:len(seed_node)] = seed_node\n",
    "\n",
    "            sampled_nodes[n_seed_idx]=to_swap.clone()\n",
    "            sampled_nodes[:len(seed_unique)]=seed_unique\n",
    "            \n",
    "            ## make exact batch size\n",
    "            if self.weight_func[i]['exact']:\n",
    "                if len(sampled_nodes)>=batch_size:                \n",
    "                    sampled_nodes = sampled_nodes[:batch_size]\n",
    "                else:\n",
    "                    requirement = batch_size-len(sampled_nodes)\n",
    "                    #print(\"node required: \", requirement)\n",
    "                    mask = torch.ones(self.N, dtype=torch.bool)\n",
    "                    mask[sampled_nodes]=False\n",
    "                    other_nodes = torch.nonzero(mask).flatten()\n",
    "\n",
    "                    #print(other_nodes, len(other_nodes))\n",
    "\n",
    "                    random_indices = torch.randperm(other_nodes.size(0))[:requirement]\n",
    "                    extra_sampled = other_nodes[random_indices]\n",
    "                    sampled_nodes = torch.cat((sampled_nodes,extra_sampled))\n",
    "                    \n",
    "                    #print(sampled_nodes)\n",
    "                assert len(sampled_nodes) == batch_size                                \n",
    "        \n",
    "            subgraph_node_idx.append(sampled_nodes)                \n",
    "            \n",
    "        #print(seed_unique, subgraph_node_idx)        \n",
    "        return seed_unique, subgraph_node_idx\n",
    "    \n",
    "    def __sample_nodes__(self, batch_size):                \n",
    "        start = torch.randint(0, self.N, (batch_size, ), dtype=torch.long)\n",
    "        node_idx = self.adj.random_walk(start.flatten(), self.walk_length)        \n",
    "        return node_idx.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2015eb45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "===========================================================================================================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "17\n",
      "loading saved norm\n",
      "Loading weights  knncosine\n",
      "Loading weights  submodularcosine\n",
      "Loading weights  fastlink\n",
      "Loading weights  link-nn\n",
      "Loading weights  link-sub\n",
      "Loading weights  apricotfacilitycosine\n",
      "Data(seed_node=[2], batch_size=2, num_nodes=5, edge_index=[2, 10], x=[5, 34], y=[5], train_mask=[5], val_mask=[5], test_mask=[5], node_norm=[5], edge_norm=[10])\n",
      "Data(seed_node=[2], batch_size=2, num_nodes=4, edge_index=[2, 6], x=[4, 34], y=[4], train_mask=[4], val_mask=[4], test_mask=[4], node_norm=[4], edge_norm=[6])\n",
      "Data(seed_node=[2], batch_size=2, num_nodes=4, edge_index=[2, 8], x=[4, 34], y=[4], train_mask=[4], val_mask=[4], test_mask=[4], node_norm=[4], edge_norm=[8])\n",
      "Data(seed_node=[2], batch_size=2, num_nodes=5, edge_index=[2, 10], x=[5, 34], y=[5], train_mask=[5], val_mask=[5], test_mask=[5], node_norm=[5], edge_norm=[10])\n",
      "Data(seed_node=[2], batch_size=2, num_nodes=5, edge_index=[2, 12], x=[5, 34], y=[5], train_mask=[5], val_mask=[5], test_mask=[5], node_norm=[5], edge_norm=[12])\n",
      "Data(seed_node=[2], batch_size=2, num_nodes=5, edge_index=[2, 8], x=[5, 34], y=[5], train_mask=[5], val_mask=[5], test_mask=[5], node_norm=[5], edge_norm=[8])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    whole_data, dataset = get_data('karate')        \n",
    "#     whole_data, dataset = get_data('karate')        \n",
    "    \n",
    "#     #file_path = 'graphsaintrandomwalksampler_2_100.pt'\n",
    "#     #file_path = 'graphsaintrandomwalksampler_100.pt'\n",
    "#     file_path = 'graphsaintnodesampler_10.pt'\n",
    "    \n",
    "#     if os.path.exists(file_path):\n",
    "#         !rm 'graphsaintnodesampler_10.pt'        \n",
    "#         print(\"Deleted: \",file_path)\n",
    "#     else:\n",
    "#         print('File not exist')\n",
    "            \n",
    "    #sample_func=['rw', 'exact', 'wrw', 'wexact', 'disjoint']\n",
    "    \n",
    "#     sample_func =['disjoint']\n",
    "#     weight_func =[\n",
    "#         {'exact':False,'weight':'disjoint'}, #exact for exact size to the batch\n",
    "#         #{'exact':False,'weight':'random'}, #exact for exact size to the batch\n",
    "#     ]\n",
    "\n",
    "    sample_func =['wrw','wrw','wrw','wrw','wrw','wrw']\n",
    "    weight_func =[\n",
    "        {'exact':False,'weight':'knn'},\n",
    "        {'exact':False,'weight':'submodular'},\n",
    "        {'exact':False,'weight':'fastlink'}, \n",
    "        {'exact':False,'weight':'link-nn'},\n",
    "        {'exact':False,'weight':'link-sub'},\n",
    "        {'exact':False,'weight':'apricot'},\n",
    "    ]\n",
    "\n",
    "    ##minimum corresponds to decreasing subgraph\n",
    "    \n",
    "    params={'knn':{'metric':'cosine'},\n",
    "            'submodular':{'metric':'cosine'},\n",
    "            'link-nn':{'value':'min'},\n",
    "            'link-sub':{'value':'max'},\n",
    "            'disjoint':{'value':'mst', 'sel_K':2, 'max_K':5, 'metric':'cosine','minimum':True}, \n",
    "            'apricot':{'sub_func':'facility','metric':'cosine'},\n",
    "           }\n",
    "    \n",
    "    batch_size = 2\n",
    "    num_steps = int(math.ceil(whole_data.num_nodes/batch_size))\n",
    "    print(num_steps)\n",
    "\n",
    "    loader = AGSGraphSampler(\n",
    "        whole_data, batch_size=batch_size, \n",
    "        input_nodes = None,\n",
    "        #input_nodes = whole_data.train_mask, \n",
    "        walk_length=2,\n",
    "        num_steps=num_steps, \n",
    "        sample_coverage=2,\n",
    "        num_workers=0,\n",
    "        sample_func = sample_func,\n",
    "        weight_func=weight_func,\n",
    "        params=params,\n",
    "        save_dir = 'Results/GS',\n",
    "        recompute = False,\n",
    "        shuffle = False,\n",
    "        log = True,\n",
    "    )\n",
    "\n",
    "    batch  = next(iter(loader))\n",
    "    \n",
    "    for b in batch:\n",
    "        print(b)\n",
    "    \n",
    "    for i in range(10):\n",
    "        for batch in loader:\n",
    "            #print(batch)\n",
    "            None\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc8a9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# a = torch.Tensor([12,23])\n",
    "# a.clamp(min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5ac8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d98ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51184ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
