{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bffbb0c",
   "metadata": {},
   "source": [
    "# Attribute-Guided Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bed04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pyg-team/pytorch_geometric/issues/1961\n",
    "#https://github.com/rusty1s/pytorch_sparse/blob/99735df9ac54b1ae8d46ddf5360da58fd0eeef0c/torch_sparse/sample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0288930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importance pooling: https://arxiv.org/pdf/1806.01973.pdf\n",
    "#graphsaint sampling: https://arxiv.org/pdf/1907.04931.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bec6571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as it turned out interactive shell (like Jupyter cannot handle CPU multiprocessing well so check which medium the code is runing)\n",
    "#we will write code in Jupyter for understanding purposes but final execuation will be in shell\n",
    "from ipynb.fs.full.Utils import isnotebook\n",
    "from ipynb.fs.full.Dataset import get_data\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8e61e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch_sparse import SparseTensor\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import getpy as gp\n",
    "\n",
    "import random\n",
    "random.seed(12345)\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8786d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.loader.base import BaseDataLoader\n",
    "from torch_geometric.loader.utils import (edge_type_to_str, filter_data,\n",
    "                                          filter_hetero_data, to_csc,\n",
    "                                          to_hetero_csc)\n",
    "from torch_geometric.typing import EdgeType, InputNodes\n",
    "\n",
    "NumNeighbors = Union[List[int], Dict[EdgeType, List[int]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12942a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.SubmodularWeights import SubModularWeightFacilityFaster\n",
    "from ipynb.fs.full.KNNWeights import KNNWeight\n",
    "#from KNNWeights import KNNWeight\n",
    "#from ipynb.fs.full.PretrainedLink import LinkPred, LinkNN, LinkSub\n",
    "from ipynb.fs.full.RandomSparse import RandomSparse\n",
    "from ipynb.fs.full.PretrainedLinkFast import get_link_weight,  LinkNN, LinkSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17d46173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import choices\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def sample_with_weights(colptr, row, input_node, num_neighbors, replace, directed, weights):\n",
    "    \n",
    "    samples = input_node\n",
    "    \n",
    "    to_local_node = defaultdict(lambda: len(samples))\n",
    "    for i, v in enumerate(input_node):\n",
    "        to_local_node[v] = i\n",
    "\n",
    "    rows, cols, edges, sampled_weights = [], [], [], []\n",
    "\n",
    "    begin, end = 0, len(samples)\n",
    "    for ell, num_samples in enumerate(num_neighbors):\n",
    "        for i in range(begin, end):\n",
    "            w = samples[i]\n",
    "            col_start = colptr[w]\n",
    "            col_end = colptr[w + 1]\n",
    "            col_count = col_end - col_start\n",
    "\n",
    "            if col_count == 0:\n",
    "                continue\n",
    "\n",
    "            if (num_samples < 0) or (not replace and num_samples >= col_count):\n",
    "                sampled_indices = range(col_start,col_end)                \n",
    "            elif replace:\n",
    "                probs = weights[col_start:col_end] / np.sum(weights[col_start:col_end])                \n",
    "                sampled_indices = choices(np.arange(col_start, col_end), weights=probs, k=num_samples) #with replacement\n",
    "            else:                \n",
    "                probs = weights[col_start:col_end] / np.sum(weights[col_start:col_end])\n",
    "                #probs[-1] = 1 - np.sum(probs[0:-1])                \n",
    "                sampled_indices = np.random.choice(np.arange(col_start, col_end), size=num_samples, replace=replace, p=probs)\n",
    "                #sampled_indices = choices(np.arange(col_start, col_end), weights=probs, k=num_samples) #with replacement\n",
    "\n",
    "            for offset in sampled_indices:\n",
    "                v = row[offset]\n",
    "                res = to_local_node[v]\n",
    "                if res == len(samples):\n",
    "                    samples.append(v)\n",
    "                if directed:\n",
    "                    cols.append(i)\n",
    "                    rows.append(res)\n",
    "                    edges.append(offset)\n",
    "                sampled_weights.append(weights[offset])\n",
    "\n",
    "        begin, end = end, len(samples)\n",
    "\n",
    "    if not directed:\n",
    "        local_node_indices = {v: i for i, v in enumerate(samples)}\n",
    "        for i, w in enumerate(samples):\n",
    "            col_start = colptr[w]\n",
    "            col_end = colptr[w + 1]\n",
    "            for offset in range(col_start, col_end):\n",
    "                v = row[offset]\n",
    "                res = local_node_indices.get(v)\n",
    "                if res is not None:\n",
    "                    rows.append(res)\n",
    "                    cols.append(i)\n",
    "                    edges.append(offset)\n",
    "                    sampled_weights.append(weights[offset])\n",
    "\n",
    "    return (\n",
    "        torch.tensor(samples, dtype=torch.int64),\n",
    "        torch.tensor(rows, dtype=torch.int64),\n",
    "        torch.tensor(cols, dtype=torch.int64),\n",
    "        torch.tensor(edges, dtype=torch.int64),\n",
    "        torch.tensor(sampled_weights, dtype=torch.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b4ac350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DeviceDir\n",
    "# DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "# device, NUM_PROCESSORS = DeviceDir.get_device()\n",
    "\n",
    "# n=7\n",
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],\n",
    "#                                [5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# mask = torch.zeros(n, dtype=torch.bool)\n",
    "# mask[[0,1,4,5]] = True\n",
    "# data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = ~mask, val_mask = ~mask)    \n",
    "# print(data)\n",
    "\n",
    "# #data, dataset = get_data('Cora', DIR=DIR, log = False) \n",
    "\n",
    "# (row, col) = data.edge_index\n",
    "# size = data.size()\n",
    "# perm = (col * size[0]).add_(row).argsort()\n",
    "# colptr = torch.ops.torch_sparse.ind2ptr(col[perm], size[1])\n",
    "# row = row[perm]\n",
    "\n",
    "# weights = (1. / degree(col, data.num_nodes)[col]) # Norm by in-degree.\n",
    "# weights = weights[perm]\n",
    "# index = torch.LongTensor([0,1])\n",
    "# num_neighbors= [20, 10]\n",
    "# # sample_with_weights_getpy(colptr,row,index,num_neighbors,False, True, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a347413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import torch\n",
    "#sys.path.append(\"/home/sferdou/CPPSamplerNew/build/src\")\n",
    "\n",
    "sys.path.append(\"/home/das90/GNNcodes/CVE2020/GNN-NC/Graph-Sparsification/CPPsamplerPy/build/src\")\n",
    "\n",
    "import sampling_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00137037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = sampling_module.sample(\n",
    "#     colptr,\n",
    "#     row,\n",
    "#     index,\n",
    "#     num_neighbors\n",
    "# )\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "929458c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_weight(method,save_dir,weights):\n",
    "    filename= save_dir+method+\".pt\"\n",
    "    \n",
    "    directory = osp.dirname(filename)    \n",
    "    if not osp.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    torch.save(weights, filename)\n",
    "    \n",
    "def load_weight(method, save_dir):\n",
    "    filename= save_dir+method+\".pt\"\n",
    "    if not osp.exists(filename):\n",
    "        return None\n",
    "    else:\n",
    "        return torch.load(filename)\n",
    "    \n",
    "def is_compute(kwargs, method):\n",
    "    compute=False\n",
    "    w = None\n",
    "    if kwargs['recompute']==True:                         \n",
    "        compute=True\n",
    "    else:\n",
    "        w = load_weight(method, kwargs['save_dir'])\n",
    "        if w is None:\n",
    "            compute=True            \n",
    "    return compute, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bb939cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeighborSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Union[Data, HeteroData],\n",
    "        num_neighbors: NumNeighbors,\n",
    "        replace: bool = False,\n",
    "        directed: bool = True,\n",
    "        input_node_type: Optional[str] = None,\n",
    "        **kwargs,\n",
    "                \n",
    "    ):\n",
    "        self.data_cls = data.__class__\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.replace = replace\n",
    "        self.directed = directed\n",
    "         \n",
    "        ##addded \n",
    "        self.N = N = data.num_nodes\n",
    "        self.E = E = data.num_edges\n",
    "        self.data = data\n",
    "        \n",
    "        self.adj = SparseTensor(\n",
    "            row=data.edge_index[0], col=data.edge_index[1],\n",
    "            value=torch.arange(E, device=data.edge_index.device),\n",
    "            sparse_sizes=(N, N))\n",
    "        \n",
    "        \n",
    "        self.data=data\n",
    "        #log = False\n",
    "        log = kwargs['log']\n",
    "        self.log = log\n",
    "        \n",
    "        weight_funcs = kwargs['weight_func']\n",
    "        params = kwargs['params']                \n",
    "        self.weight_funcs = weight_funcs\n",
    "        \n",
    "#         ######## delete this alter\n",
    "#         weights = []\n",
    "#         row, col = data.edge_index\n",
    "#         weights.append(1. / degree(col, data.num_nodes)[col]) # Norm by in-degree.        \n",
    "#         weights.append(1. / degree(col, data.num_nodes)[col])         \n",
    "#         self.weights = weights\n",
    "#         data.weights = self.weights\n",
    "#         #######\n",
    "        \n",
    "        if 'weights' not in data:\n",
    "            \n",
    "#             weight_funcs = kwargs['weight_func']\n",
    "#             params = kwargs['params']            \n",
    "#             self.weight_funcs = weight_funcs\n",
    "            \n",
    "            weights = []\n",
    "            \n",
    "            #print('Weight not given, computing edge weights....')            \n",
    "            \n",
    "            if len(weight_funcs) > 0:\n",
    "                for i,method in enumerate(weight_funcs):\n",
    "                    if method == 'knn':       \n",
    "                        \n",
    "                        m_name = method+params[method]['metric']                        \n",
    "                        compute, w = is_compute(kwargs, m_name)\n",
    "                        \n",
    "                        if compute:\n",
    "                            knn = KNNWeight(data, metric=params[method]['metric'], log=log)                \n",
    "                            w = knn.compute_weights()\n",
    "                            weights.append(w)\n",
    "                            \n",
    "                            if log:\n",
    "                                print(\"saving weights \",m_name)\n",
    "                            save_weight(m_name, kwargs['save_dir'], w)\n",
    "                        else:\n",
    "                            if log:\n",
    "                                print(\"Loading weights \",m_name)\n",
    "                            weights.append(w)\n",
    "\n",
    "                    elif method == 'submodular':\n",
    "                        \n",
    "                        m_name = method+params[method]['metric']                        \n",
    "                        compute, w = is_compute(kwargs, m_name)                    \n",
    "                        \n",
    "                        if compute:\n",
    "                            sub = SubModularWeightFacilityFaster(data, metric=params[method]['metric'], log=log)\n",
    "                            w = sub.compute_weights()\n",
    "                            weights.append(w)\n",
    "                            \n",
    "                            if log:\n",
    "                                print(\"saving weights \",m_name)\n",
    "                            save_weight(m_name, kwargs['save_dir'], w)\n",
    "                        else:\n",
    "                            if log:\n",
    "                                print(\"Loading weights \",method)\n",
    "                            weights.append(w)\n",
    "                        \n",
    "                    elif method == 'fastlink':    \n",
    "                        compute, w = is_compute(kwargs, method)\n",
    "                        if compute:                            \n",
    "                            w = get_link_weight(data, selfloop = True, log = log, worker=kwargs['num_workers'])\n",
    "                            weights.append(w)\n",
    "                            if log:\n",
    "                                print(\"saving weights \",method)\n",
    "                            save_weight(method, kwargs['save_dir'], w)\n",
    "                        else:\n",
    "                            if log:\n",
    "                                print(\"Loading weights \",method)\n",
    "                            weights.append(w)\n",
    "                    \n",
    "                    elif method == 'link-nn':  \n",
    "                        m_name = method+params[method]['value']\n",
    "                        \n",
    "                        compute, w = is_compute(kwargs, m_name)\n",
    "                        if compute:                            \n",
    "                            nn_weight = LinkNN(data, value=params[method]['value'], log=log) #min favor similar ones, max disimilar\n",
    "                            w = nn_weight.compute_weights()                            \n",
    "                            weights.append(w)                            \n",
    "                            if log:\n",
    "                                print(\"saving weights \",m_name)\n",
    "                            save_weight(m_name, kwargs['save_dir'], w)\n",
    "                        else:\n",
    "                            if log:\n",
    "                                print(\"Loading weights \",m_name)\n",
    "                            weights.append(w)\n",
    "                    \n",
    "                    elif method == 'link-sub': \n",
    "                        m_name = method+params[method]['value']\n",
    "                        compute, w = is_compute(kwargs, m_name)\n",
    "                        if compute:\n",
    "                            #default value = 'max'                        \n",
    "                            linksub = LinkSub(data, value=params[method]['value'], selfloop = True, log=log) #min favor similar ones, max disimilar    \n",
    "                            w = linksub.compute_weights()                        \n",
    "                            weights.append(w)\n",
    "                            \n",
    "                            if log:\n",
    "                                print(\"saving weights \",m_name)\n",
    "                            save_weight(m_name, kwargs['save_dir'], w)\n",
    "                        else:\n",
    "                            if log:\n",
    "                                print(\"Loading weights \",m_name)\n",
    "                            weights.append(w)\n",
    "                    \n",
    "                    elif method == 'apricot':\n",
    "                        \n",
    "                        from ipynb.fs.full.SubmodularWeightsApricot import SubModularWeightApricot\n",
    "                        \n",
    "                        m_name = method+params[method]['sub_func']+params[method]['metric']\n",
    "                        \n",
    "                        compute, w = is_compute(kwargs, m_name)\n",
    "                        if compute:\n",
    "                            #default value = 'max'                        \n",
    "                            sub = SubModularWeightApricot(data, metric=params[method]['metric'], sub_func= params[method]['sub_func'],log=log) #min favor similar ones, max disimilar    \n",
    "                            w = sub.compute_weights()                        \n",
    "                            weights.append(w)\n",
    "                            \n",
    "                            if log:\n",
    "                                print(\"saving weights \",m_name)\n",
    "                            save_weight(m_name, kwargs['save_dir'], w)\n",
    "                        else:\n",
    "                            if log:\n",
    "                                print(\"Loading weights \",m_name)\n",
    "                            weights.append(w)\n",
    "                    \n",
    "                    elif method == 'random':\n",
    "                        row, col = data.edge_index\n",
    "                        weights.append(1. / degree(col, data.num_nodes)[col]) # Norm by in-degree.\n",
    "                    \n",
    "                    else:\n",
    "                        raise NotImplemented\n",
    "                                                                \n",
    "            else:\n",
    "                row, col = data.edge_index\n",
    "                data.weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.                    \n",
    "                data.weight.to(data.edge_index.device)      \n",
    "                \n",
    "            #print(weights)\n",
    "            self.weights = torch.stack(weights).to(data.edge_index.device)\n",
    "            data.weights = self.weights\n",
    "        else:\n",
    "            self.weights = data.weights\n",
    "            #print(data.weights.shape)\n",
    "#             print(data.weights)\n",
    "        \n",
    "    \n",
    "        if isinstance(data, Data):\n",
    "            # Convert the graph data into a suitable format for sampling.\n",
    "            #self.colptr, self.row, self.perm = to_csc(data, device='cpu')\n",
    "            self.colptr, self.row, self.perm = to_csc(data, device='cpu')\n",
    "            self.colptr_npy =  self.colptr.numpy()\n",
    "            self.row_npy = self.row.numpy()\n",
    "            \n",
    "            assert isinstance(num_neighbors, (list, tuple))\n",
    "        else:\n",
    "            raise TypeError(f'NeighborLoader found invalid type: {type(data)}')\n",
    "                \n",
    "        self.sample_fn = torch.ops.torch_sparse.neighbor_sample    \n",
    "        \n",
    "        self.weights_npy=[]\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i][self.perm]\n",
    "            self.weights_npy.append(self.weights[i].numpy())\n",
    "    \n",
    "    def weighted_sample(self, index: Union[List[int], Tensor], weight_index):\n",
    "        \n",
    "        if not isinstance(index, torch.LongTensor):\n",
    "            index = torch.LongTensor(index)\n",
    "        \n",
    "        nodes=[]\n",
    "        rows=[]\n",
    "        cols=[]\n",
    "        edges=[]\n",
    "        \n",
    "        #print(\"Start: \", index)\n",
    "        \n",
    "        u_src=index\n",
    "        nodes.append(u_src) ##to have main nodes at first\n",
    "        \n",
    "        for k in self.num_neighbors: \n",
    "            \n",
    "            n_u_src=[]\n",
    "            \n",
    "            for u in u_src:\n",
    "                \n",
    "                col, row, edge = self.adj[u.item(),:].coo()  \n",
    "                \n",
    "#                 print(\"-*-\"*50)\n",
    "#                 print(row, col, edge)\n",
    "\n",
    "                if k==-1 or k>=len(row):\n",
    "                    n_u_src.extend(row)\n",
    "                    nodes.append(row)\n",
    "                    rows.append(self.data.edge_index[1][edge])\n",
    "                    cols.append(self.data.edge_index[0][edge])\n",
    "                    edges.append(edge)\n",
    "                                        \n",
    "                else:\n",
    "                    edge_weight = self.weights[weight_index][edge].numpy() \n",
    "                \n",
    "                    #new_src = random.choices(row, weights=edge_weight, k=k) #with replacement\n",
    "                    \n",
    "                    edge_weight = edge_weight/sum(edge_weight)\n",
    "                    edge_weight[-1] = 1 - np.sum(edge_weight[0:-1])\n",
    "                    \n",
    "                    #print(edge_weight)\n",
    "                    \n",
    "                    new_src = np.random.choice(len(row), k, replace=False, p=edge_weight)    \n",
    "                        \n",
    "                    n_u = row[new_src]\n",
    "                    \n",
    "                    n_u_src.extend(n_u)\n",
    "                    nodes.append(n_u)\n",
    "                    rows.append(n_u)\n",
    "                    cols.append(u.repeat(len(new_src)))\n",
    "                    edges.append(edge[new_src])\n",
    "                    \n",
    "                    #print(n_u_src)\n",
    "                \n",
    "            u_src = n_u_src\n",
    "            \n",
    "        #print(\"Nodes\",nodes)\n",
    "        \n",
    "        node=torch.cat(nodes)\n",
    "        row=torch.cat(rows)\n",
    "        col=torch.cat(cols)\n",
    "        edge=torch.cat(edges)\n",
    "        \n",
    "#         print(\"final:-----------\")\n",
    "#         print(node, row, col, edge)\n",
    "        \n",
    "        node_list = node.tolist()\n",
    "        node_dict={}\n",
    "        number=0\n",
    "        for u in node_list:\n",
    "            if u not in node_dict:\n",
    "                node_dict[u]=number\n",
    "                number+=1\n",
    "        \n",
    "        node_unique = torch.LongTensor(list(node_dict.keys()))\n",
    "                \n",
    "        row=torch.LongTensor([node_dict[i.item()] for i in row])\n",
    "        col=torch.LongTensor([node_dict[i.item()] for i in col])\n",
    "\n",
    "        \n",
    "#         node_unique, inverse_indices = torch.unique(node, sorted=False, return_inverse=True)\n",
    "#         node_dict = dict(zip(node_unique.tolist(), range(len(node_unique))))    \n",
    "#         row=torch.LongTensor([node_dict[i] for i in row.tolist()])\n",
    "#         col=torch.LongTensor([node_dict[i] for i in col.tolist()])\n",
    "        \n",
    "        return node_unique, row, col, edge, index.numel()\n",
    "    \n",
    "    \n",
    "    def call__original(self, index: Union[List[int], Tensor]):\n",
    "        if not isinstance(index, torch.LongTensor):\n",
    "            index = torch.LongTensor(index)\n",
    "\n",
    "        if issubclass(self.data_cls, Data):\n",
    "            \n",
    "            sample_fn = torch.ops.torch_sparse.neighbor_sample\n",
    "            node, row, col, edge = sample_fn(\n",
    "                self.colptr,\n",
    "                self.row,\n",
    "                index,\n",
    "                self.num_neighbors,\n",
    "                self.replace,\n",
    "                self.directed,\n",
    "            )\n",
    "            return (node, row, col, edge, index.numel())\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f'NeighborLoader found invalid type: {type(data)}')\n",
    "            \n",
    "    \n",
    "    def call_weighted_sample(self, index: Union[List[int], Tensor], weight_index):\n",
    "        if not isinstance(index, torch.LongTensor):\n",
    "            index = torch.LongTensor(index)\n",
    "\n",
    "        if issubclass(self.data_cls, Data):        \n",
    "            node, row, col, edge, sampled_weight = sample_with_weights(\n",
    "                self.colptr_npy,\n",
    "                self.row_npy,\n",
    "                index.tolist(),\n",
    "                self.num_neighbors,\n",
    "                self.replace,\n",
    "                self.directed,\n",
    "                self.weights_npy[weight_index]\n",
    "            )\n",
    "            return (node, row, col, edge, index.numel())\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f'NeighborLoader found invalid type: {type(data)}')\n",
    "            \n",
    "    def call_weighted_sample_cpp(self, index: Union[List[int], Tensor], weight_index):\n",
    "        \n",
    "        #print(\"Here....\")\n",
    "        \n",
    "            \n",
    "        if not isinstance(index, torch.LongTensor):\n",
    "            index = torch.LongTensor(index)\n",
    "        \n",
    "        if issubclass(self.data_cls, Data):        \n",
    "            node, row, col, edge = sampling_module.weighted_sample(\n",
    "                self.colptr,\n",
    "                self.row,\n",
    "                index,\n",
    "                self.num_neighbors,\n",
    "                self.weights[weight_index],\n",
    "                self.replace,\n",
    "                self.directed,\n",
    "            )\n",
    "            return (node, row, col, edge, index.numel())\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f'NeighborLoader found invalid type: {type(data)}')\n",
    "\n",
    "    \n",
    "    def __call__(self, index: Union[List[int], Tensor]):\n",
    "        \n",
    "        output = []\n",
    "        a = 1\n",
    "        b = 1\n",
    "        \n",
    "        for i,method in enumerate(self.weight_funcs):\n",
    "            if method == 'random':\n",
    "#                 start = time.time()\n",
    "                output.append(self.call__original(index))            \n",
    "#                 end = time.time()\n",
    "#                 a = end-start\n",
    "#                 print(\"Random sample:\", end-start)\n",
    "            else:\n",
    "#                 start = time.time()\n",
    "                #output.append(self.weighted_sample(index, i)) ##my sparse tensor based implementation\n",
    "                #output.append(self.call_weighted_sample(index, i)) ## c inspired implementation\n",
    "                output.append(self.call_weighted_sample_cpp(index, i)) ## modified c installation\n",
    "#                 end = time.time()\n",
    "#                 b = end-start\n",
    "#                 print(\"Biased sample:\", end-start)\n",
    "        \n",
    "#         print(\"Scale:\", b/a)\n",
    "        \n",
    "        return output            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f769d19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_node_type = get_input_node_type(data.train_mask)\n",
    "# replace=False\n",
    "# directed=True\n",
    "# sampler = CustomNeighborSampler(data, [1,1],replace, directed,input_node_type)\n",
    "# print(sampler.call__original([1,0]))\n",
    "\n",
    "# print(\"-\"*100)\n",
    "\n",
    "# print(sampler.__call__([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93c3bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedNeighborLoader(BaseDataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Union[Data, HeteroData],\n",
    "        num_neighbors: NumNeighbors,\n",
    "        input_nodes: InputNodes = None,\n",
    "        replace: bool = False,\n",
    "        directed: bool = True,\n",
    "        transform: Callable = None,\n",
    "        neighbor_sampler: Optional[CustomNeighborSampler] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if 'dataset' in kwargs:\n",
    "            del kwargs['dataset']\n",
    "        if 'collate_fn' in kwargs:\n",
    "            del kwargs['collate_fn']   \n",
    "            \n",
    "        if 'save_dir' not in kwargs:\n",
    "            kwargs['save_dir'] = 'weights/'\n",
    "        if 'recompute' not in kwargs:\n",
    "            kwargs['recompute'] =False\n",
    "\n",
    "        # Save for PyTorch Lightning:\n",
    "        self.data = data\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.input_nodes = input_nodes\n",
    "        self.replace = replace\n",
    "        self.directed = directed\n",
    "        self.transform = transform\n",
    "        self.neighbor_sampler = neighbor_sampler\n",
    "        self.log = kwargs['log']\n",
    "        \n",
    "        self.weight_funcs = kwargs['weight_func']\n",
    "\n",
    "        if neighbor_sampler is None:\n",
    "            input_node_type = get_input_node_type(input_nodes)\n",
    "            self.neighbor_sampler = CustomNeighborSampler(data, num_neighbors,\n",
    "                                                    replace, directed,\n",
    "                                                    input_node_type,**kwargs)\n",
    "\n",
    "            self.weights = self.neighbor_sampler.weights\n",
    "            \n",
    "        if 'weight_func' in kwargs:\n",
    "            del kwargs['weight_func']\n",
    "        if 'params' in kwargs:\n",
    "            del kwargs['params']\n",
    "        if 'log' in kwargs:\n",
    "            del kwargs['log']\n",
    "        if 'save_dir' in kwargs:\n",
    "            del kwargs['save_dir']\n",
    "        if 'recompute' in kwargs:\n",
    "            del kwargs['recompute']\n",
    "        \n",
    "\n",
    "        return super().__init__(get_input_node_indices(self.data, input_nodes),\n",
    "                                collate_fn=self.neighbor_sampler, **kwargs)\n",
    "\n",
    "    def transform_fn(self, out: Any) -> Union[Data, HeteroData]:\n",
    "        \n",
    "        batch_data = []\n",
    "        \n",
    "        if isinstance(self.data, Data):            \n",
    "            for i, (node, row, col, edge, batch_size) in enumerate(out):\n",
    "                \n",
    "                #node, row, col, edge, batch_size = out     \n",
    "                \n",
    "#                 if self.weight_funcs[i] == 'random':        \n",
    "#                     b_data = filter_data(self.data, node, row, col, edge, self.neighbor_sampler.perm)                \n",
    "#                 else:\n",
    "#                     b_data = filter_data(self.data, node, row, col, edge, None)\n",
    "                    \n",
    "                b_data = filter_data(self.data, node, row, col, edge, self.neighbor_sampler.perm)\n",
    "                b_data.weight = self.weights[i][self.neighbor_sampler.perm[edge]]\n",
    "            \n",
    "#                 print('-'*50)\n",
    "#                 print(node, row, col, edge, batch_size)\n",
    "#                 print(b_data)\n",
    "#                 print('-'*50)\n",
    "                \n",
    "                b_data.batch_size = batch_size                \n",
    "                #b_data.weight = self.weights[i][edge]                \n",
    "                batch_data.append(b_data)\n",
    "                \n",
    "#         print(batch_data)\n",
    "\n",
    "        if len(batch_data)==1:\n",
    "            batch_data=batch_data[0]\n",
    "\n",
    "        return batch_data if self.transform is None else self.transform(data)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c6dca82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_node_type(input_nodes: InputNodes) -> Optional[str]:\n",
    "    if isinstance(input_nodes, str):\n",
    "        return input_nodes\n",
    "    if isinstance(input_nodes, (list, tuple)):\n",
    "        assert isinstance(input_nodes[0], str)\n",
    "        return input_nodes[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_input_node_indices(data: Union[Data, HeteroData],\n",
    "                           input_nodes: InputNodes) -> Sequence:\n",
    "    if isinstance(data, Data) and input_nodes is None:\n",
    "        return range(data.num_nodes)\n",
    "   \n",
    "    if isinstance(input_nodes, Tensor):\n",
    "        if input_nodes.dtype == torch.bool:\n",
    "            input_nodes = input_nodes.nonzero(as_tuple=False).view(-1)\n",
    "        input_nodes = input_nodes.tolist()\n",
    "\n",
    "    assert isinstance(input_nodes, Sequence)\n",
    "    return input_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8e3c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_index\n",
    "# #data.weight = torch.Tensor(list(range(data.edge_index.shape[1])))+100\n",
    "# data.weight = torch.ones(data.edge_index.shape[1])\n",
    "# data.weight.to(data.edge_index.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73391444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8331a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loader = WeightedNeighborLoader(data, batch_size=2, num_neighbors=[-1], input_nodes=data.train_mask, save_dir=dataset.processed_dir, num_workers=0, shuffle=False)\n",
    "# loader = WeightedNeighborLoader(data, batch_size=2, num_neighbors=[-1,-1], input_nodes=data.train_mask, num_workers=0, shuffle=False)\n",
    "# batch  = next(iter(loader))\n",
    "# print(batch.weight)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7bf33254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_data in loader:\n",
    "    \n",
    "#     print(\"*\"*50)\n",
    "#     print(batch_data.edge_index)\n",
    "#     print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbb922",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec49c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 Metric:  cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 34/34 [00:00<00:00, 2366.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  knn\n",
      "Metric:  cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 34/34 [00:00<00:00, 1571.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  submodular\n",
      "Loading weights  fastlink\n",
      "Loading weights  link-sub\n",
      "Loading weights  link-nn\n",
      "cosine\n",
      "Pool Size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Nodes: 100%|██████████| 34/34 [00:01<00:00, 31.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  apricot\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34], weights=[7, 156])\n",
      "[Data(x=[14, 34], edge_index=[2, 18], y=[14], train_mask=[14], val_mask=[14], test_mask=[14], weights=[7, 156], weight=[18], batch_size=4), Data(x=[15, 34], edge_index=[2, 18], y=[15], train_mask=[15], val_mask=[15], test_mask=[15], weights=[7, 156], weight=[18], batch_size=4), Data(x=[18, 34], edge_index=[2, 22], y=[18], train_mask=[18], val_mask=[18], test_mask=[18], weights=[7, 156], weight=[22], batch_size=4), Data(x=[19, 34], edge_index=[2, 24], y=[19], train_mask=[19], val_mask=[19], test_mask=[19], weights=[7, 156], weight=[24], batch_size=4), Data(x=[19, 34], edge_index=[2, 21], y=[19], train_mask=[19], val_mask=[19], test_mask=[19], weights=[7, 156], weight=[21], batch_size=4), Data(x=[16, 34], edge_index=[2, 20], y=[16], train_mask=[16], val_mask=[16], test_mask=[16], weights=[7, 156], weight=[20], batch_size=4), Data(x=[16, 34], edge_index=[2, 20], y=[16], train_mask=[16], val_mask=[16], test_mask=[16], weights=[7, 156], weight=[20], batch_size=4)]\n",
      "[Data(x=[14, 34], edge_index=[2, 18], y=[14], train_mask=[14], val_mask=[14], test_mask=[14], weights=[7, 156], weight=[18], batch_size=4), Data(x=[14, 34], edge_index=[2, 21], y=[14], train_mask=[14], val_mask=[14], test_mask=[14], weights=[7, 156], weight=[21], batch_size=4), Data(x=[17, 34], edge_index=[2, 22], y=[17], train_mask=[17], val_mask=[17], test_mask=[17], weights=[7, 156], weight=[22], batch_size=4), Data(x=[17, 34], edge_index=[2, 24], y=[17], train_mask=[17], val_mask=[17], test_mask=[17], weights=[7, 156], weight=[24], batch_size=4), Data(x=[15, 34], edge_index=[2, 18], y=[15], train_mask=[15], val_mask=[15], test_mask=[15], weights=[7, 156], weight=[18], batch_size=4), Data(x=[18, 34], edge_index=[2, 20], y=[18], train_mask=[18], val_mask=[18], test_mask=[18], weights=[7, 156], weight=[20], batch_size=4), Data(x=[20, 34], edge_index=[2, 22], y=[20], train_mask=[20], val_mask=[20], test_mask=[20], weights=[7, 156], weight=[22], batch_size=4)]\n",
      "**************************************************\n",
      "[Data(x=[14, 34], edge_index=[2, 18], y=[14], train_mask=[14], val_mask=[14], test_mask=[14], weights=[7, 156], weight=[18], batch_size=4), Data(x=[17, 34], edge_index=[2, 20], y=[17], train_mask=[17], val_mask=[17], test_mask=[17], weights=[7, 156], weight=[20], batch_size=4), Data(x=[17, 34], edge_index=[2, 20], y=[17], train_mask=[17], val_mask=[17], test_mask=[17], weights=[7, 156], weight=[20], batch_size=4), Data(x=[14, 34], edge_index=[2, 19], y=[14], train_mask=[14], val_mask=[14], test_mask=[14], weights=[7, 156], weight=[19], batch_size=4), Data(x=[14, 34], edge_index=[2, 20], y=[14], train_mask=[14], val_mask=[14], test_mask=[14], weights=[7, 156], weight=[20], batch_size=4), Data(x=[18, 34], edge_index=[2, 22], y=[18], train_mask=[18], val_mask=[18], test_mask=[18], weights=[7, 156], weight=[22], batch_size=4), Data(x=[17, 34], edge_index=[2, 22], y=[17], train_mask=[17], val_mask=[17], test_mask=[17], weights=[7, 156], weight=[22], batch_size=4)]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    \n",
    "    from ipynb.fs.full.Dataset import get_data\n",
    "    \n",
    "    data, dataset = get_data('karate', log=False, h_score=True) \n",
    "    \n",
    "#     n=7\n",
    "#     x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "#     y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "#     edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "#     edge_index = edge_index-1\n",
    "    \n",
    "#     mask = torch.zeros(n, dtype=torch.bool)\n",
    "#     mask[[1,3]] = True\n",
    "    \n",
    "#     data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = mask, val_mask = mask)    \n",
    "#     print(data)\n",
    "\n",
    "    #weight_func=['knn', 'submodular'],\n",
    "    #weight_func=['knn','submodular','random', 'link-nn', 'link-sub'],\n",
    "    #weight_func=['knn','submodular'],\n",
    "    \n",
    "    loader = WeightedNeighborLoader(data, batch_size=16, num_neighbors=[2,2], \n",
    "                                    input_nodes=data.train_mask, \n",
    "                                    log = True,\n",
    "                                    num_workers=0, shuffle=False, \n",
    "                                    weight_func = ['random','knn','submodular','fastlink','link-sub','link-nn','apricot'],                                    \n",
    "                                    #weight_func = ['apricot'],                                    \n",
    "                                    params={\n",
    "                                        'knn':{'metric':'cosine'},\n",
    "                                        'submodular':{'metric':'cosine'},\n",
    "                                        'link-nn':{'value':'min'},\n",
    "                                        'link-sub':{'value':'max'},\n",
    "                                        'apricot':{'sub_func':'facility','metric':'cosine'}\n",
    "                                    },\n",
    "                                    replace = False,\n",
    "                                    directed = True,\n",
    "                                    save_dir = 'Results/',\n",
    "                                    recompute = False\n",
    "                                   )\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    batch  = next(iter(loader))\n",
    "    #print(batch.weight)\n",
    "    print(batch)\n",
    "    \n",
    "#     from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "#     loader = NeighborLoader(data, batch_size=1, num_neighbors=[-1], input_nodes=data.train_mask, num_workers=0, shuffle=False)\n",
    "    \n",
    "    batch  = next(iter(loader))\n",
    "#     print(batch.weight)\n",
    "    print(batch)\n",
    "    \n",
    "    for batch_data in loader:\n",
    "        print(\"*\"*50)\n",
    "        print(batch_data)\n",
    "        #print(batch_data.edge_index)\n",
    "        #print(batch_data.node_id)\n",
    "        #print(batch_data.weight)\n",
    "        print(\"*\"*50)\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf9869",
   "metadata": {},
   "source": [
    "# Scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7fb1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DeviceDir\n",
    "\n",
    "# DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "# device, NUM_PROCESSORS = DeviceDir.get_device()\n",
    "\n",
    "# data, dataset = get_data('Reddit', DIR=DIR+'RedditPyg204', log = False) \n",
    "\n",
    "# (row, col) = data.edge_index\n",
    "# data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "75b96252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_func=['random', 'knn']\n",
    "\n",
    "# params={\n",
    "#     'knn':{'metric':'cosine'},\n",
    "#     'submodular':{'metric':'cosine'}\n",
    "# }\n",
    "\n",
    "# loader = WeightedNeighborLoader(data, batch_size=1024, num_neighbors=[4, 4], input_nodes=data.train_mask, log = True,\n",
    "#                                 num_workers=0, shuffle=False, weight_func=weight_func,params=params,\n",
    "#                                replace=False, directed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca566013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(loader.weights)\n",
    "# batch  = next(iter(loader))\n",
    "# for b in batch:\n",
    "#     print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7ba9ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(loader):\n",
    "#     print(\"-\"*50)\n",
    "#     for b in batch:\n",
    "#         #print(b)\n",
    "#         None\n",
    "#     if i>100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6f095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8aee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cacf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
