{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ffef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "kernel_name = os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))\n",
    "\n",
    "if kernel_name == 'py38cu11':\n",
    "    import ctypes\n",
    "    ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcusparse.so.11\");\n",
    "    ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcublas.so.11\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4b641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset, Amazon, Coauthor\n",
    "\n",
    "#import ipynb.fs.full.utils.MoonGraph as MoonGraph\n",
    "# import utils.MoonGraph as MoonGraph\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import torch_geometric.utils.subgraph as subgraph\n",
    "\n",
    "#import torch_geometric.utils.assortativity as assortativity #pytorch geometric's latest version has this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c20145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gamma, uniform, expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e02fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.typing import Adj, SparseTensor\n",
    "from torch_geometric.utils import coalesce, degree\n",
    "from torch_geometric.utils.to_dense_adj import to_dense_adj\n",
    "\n",
    "\n",
    "def assortativity(edge_index: Adj) -> float:\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        adj: SparseTensor = edge_index\n",
    "        row, col, _ = adj.coo()\n",
    "    else:\n",
    "        assert isinstance(edge_index, Tensor)\n",
    "        row, col = edge_index\n",
    "\n",
    "    device = row.device\n",
    "    out_deg = degree(row, dtype=torch.long)\n",
    "    in_deg = degree(col, dtype=torch.long)\n",
    "    degrees = torch.unique(torch.cat([out_deg, in_deg]))\n",
    "    mapping = row.new_zeros(degrees.max().item() + 1)\n",
    "    mapping[degrees] = torch.arange(degrees.size(0), device=device)\n",
    "\n",
    "    # Compute degree mixing matrix (joint probability distribution) `M`\n",
    "    num_degrees = degrees.size(0)\n",
    "    src_deg = mapping[out_deg[row]]\n",
    "    dst_deg = mapping[in_deg[col]]\n",
    "\n",
    "    pairs = torch.stack([src_deg, dst_deg], dim=0)\n",
    "    occurrence = torch.ones(pairs.size(1), device=device)\n",
    "    pairs, occurrence = coalesce(pairs, occurrence)\n",
    "    M = to_dense_adj(pairs, edge_attr=occurrence, max_num_nodes=num_degrees)[0]\n",
    "    # normalization\n",
    "    M /= M.sum()\n",
    "\n",
    "    # numeric assortativity coefficient, computed by\n",
    "    # Pearson correlation coefficient of the node degrees\n",
    "    x = y = degrees.float()\n",
    "    a, b = M.sum(0), M.sum(1)\n",
    "\n",
    "    vara = (a * x**2).sum() - ((a * x).sum())**2\n",
    "    varb = (b * x**2).sum() - ((b * x).sum())**2\n",
    "    xy = torch.outer(x, y)\n",
    "    ab = torch.outer(a, b)\n",
    "    out = (xy * (M - ab)).sum() / (vara * varb).sqrt()\n",
    "    return out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6c6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/SitaoLuan/ACM-GNN/tree/main/synthetic-experiments\n",
    "#https://github.com/siddhartha047/Geom_GCN_pytorch_implementation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def func(feature):\n",
    "\n",
    "    f = list(map(int, feature.split(',')))\n",
    "    \n",
    "    return f\n",
    "\n",
    "def get_heterophily(root, DATASET_NAME='texas', train=0.6, val=0.2, test=0.2):\n",
    "    \n",
    "    edge_file = root+'/'+DATASET_NAME+'/out1_graph_edges.txt'\n",
    "    id_feature_label_file = root+'/'+DATASET_NAME+'/out1_node_feature_label.txt'\n",
    "    \n",
    "    edges = pd.read_csv(edge_file, sep='\\t', header=0)\n",
    "    id_feature_label = pd.read_csv(id_feature_label_file, sep='\\t', header=0)\n",
    "    \n",
    "#     print(edges)\n",
    "#     print(id_feature_label)\n",
    "    \n",
    "    edge_index = torch.LongTensor(edges.values.tolist()).T\n",
    "    node_id  = torch.LongTensor(id_feature_label['node_id'].values.tolist())\n",
    "    y = torch.LongTensor(id_feature_label['label'].values.tolist())\n",
    "    x = id_feature_label['feature'].apply(func)\n",
    "    x = torch.Tensor(x.values.tolist())\n",
    "    \n",
    "    N = len(node_id)\n",
    "    indexs = list(range(N))\n",
    "    \n",
    "    train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "    val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    \n",
    "    data = Data(edge_index=edge_index, \n",
    "                x=x, node_id=node_id, \n",
    "                y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#get_heterophily('/scratch/gilbreth/das90/Dataset/heterophily/','squirrel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd02868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_film(root, DATASET_NAME='film', train=0.6, val=0.2, test=0.2):\n",
    "    \n",
    "    file = root+DATASET_NAME+'/'    \n",
    "    f = open(file+'class_map.json')\n",
    "    class_map = json.load(f)\n",
    "    class_map = {int(key):int(value) for key, value in class_map.items()}\n",
    "    #print(class_map)\n",
    "    f.close()    \n",
    "    \n",
    "    y = list(class_map.values())\n",
    "    x = np.load(file+'feats.npy')    \n",
    "    #print(x.shape)\n",
    "    \n",
    "#     f = open(file+'id_map.json')\n",
    "#     id_map = json.load(f)\n",
    "#     id_map = {int(key):int(value) for key, value in id_map.items()}    \n",
    "#     #print(id_map)\n",
    "#     f.close()\n",
    "    \n",
    "    #target = pd.read_csv(file+'film_target.csv', sep=',', header=0)\n",
    "    #print(target)\n",
    "    #target['new_id']=target['id'].apply(lambda x: id_map[x])\n",
    "    \n",
    "    \n",
    "    edges = pd.read_csv(file+'film_edges.csv', sep=',', header=0)\n",
    "    \n",
    "    u = edges['id1'].values.tolist()\n",
    "    v = edges['id2'].values.tolist()\n",
    "    \n",
    "    edge_index=[u,v]\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "    edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    indexs = list(range(N))\n",
    "    train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "    val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "#     train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "#     val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    \n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    \n",
    "    data = Data(edge_index=edge_index, \n",
    "                x=x,\n",
    "                y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#get_film('/scratch/gilbreth/das90/Dataset/heterophily/','film')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c29260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroDataset(Dataset):\n",
    "    def __init__(self, root, dataset_name, train=0.6, val=0.2, test=0.2,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(None, transform, pre_transform, pre_filter)\n",
    "        \n",
    "        self.root = root\n",
    "        self.dataset_name=dataset_name\n",
    "        self.degree=degree\n",
    "        self.train=train\n",
    "        self.val=val\n",
    "        self.test=test\n",
    "        \n",
    "        if dataset_name == 'film':\n",
    "            self.data = get_film(root,dataset_name, train, val, test)\n",
    "        else:\n",
    "            self.data = get_heterophily(root,dataset_name, train, val, test)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return self.dataset_name\n",
    "    \n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return self.root\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self.data.x.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return max(self.data.y).item()+1\n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "# dataset = HeteroDataset('/scratch/gilbreth/das90/Dataset/heterophily/','texas')\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c51d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=False):\n",
    "    \n",
    "    N = data.x.shape[0]\n",
    "    indexs = list(range(N))\n",
    "    \n",
    "    if random_state:\n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test))\n",
    "    else:        \n",
    "        train_index, test_index = train_test_split(indexs, test_size=val+test, random_state=1)\n",
    "        val_index, test_index = train_test_split(test_index, test_size=test/(val+test), random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=bool)\n",
    "    train_mask[train_index]=True    \n",
    "    val_mask = torch.zeros(N, dtype=bool)\n",
    "    val_mask[val_index]=True\n",
    "    test_mask = torch.zeros(N, dtype=bool)\n",
    "    test_mask[test_index]=True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2767bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(DATASET_NAME='Cora', DIR=None, params=None, train=None, random_state=False, log=True, h_score=False, split_no=0):\n",
    "    \n",
    "    if DIR is not None:\n",
    "        print('Looking at: ',DIR)    \n",
    "    elif os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "        DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "    elif os.uname()[1].find('unimodular')==0:\n",
    "        DIR='/scratch2/das90/Dataset/'\n",
    "    elif os.uname()[1].find('Siddharthas')==0:\n",
    "        DIR='/Users/siddharthashankardas/Purdue/Dataset/'  \n",
    "    else:\n",
    "        DIR='./Dataset/'\n",
    "\n",
    "    Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    RESULTS_DIR=DIR+'RESULTS/'\n",
    "    Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if log:\n",
    "        print(\"Data directory: \", DIR)\n",
    "        print(\"Result directory:\", RESULTS_DIR)\n",
    "    \n",
    "    from torch_geometric.datasets import Planetoid,  KarateClub, CitationFull\n",
    "    from torch_geometric.transforms import NormalizeFeatures\n",
    "    from torch_geometric.datasets import Reddit, Reddit2\n",
    "    \n",
    "    #DATASET_NAME='Cora' #\"Cora\", \"CiteSeer\", \"PubMed\"\n",
    "\n",
    "    if DATASET_NAME in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "        dataset = Planetoid(root=DIR+'Planetoid', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME in ['cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed']:\n",
    "        #['cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed']\n",
    "        dataset = CitationFull(root=DIR+'Citation', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"Reddit2\":\n",
    "        #dataset = Reddit2(root=DIR+'Reddit2', transform=NormalizeFeatures())\n",
    "        dataset = Reddit2(root=DIR+'Reddit2')\n",
    "\n",
    "    elif DATASET_NAME == \"Reddit\":\n",
    "        #dataset = Reddit(root=DIR+'Reddit', transform=NormalizeFeatures())\n",
    "        dataset = Reddit(root=DIR+'Reddit')\n",
    "        \n",
    "    elif DATASET_NAME == \"AmazonProducts\":\n",
    "        #dataset = AmazonProducts(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "        dataset = AmazonProducts(root=DIR+'AmazonProducts')\n",
    "        \n",
    "    elif DATASET_NAME in ['Computers', 'Photo']:\n",
    "        #dataset = Amazon(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "        dataset = Amazon(root=DIR+'Amazon/', name = DATASET_NAME)        \n",
    "        \n",
    "\n",
    "    elif DATASET_NAME in ['CS', 'Physics']:\n",
    "        dataset = Coauthor(root=DIR+'Coauthor/', name = DATASET_NAME)        \n",
    "\n",
    "        \n",
    "    elif DATASET_NAME == \"Moon\":\n",
    "        dataset = MoonGraph.MoonDataset(n_samples=100, degree=5, train=0.5)    \n",
    "        G, data =dataset[0]\n",
    "    \n",
    "    elif DATASET_NAME == \"karate\":\n",
    "        dataset = KarateClub()        \n",
    "        data = dataset[0]\n",
    "        data.val_mask = ~data.train_mask\n",
    "        data.test_mask = data.val_mask\n",
    "    \n",
    "    elif DATASET_NAME == \"Fake\":\n",
    "        dataset = FakeDataset(num_graphs = 1, \n",
    "                              avg_num_nodes = 2000, \n",
    "                              avg_degree = 10, \n",
    "                              num_channels = 64, \n",
    "                              edge_dim = 0, \n",
    "                              num_classes = 10, \n",
    "                              task = 'auto', \n",
    "                              is_undirected = True,                               \n",
    "                              transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME == \"OGB_MAG\":\n",
    "        #dataset = OGB_MAG(root=DIR+'OGB_MAG', preprocess='metapath2vec', transform=NormalizeFeatures())\n",
    "        dataset = OGB_MAG(root=DIR+'OGB_MAG2', preprocess='metapath2vec')\n",
    "        data = dataset[0]        \n",
    "        print(dataset, data, data['paper'])\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    elif DATASET_NAME == \"Flickr\":\n",
    "        dataset = Flickr(root=DIR+'Flickr')\n",
    "    \n",
    "    elif DATASET_NAME == \"Yelp\":\n",
    "        dataset = Yelp(root=DIR+'Yelp')\n",
    "    \n",
    "    elif DATASET_NAME == \"PPI\":\n",
    "        \n",
    "        dataset = PPI(root=DIR+'PPI')\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    ###heterophilic dataset\n",
    "    #https://github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py\n",
    "    elif DATASET_NAME in [\"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import LINKXDataset\n",
    "        \n",
    "        dataset = LINKXDataset(DIR+'/Heterophilous/', DATASET_NAME)    \n",
    "        #transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME in [\"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import HeterophilousGraphDataset\n",
    "        \n",
    "        dataset = HeterophilousGraphDataset(DIR+'/Heterophilous/', DATASET_NAME)    \n",
    "        #transform=NormalizeFeatures())\n",
    "    elif DATASET_NAME == 'Actor':\n",
    "        from ipynb.fs.full.HeterophilousDataset import Actor\n",
    "        \n",
    "        dataset = Actor(root=DIR+'/Heterophilous/Actor')\n",
    "        #transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME in [\"Cornell\", \"Texas\", \"Wisconsin\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import WebKB\n",
    "        \n",
    "        dataset = WebKB(DIR+'/Heterophilous/', DATASET_NAME)\n",
    "        #transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME in [\"Chameleon\", \"Crocodile\", \"Squirrel\"]:\n",
    "        from ipynb.fs.full.HeterophilousDataset import WikipediaNetwork\n",
    "        \n",
    "        if DATASET_NAME == 'Crocodile':\n",
    "            dataset = WikipediaNetwork(DIR+'/Heterophilous/', DATASET_NAME.lower(), geom_gcn_preprocess= False)\n",
    "        else:        \n",
    "            dataset = WikipediaNetwork(DIR+'/Heterophilous/', DATASET_NAME.lower())\n",
    "            #transform=NormalizeFeatures())\n",
    "    \n",
    "    #implemented heterophily\n",
    "    elif DATASET_NAME in ['chameleon','cornell','film', 'squirrel', 'texas','wisconsin']:\n",
    "        dataset = HeteroDataset(DIR+'/heterophily/', DATASET_NAME)       \n",
    "    \n",
    "    else:    \n",
    "        raise Exception('dataset not found')\n",
    "\n",
    "    if DATASET_NAME in ['Moon', 'karate']:\n",
    "        #MoonGraph.draw_blobs_data(G, data)        \n",
    "        None\n",
    "    else:\n",
    "        data = dataset[0]  # Get the first graph object.\n",
    "        if 'train_mask' not in data:\n",
    "            data = train_val_test_mask(data, train=0.6, val=0.2, test=0.2, random_state=random_state)\n",
    "            \n",
    "        elif data.train_mask.dim()>1:\n",
    "            data.train_mask = data.train_mask[:,split_no]\n",
    "            data.val_mask = data.val_mask[:,split_no]\n",
    "            data.test_mask = data.test_mask[:,split_no]\n",
    "            \n",
    "        \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "        \n",
    "    if log:\n",
    "        print()\n",
    "        print(f'Dataset: {dataset}:')\n",
    "        print('======================')\n",
    "        print(f'Number of graphs: {len(dataset)}')\n",
    "        print(f'Number of features: {dataset.num_features}')\n",
    "        print(f'Number of classes: {dataset.num_classes}')\n",
    "        print()\n",
    "        print(data)\n",
    "        print('===========================================================================================================')\n",
    "\n",
    "        # Gather some statistics about the graph.\n",
    "        print(f'Number of nodes: {data.num_nodes}')\n",
    "        print(f'Number of edges: {data.num_edges}')\n",
    "        print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "        print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "        print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "        print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "        print(f'Has self-loops: {data.has_self_loops()}')\n",
    "        print(f'Is undirected: {data.is_undirected()}')\n",
    "    \n",
    "    if len(data.y.shape) == 1:\n",
    "        labels = data.y\n",
    "    else:\n",
    "        if log: print(\"Testing homophily by converting multi-label to one-label\")\n",
    "        labels = data.y.argmax(dim=1)\n",
    "    \n",
    "    if h_score:\n",
    "        print(homophily(data.edge_index, labels, method='node'),homophily(data.edge_index, labels, method='edge'), end=' ')\n",
    "        \n",
    "        try:\n",
    "            esen = homophily(data.edge_index, labels, method='edge_insensitive')\n",
    "        except:\n",
    "            esen = -1        \n",
    "        print(esen, end=' ')            \n",
    "        print(assortativity(data.edge_index))\n",
    "        \n",
    "    \n",
    "    return data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da8603c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'karate','Moon','Fake',\n",
    "    \"Cora\", \"CiteSeer\", \"PubMed\",'cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed',\n",
    "    'chameleon','cornell','film', 'squirrel', 'texas','wisconsin',\n",
    "    'Computers', 'Photo',\n",
    "    'CS', 'Physics',\n",
    "    'Flickr','Yelp',\n",
    "    \"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\",\n",
    "    \"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\",\n",
    "    \"Actor\", \n",
    "    \"Cornell\", \"Texas\", \"Wisconsin\", \n",
    "    \"Chameleon\", \"Squirrel\", #\"Crocodile\",\n",
    "    'Reddit',\n",
    "    'Reddit2',\n",
    "    'AmazonProducts' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd8f096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: HeteroDataset():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1703\n",
      "Number of classes: 5\n",
      "\n",
      "Data(x=[251, 1703], edge_index=[2, 515], y=[251], node_id=[251], train_mask=[251], val_mask=[251], test_mask=[251])\n",
      "===========================================================================================================\n",
      "Number of nodes: 251\n",
      "Number of edges: 515\n",
      "Average node degree: 2.05\n",
      "Number of training nodes: 150\n",
      "Training node label rate: 0.60\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: False\n",
      "0.17191579937934875 0.19611650705337524 0.0839187353849411 -0.27229174971580505\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':   \n",
    "#     data, dataset = get_data('karate', log=True, h_score = True)\n",
    "#     print(sum(torch.where(data.train_mask==True)))\n",
    "#     data, dataset = get_data('Cora', train=0.2, random_state=True)\n",
    "#     print(sum(torch.where(data.train_mask==True)))\n",
    "    \n",
    "    data, dataset = get_data('wisconsin', log=True, h_score = True)\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766100a2",
   "metadata": {},
   "source": [
    "## Homophily plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99973697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MoonGraph.MoonDataset(n_samples=1000, degree=5, train=0.5)    \n",
    "# G, data =dataset[0] \n",
    "# # MoonGraph.draw_blobs_data(G,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7173632f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'Cora'\n",
    "# data, dataset = get_data(DATASET_NAME)\n",
    "# # data.y = data.y.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae5b5f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hp_compute(data):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    hp_data=np.zeros(N)\n",
    "    \n",
    "    pbar = tqdm(total=N)\n",
    "    pbar.set_description(f'Nodes')\n",
    "        \n",
    "    for i in range(N):\n",
    "        row, col, edge = adj[i,:].coo()      \n",
    "        \n",
    "        if len(col) == 0: \n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        y_current = data.y[i]\n",
    "        y_neighbors = data.y[col]\n",
    "        \n",
    "        match  = (y_neighbors==y_current).type(torch.int).sum()\n",
    "        \n",
    "        hp_data[i] = match.item()/len(y_neighbors)\n",
    "        \n",
    "        #print(y_current, y_neighbors, match, hp_data[i])\n",
    "        \n",
    "        pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    return hp_data\n",
    "\n",
    "#hp_data = hp_compute(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f86059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_hist(data, DATASET_NAME=''):\n",
    "    # Generate some random data\n",
    "    #data = np.random.normal(size=1000)\n",
    "    # Calculate the probability density function\n",
    "    density, bins, _ = plt.hist(data, density=True, bins=25)\n",
    "\n",
    "    # Plot the probability density function\n",
    "    plt.plot(bins[:-1], density)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title('Probability Density Function'+' '+DATASET_NAME)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "#hp_data = [0.01, 0.1,0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e81ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp_data = hp_compute(data)\n",
    "# pd_hist(hp_data, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a0669",
   "metadata": {},
   "source": [
    "## Gephi Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e694ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "# from ipynb.fs.full.utils.GNNutils import save_gephi_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9c8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gephi_graph(data, DATASET_NAME):\n",
    "    G_fillename = '/scratch/gilbreth/das90/Dataset/GephiGraphs/'+DATASET_NAME\n",
    "\n",
    "    if os.path.exists(G_fillename)==False:\n",
    "        print(\"Graph is not found, creating it....\")\n",
    "        G = to_networkx(data, to_undirected=True)\n",
    "        nx.write_gpickle(G, G_fillename)\n",
    "        print(\"Done\")\n",
    "    else:\n",
    "        print(\"Loading Saved graph...\")\n",
    "        G = nx.read_gpickle(G_fillename)\n",
    "        print(\"Done\")\n",
    "    \n",
    "    graph_name = '/scratch/gilbreth/das90/Dataset/GephiGraphs/'+DATASET_NAME+'_original'\n",
    "    save_gephi_graph(G, data.y, graph_name)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992277c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [\n",
    "#     'karate','Moon','Fake',\n",
    "#     \"Cora\", \"CiteSeer\", \"PubMed\",'cora', 'cora_ml', 'citeseer', 'dblp', 'pubmed',\n",
    "#     'chameleon','cornell','film', 'squirrel', 'texas','wisconsin',\n",
    "#     'Computers', 'Photo',\n",
    "#     'CS', 'Physics',\n",
    "#     'Flickr','Yelp',\n",
    "#     \"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\",\n",
    "#     \"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\",\n",
    "#     \"Actor\", \n",
    "#     \"Cornell\", \"Texas\", \"Wisconsin\", \n",
    "#     \"Chameleon\", \"Squirrel\", #\"Crocodile\",\n",
    "#     'Reddit',\n",
    "#     'Reddit2',\n",
    "#     'AmazonProducts' \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85647b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for DATASET_NAME in datasets:\n",
    "#     #DATASET_NAME = 'karate'\n",
    "#     print('-'*20,DATASET_NAME,'-'*20)\n",
    "#     data, dataset = get_data(DATASET_NAME)\n",
    "#     if data.y.ndim >1:\n",
    "#         data.y = data.y.argmax(dim=1)\n",
    "#     create_gephi_graph(data,DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75f1377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_gephi_graph(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ab45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# data = Data(x=x, y=y, edge_index = edge_index)\n",
    "# draw_graph(edge_index, y, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907de0f",
   "metadata": {},
   "source": [
    "# Create Synthetic Graphs of different homophily scores and degree of different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa26cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data('karate', log=True, h_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a98a0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_class(data):\n",
    "    \n",
    "    unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "#     print(unique_elements)\n",
    "#     print(counts)\n",
    "    mincount = min(counts).item()\n",
    "#     print(mincount)\n",
    "\n",
    "    subset = []\n",
    "\n",
    "    for i in unique_elements:\n",
    "        indexes = ((data.y == i).nonzero()).view(-1).numpy()\n",
    "#         print(len(indexes))\n",
    "#         print(indexes)\n",
    "        samples = np.random.choice(indexes, mincount, replace=False)\n",
    "        subset.extend(samples)\n",
    "\n",
    "#     print(len(subset), mincount*num_classes)\n",
    "\n",
    "    node_idx = torch.tensor(subset)\n",
    "    edge_index = subgraph(node_idx, data.edge_index)[0]\n",
    "    \n",
    "#     print(node_idx, edge_index)\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "\n",
    "    data.num_nodes = node_idx.size(0)\n",
    "    data.edge_index = edge_index\n",
    "\n",
    "    for key, item in data:\n",
    "        if key in ['edge_index', 'num_nodes']:\n",
    "            continue\n",
    "        if isinstance(item, torch.Tensor) and item.size(0) == N:\n",
    "            data[key] = item[node_idx]\n",
    "        elif isinstance(item, torch.Tensor) and item.size(0) == E:\n",
    "            data[key] = item[edge_idx]\n",
    "        else:\n",
    "            data[key] = item\n",
    "    \n",
    "    return data\n",
    "\n",
    "# print(data.edge_index)\n",
    "# data = balance_class(data)\n",
    "# print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfd20078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic(data, d=5, h=0.8, train=0.6, random_state=None, log=True, balance = False):\n",
    "    \n",
    "    if balance:\n",
    "        data = balance_class(data)\n",
    "        \n",
    "    num_class = max(data.y)+1\n",
    "    cluster_vertices = {}\n",
    "    for c in range(num_class):\n",
    "        indices = torch.where(data.y == c)[0]\n",
    "        cluster_vertices[c]=indices\n",
    "    \n",
    "    n = data.num_nodes\n",
    "    \n",
    "#     intra_d = np.random.multinomial(n*d*h, np.ones(n)/n, size=1)[0]\n",
    "#     inter_d = np.random.multinomial(n*d*(1-h), np.ones(n)/n, size=1)[0]\n",
    "    \n",
    "    intra_d = np.round(np.ones(n)*(d*h)).astype(int)\n",
    "    inter_d = np.round(np.ones(n)*(d*(1-h))).astype(int)\n",
    "    \n",
    "#     print(intra_d, inter_d)\n",
    "    \n",
    "    edge_index = [[],[]]\n",
    "    \n",
    "    for c in range(num_class):\n",
    "        intra_vertices = cluster_vertices[c]\n",
    "        inter_vertices = torch.cat([value for key, value in cluster_vertices.items() if key!=c])\n",
    "        \n",
    "        intra_vertices = intra_vertices.numpy()\n",
    "        inter_vertices = inter_vertices.numpy()\n",
    "        \n",
    "#         print('Class:', c)\n",
    "#         print(intra_vertices)\n",
    "#         print(inter_vertices)\n",
    "        \n",
    "        for u in intra_vertices:\n",
    "            \n",
    "            ## remove self-loop\n",
    "            #intra_vertices_u = \n",
    "            \n",
    "            intra_v = np.random.choice(intra_vertices, min(len(intra_vertices),intra_d[u]), replace=False)\n",
    "            inter_v = np.random.choice(inter_vertices, min(len(inter_vertices),inter_d[u]), replace=False)\n",
    "            \n",
    "            Vs = np.append(intra_v,inter_v)\n",
    "            Us = np.repeat(u,len(Vs))\n",
    "            \n",
    "            unique_elements, counts = np.unique(inter_v, return_counts=True)\n",
    "            \n",
    "#             print(\"-\"*50)\n",
    "#             print(u)\n",
    "#             print(Vs)\n",
    "#             print(unique_elements)\n",
    "#             print(counts)\n",
    "#             print(\"-\"*50)\n",
    "            \n",
    "#             if len(unique_elements)< (num_class-1):\n",
    "#                 print('un du toa:')\n",
    "#                 print(unique_elements)\n",
    "#                 print(counts)\n",
    "            \n",
    "            edge_index[0].extend(Us)\n",
    "            edge_index[1].extend(Vs)\n",
    "             \n",
    "#             edge_index[1].extend(Us)\n",
    "#             edge_index[0].extend(Vs)\n",
    "    \n",
    "    data.edge_index = torch.LongTensor(edge_index)\n",
    "    \n",
    "    if train is not None:\n",
    "        val = (1-train)/2.0\n",
    "        data = train_val_test_mask(data, train=train, val=val, test=1-(train+val), random_state=random_state)\n",
    "    \n",
    "    if log:\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='node'):0.4f}\",end=' ')\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='edge'):0.4f}\",end=' ')\n",
    "        print(f\"{homophily(data.edge_index, data.y, method='edge_insensitive'):0.4f}\",end=' ')\n",
    "        print(f\"{assortativity(data.edge_index):0.4f}\", end=' ')\n",
    "        \n",
    "        unique_elements, counts = torch.unique(data.y, return_counts=True)\n",
    "        print(unique_elements.tolist(), counts.tolist(), end=' ')\n",
    "\n",
    "        print(f'{int(data.train_mask.sum()) / data.num_nodes:.4f}', end=' ')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# data = generate_synthetic(data, d=10, h=0.0, train=0.6, random_state=None, log=True, balance = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae83985",
   "metadata": {},
   "source": [
    "# Diversity heterophily definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbd5ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b65ef2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'chameleon'\n",
    "# data, dataset = get_data(DATASET_NAME, log = False)\n",
    "# data = generate_synthetic(data, d=10, h=0.0, train=0.6, random_state=1, log=True, balance = False)\n",
    "# num_classes = dataset.num_classes\n",
    "# print(num_classes)\n",
    "# N = data.num_nodes\n",
    "# E = data.num_edges\n",
    "\n",
    "# adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "#     value=torch.arange(E, device=data.edge_index.device),\n",
    "#     sparse_sizes=(N, N))\n",
    "\n",
    "# # if len(col)==0:\n",
    "# #     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11f4bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_uniformity(col_labels, num_classes, cur_y, log=True, ):    \n",
    "    if log:\n",
    "        print(col_labels)\n",
    "        print(cur_y)\n",
    "        \n",
    "    unique_elements, counts = np.unique(col_labels, return_counts=True)\n",
    "    \n",
    "    if log:\n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "    \n",
    "    index = np.where(unique_elements == cur_y)[0]\n",
    "    unique_elements = np.delete(unique_elements, index)\n",
    "    counts = np.delete(counts, index)\n",
    "\n",
    "    if log: \n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "        \n",
    "    if len(unique_elements)<(num_classes-1):\n",
    "        if log:\n",
    "            print('unexpted: ')\n",
    "            print(unique_elements)\n",
    "            print(counts)\n",
    "        return -1\n",
    "    \n",
    "    expected_frequency = sum(counts) / (num_classes-1)\n",
    "    observed_frequency = np.array(counts)\n",
    "\n",
    "    if log:\n",
    "        print(expected_frequency)\n",
    "        print(observed_frequency)\n",
    "\n",
    "    chi2, p_value = chisquare(observed_frequency, f_exp=expected_frequency)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Chi-squared statistic:\", chi2)\n",
    "        print(\"p-value:\", p_value)\n",
    "\n",
    "    significance_level = 0.05\n",
    "    if p_value < significance_level:\n",
    "        if log:print(\"The distribution significantly deviates from the uniform distribution.\")\n",
    "        return 0\n",
    "    else:\n",
    "        if log:print(\"The distribution is similar to the uniform distribution.\")\n",
    "        return 1\n",
    "\n",
    "    \n",
    "# u = 0\n",
    "# row, col, ed = adj[u,:].coo()   \n",
    "# col_labels = data.y[col]\n",
    "\n",
    "# print(row)\n",
    "# print(col)\n",
    "# print(col_labels)\n",
    "# print(len(col_labels))\n",
    "\n",
    "# cur_y = data.y[u].item()\n",
    "# print(cur_y)\n",
    "# given_uniformity(col_labels, num_classes, cur_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bfed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uniformity(data, num_classes, log=True):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for u in range(N):            \n",
    "        row, col, edge_index = adj[u,:].coo()   \n",
    "        col_labels = data.y[col]\n",
    "        cur_y = data.y[u].item()\n",
    "        is_diverse = given_uniformity(col_labels, num_classes, cur_y, log = False)\n",
    "        \n",
    "#         if is_diverse ==-1:\n",
    "#             print(u)\n",
    "        \n",
    "        count+=max(0,is_diverse)\n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:\n",
    "        pbar.close()\n",
    "    \n",
    "    return count, count/N\n",
    "    \n",
    "# test_uniformity(data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f2a8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_homophily(data, matrix_tu = 'affinity'):\n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "\n",
    "    adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "\n",
    "    A = torch.zeros((N,N))\n",
    "    edges = data.edge_index.t()\n",
    "    A[edges[:,0], edges[:,1]] = 1\n",
    "    A[edges[:,1], edges[:,0]] = 1    \n",
    "    \n",
    "    I = torch.eye(N)\n",
    "    D = torch.diag(torch.Tensor(torch.sum(A,dim=1)))\n",
    "#     print(A)\n",
    "#     print(D)\n",
    "\n",
    "    Ai = A + I\n",
    "    DiInv = torch.diag(torch.Tensor(1/torch.sum(Ai,dim=1)))\n",
    "    Arw = torch.mm(DiInv, Ai)\n",
    "    \n",
    "    M = Arw\n",
    "    \n",
    "    if matrix_tu == 'affinity':\n",
    "        M = Arw\n",
    "    elif matrix_tu == 'laplacian':\n",
    "        M = I - Arw\n",
    "    \n",
    "    AX = torch.mm(M,data.x)\n",
    "    #print(AX)\n",
    "    SAX = torch.mm(AX,AX.T)    \n",
    "    #print(SAX)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for u in range(N):            \n",
    "        row, col, edge_index = adj[u,:].coo()\n",
    "        \n",
    "        col_labels = data.y[col]\n",
    "        cur_y = data.y[u]\n",
    "        \n",
    "#         print(col_labels)\n",
    "#         print(cur_y)\n",
    "        \n",
    "        zu_eq = col[torch.where(col_labels == cur_y)[0]]\n",
    "        zu_neq = col[torch.where(col_labels != cur_y)[0]]\n",
    "        #print(zu_eq, zu_neq)\n",
    "        \n",
    "        left_u = torch.mean(SAX[u,zu_eq])\n",
    "        right_u = torch.mean(SAX[u,zu_neq])\n",
    "        #print(left_u, right_u)\n",
    "        \n",
    "        if torch.isnan(right_u):\n",
    "            count+=1        \n",
    "                            \n",
    "        elif (left_u >= right_u):\n",
    "            count+=1\n",
    "        \n",
    "    return count/N\n",
    "    \n",
    "    \n",
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME, log=False)\n",
    "# data = generate_synthetic(data, d=10, h = 0.2, train=0.6, random_state=1, log=False, balance = True)\n",
    "# data.x = F.one_hot(data.y).float()\n",
    "\n",
    "# print(agg_homophily(data, 'affinity'))\n",
    "# print(agg_homophily(data, 'laplacian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ca993f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME, log=False)\n",
    "# data = generate_synthetic(data, d=10, h = 0.2, train=0.6, random_state=1, log=False, balance = True)\n",
    "\n",
    "# num_classes = dataset.num_classes\n",
    "# print(num_classes)\n",
    "# N = data.num_nodes\n",
    "# E = data.num_edges\n",
    "\n",
    "# adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "#     value=torch.arange(E, device=data.edge_index.device),\n",
    "#     sparse_sizes=(N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8a8184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def scipyentropy(labels, num_class = 10):\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_probs = class_counts / len(labels)\n",
    "    \n",
    "    print(class_probs)\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy_value = entropy(class_probs, base=2)\n",
    "    print(\"Entropy:\", entropy_value)\n",
    "    \n",
    "    return entropy_value\n",
    "\n",
    "# labels = [0, 1, 2, 0, 1, 1, 3, 2, 2, 4, 4, 5, 5, 6, 6, 6, 6]\n",
    "# scipyentropy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86910536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_entropy(class_probabilities, num_classes):\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for probability in class_probabilities:\n",
    "        if probability != 0.0:\n",
    "            entropy += -probability * math.log2(probability)\n",
    "\n",
    "    max_entropy = -math.log2(1/num_classes)\n",
    "    \n",
    "    #print(max_entropy)    \n",
    "    normalized_entropy = entropy / max_entropy\n",
    "    \n",
    "    return normalized_entropy\n",
    "\n",
    "# Example usage\n",
    "# class_probabilities = [0.9, 0.0, 0.0, 0.0, 0.1]\n",
    "# entropy_value = compute_entropy(class_probabilities, len(class_probabilities))\n",
    "# print(\"Normalized Entropy:\", entropy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a0a2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_entropy(col_labels, num_classes, cur_y, log=True):    \n",
    "    if log:\n",
    "        print(col_labels)\n",
    "        print(cur_y)\n",
    "        \n",
    "    unique_elements, counts = np.unique(col_labels, return_counts=True)\n",
    "    \n",
    "    if log:\n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "    \n",
    "    index = np.where(unique_elements == cur_y)[0]\n",
    "    unique_elements = np.delete(unique_elements, index)\n",
    "    counts = np.delete(counts, index)\n",
    "\n",
    "    if log: \n",
    "        print(unique_elements)\n",
    "        print(counts)\n",
    "        \n",
    "    \n",
    "    prob = counts/sum(counts)\n",
    "    if log: print(prob)\n",
    "    \n",
    "    return compute_entropy(prob, num_classes-1)\n",
    "\n",
    "\n",
    "# u = 0\n",
    "# row, col, ed = adj[u,:].coo()   \n",
    "# col_labels = data.y[col]\n",
    "\n",
    "# print(row)\n",
    "# print(col)\n",
    "# print(col_labels)\n",
    "# print(len(col_labels))\n",
    "\n",
    "# cur_y = data.y[u].item()\n",
    "# print(cur_y)\n",
    "# node_entropy(col_labels, num_classes, cur_y, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3e7884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_entropy(data, num_classes, log=True):\n",
    "    \n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    \n",
    "    adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    if log:\n",
    "        pbar = tqdm(total=N)\n",
    "        pbar.set_description(f'Nodes')\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for u in range(N):            \n",
    "        row, col, edge_index = adj[u,:].coo()   \n",
    "        col_labels = data.y[col]\n",
    "        cur_y = data.y[u].item()\n",
    "        is_diverse = node_entropy(col_labels, num_classes, cur_y, log = False)\n",
    "        \n",
    "#         if is_diverse ==-1:\n",
    "#             print(u)\n",
    "        \n",
    "        count+= is_diverse\n",
    "        if log:\n",
    "            pbar.update(1)\n",
    "    if log:\n",
    "        pbar.close()\n",
    "    \n",
    "    return count, count/N\n",
    "\n",
    "# total_entropy(data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c29bcdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hetero():\n",
    "    d = 20\n",
    "    for h in np.array(range(0,21))/20:\n",
    "        DATASET_NAME = 'Cora'\n",
    "        data, dataset = get_data(DATASET_NAME, log=False)\n",
    "        data = generate_synthetic(data, d=d, h = h, train=0.6, random_state=1, log=False, balance = True)\n",
    "        num_classes = dataset.num_classes\n",
    "        print('d ', d, ' h', h, end=' ')\n",
    "        count, score = test_uniformity(data, num_classes, log=False)\n",
    "        print(count, score, end = ' ')\n",
    "#         Hagg_aff = agg_homophily(data, matrix_tu = 'affinity')\n",
    "#         Hagg_lap = agg_homophily(data, matrix_tu = 'laplacian')\n",
    "#         print(Hagg_aff, Hagg_lap)\n",
    "        total_en, en_score = total_entropy(data, num_classes, log=False)\n",
    "        print(total_en, en_score)\n",
    "    \n",
    "# test_hetero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b24615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
