{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44415f5a",
   "metadata": {},
   "source": [
    "## Get Cuda and Processor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ac319c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A10\n",
      "cuda\n",
      "Cpu count:  32\n"
     ]
    }
   ],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "782d8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.PygDataset import get_data\n",
    "from ipynb.fs.full.PygDataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot\n",
    "from ipynb.fs.full.DglDataset import get_dgl_as_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d11e7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38ae0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7371bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d0c7f",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f2deccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0846860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f832",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbd4df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, ChebConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from ipynb.fs.full.SpatialConv import SpatialConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5744498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNNconv = SAGEConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007e8b8",
   "metadata": {},
   "source": [
    "### GNN option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd0e048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNHomophily(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "             \n",
    "        self.conv1 = GNNconv(num_features, hidden_channels)\n",
    "        #self.conv2 = GNNconv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = GNNconv(hidden_channels,num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GNNHeterophily(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "             \n",
    "        self.conv1 = ChebConv(num_features, hidden_channels, K=2, normalization='sym')\n",
    "        #self.conv2 = GNNconv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = ChebConv(hidden_channels,num_classes, K=2, normalization='sym')\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        hidden = int(hidden_channels/2)\n",
    "        \n",
    "        self.gnn1 = GNNHomophily(num_features, num_classes, hidden_channels)\n",
    "        self.gnn2 = GNNHeterophily(num_features, num_classes, hidden_channels)\n",
    "        \n",
    "        #self.gnn1 = GNNHomophily(num_features, hidden, hidden_channels)\n",
    "        #self.gnn2 = GNNHeterophily(num_features, hidden, hidden_channels)\n",
    "        self.p = dropout\n",
    "        self.com_lin = nn.Linear(hidden*2, num_classes)\n",
    "        \n",
    "        \n",
    "#         self.T = 2        \n",
    "#         self.layer_norm_a1 =  nn.LayerNorm(num_classes)\n",
    "#         self.layer_norm_s1 =  nn.LayerNorm(num_classes)\n",
    "        \n",
    "#         self.alpha_a1 = nn.Linear(num_classes, 1)\n",
    "#         self.alpha_s1 = nn.Linear(num_classes, 1)\n",
    "#         self.w1 = nn.Linear(self.T, self.T)\n",
    "        \n",
    "        #self.reset_parameters()\n",
    "            \n",
    "#     def reset_parameters(self):\n",
    "#         std_att = 1. / math.sqrt(self.w1.weight.size(1))\n",
    "#         std_att_vec = 1. / math.sqrt( self.alpha_a1.weight.size(1))\n",
    "        \n",
    "#         self.alpha_s1.weight.data.uniform_(-std_att, std_att)\n",
    "#         self.alpha_i1.weight.data.uniform_(-std_att, std_att)\n",
    "        \n",
    "#         self.layer_norm_a1.reset_parameters()\n",
    "#         self.layer_norm_s1.reset_parameters()        \n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "        #out = model(batch_data.x, batch_data.edge_index, batch_data.weight)\n",
    "        #out = model(batch_data.x, batch_data.edge_index, batch_data.edge_weight)\n",
    "        #out = model(batch_data.x, batch_data.edge_index)\n",
    "        \n",
    "        x1 = self.gnn1(batch_data[0].x, batch_data[0].edge_index)\n",
    "        return x1        \n",
    "        \n",
    "#         x2 = self.gnn2(batch_data[1].x, batch_data[1].edge_index)\n",
    "#         return x2\n",
    "        \n",
    "        a1 = F.relu(x1)\n",
    "        #a1 = self.layer_norm_a1(a1)\n",
    "        a1 = F.dropout(a1, p=self.p, training=self.training)\n",
    "        \n",
    "        s1 = F.relu(x2)\n",
    "        #s1 = self.layer_norm_s1(s1)\n",
    "        s1 = F.dropout(s1, p=self.p, training=self.training)\n",
    "        \n",
    "        used = batch_data[0].batch_size\n",
    "        \n",
    "        x = torch.cat([a1[:used,:], s1[:used,:]], dim=-1)\n",
    "        x = self.com_lin(x)\n",
    "        \n",
    "        \n",
    "#         ala1 = torch.sigmoid(self.alpha_a1(a1))\n",
    "#         als1 = torch.sigmoid(self.alpha_s1(s1))        \n",
    "        \n",
    "#         alpha1 = F.softmax(self.w1(torch.cat([ala1, als1],dim=-1)/self.T), dim=1)                \n",
    "#         x = torch.mm(torch.diag(alpha1[:,0]),a1) + torch.mm(torch.diag(alpha1[:,1]),s1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b17b8e",
   "metadata": {},
   "source": [
    "## GNN Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e11e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "#from ipynb.fs.full.a1AGS_Node_Sampler import WeightedNeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ac415e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    \n",
    "    if args.log_info:    \n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()    \n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "            batch_data = [batch_data, batch_data] ##comment this\n",
    "            \n",
    "            batch_data = [b.to(device) for b in batch_data]\n",
    "            used = batch_data[0].batch_size\n",
    "            \n",
    "            out = model(batch_data)\n",
    "                   \n",
    "            out=out[:used,:]\n",
    "            pred = out.argmax(dim=1)            \n",
    "\n",
    "            y_true.append(batch_data[0].y[:used].detach().cpu().numpy())\n",
    "            y_pred.append(pred.detach().cpu().numpy())\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(used)\n",
    "              \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "                    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85fa0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs=100, train_neighbors=[-1,10], test_neighbors=[-1,10]):\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "        \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    if data.y.ndim == 1:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    batch_size=1024     \n",
    "    #worker = min(8,int(sum(data.train_mask)/batch_size))\n",
    "    worker = 0\n",
    "    if args.log_info:\n",
    "        print(\"Worker: \", worker)\n",
    "    \n",
    "#     weight_func=['knn','submodular']\n",
    "    weight_func=['random', 'random']\n",
    "    params={\n",
    "        'knn':{'metric':'cosine'},\n",
    "        'submodular':{'metric':'cosine'},\n",
    "    }\n",
    "    \n",
    "#     loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info)\n",
    "\n",
    "#     train_loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info)\n",
    "    \n",
    "#     val_loader = WeightedNeighborLoader(data, input_nodes=data.val_mask,num_neighbors=test_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info)\n",
    "    \n",
    "#     test_loader = WeightedNeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info)\n",
    "\n",
    "    loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False)\n",
    "\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False)\n",
    "    \n",
    "    val_loader = NeighborLoader(data, input_nodes=data.val_mask,num_neighbors=test_neighbors, batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False)\n",
    "    \n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False)\n",
    "    \n",
    "    \n",
    "    top_k_accs = []    \n",
    "    best_acc=0  \n",
    "    \n",
    "    train_losses=[]\n",
    "    val_accuracies=[]\n",
    "    train_accuracies=[]\n",
    "    test_accuracies=[]\n",
    "    \n",
    "    num_iteration = epochs\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=int(sum(data.train_mask)))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        for i,batch_data in enumerate(loader):  \n",
    "            \n",
    "            #print(batch_data)\n",
    "            \n",
    "            \n",
    "            batch_data = [batch_data, batch_data] ##comment this\n",
    "            \n",
    "            batch_data = [b.to(device) for b in batch_data]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(batch_data)\n",
    "            \n",
    "            used = batch_data[0].batch_size #int(sum(batch_data.train_mask))\n",
    "            #loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "            loss = criterion(out[:used], batch_data[0].y[:used])\n",
    "            #loss = F.cross_entropy(out[:used], batch_data[0].y[:used])\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            total_loss += loss.item() * used\n",
    "            total_examples += used\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(used)\n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "        \n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        #print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}', end = ', ')                \n",
    "        \n",
    "        if args.log_info:\n",
    "            train_acc=test(model, train_loader,data.train_mask,'Train')            \n",
    "            train_accuracies.append(train_acc.item())        \n",
    "        else:\n",
    "            train_acc = 0 ; train_accuracies.append(train_acc)\n",
    "        \n",
    "        if args.log_info:\n",
    "            val_acc = test(model, val_loader,data.val_mask,'Validation')\n",
    "            val_accuracies.append(val_acc.item())\n",
    "        else:\n",
    "            val_acc = 0 ; val_accuracies.append(val_acc)\n",
    "\n",
    "        test_acc = test(model, test_loader,data.test_mask,'Test')\n",
    "        test_accuracies.append(test_acc.item())\n",
    "        #print(f'Epoch: {epoch:03d}, Test: {test_acc:.4f}')\n",
    "        \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        #print(f'Epoch: {epoch:03d}, Std dev: {std_dev:.4f}')\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "        if epoch>=5 and std_dev<=1e-3:\n",
    "            num_iteration = epoch\n",
    "            \n",
    "            if args.log_info:                \n",
    "                print(\"Iteration for convergence: \", epoch)\n",
    "            break\n",
    "    \n",
    "#     print(\"Loss:\", train_losses)\n",
    "#     print(\"Train: \", train_accuracies)\n",
    "#     print(\"Val: \", val_accuracies)\n",
    "#     print(\"Test: \", test_accuracies)\n",
    "    \n",
    "    acc_file = open(\"Plots/results.txt\",'w') \n",
    "    acc_file.write(str(train_losses))\n",
    "    acc_file.write(str(train_accuracies))\n",
    "    acc_file.write(str(val_accuracies))\n",
    "    acc_file.write(str(test_accuracies))\n",
    "    acc_file.close()     \n",
    "    \n",
    "    if args.log_info:\n",
    "        #save_plot([val_accuracies], labels=['Validation'], name='Plots/Validation', yname='Accuracy', xname='Epoch')    \n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Plots/Validation', yname='Accuracy', xname='Epoch')\n",
    "        \n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "        \n",
    "    best_acc = max(test_accuracies)\n",
    "    \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34bee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCNperformanceSampler(data, dataset, epochs=1, train_neighbors=[-1,-1], test_neighbors=[-1,-1]):        \n",
    "    \n",
    "    model = GNN(dataset.num_features, dataset.num_classes, hidden_channels=128).to(device)\n",
    "    \n",
    "    if args.log_info: print(model)    \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)    \n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "586728b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at:  /scratch/gilbreth/das90/Dataset/\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: Genius(1):\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 12\n",
      "Number of classes: 2\n",
      "\n",
      "Data(x=[421961, 12], edge_index=[2, 984979], y=[421961], train_mask=[421961], val_mask=[421961], test_mask=[421961])\n",
      "===========================================================================================================\n",
      "Number of nodes: 421961\n",
      "Number of edges: 984979\n",
      "Average node degree: 2.33\n",
      "Number of training nodes: 253176\n",
      "Training node label rate: 0.60\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: False\n",
      "0.47737330198287964 0.617572546005249 0.21700388193130493 -0.1065090000629425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[421961, 12], edge_index=[2, 984979], y=[421961], train_mask=[421961], val_mask=[421961], test_mask=[421961])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, dataset = get_data('genius', DIR, log=True, h_score=True, split_no=0)\n",
    "data\n",
    "\n",
    "# _, data2 = get_dgl_as_pyg(DATASET_NAME='Squirrel', DIR=DIR, split_no=0)\n",
    "# data2\n",
    "# print(data.edge_index)\n",
    "# print(data2.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "316bdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.y\n",
    "# indexs = (data.train_mask == True).nonzero().view(-1)\n",
    "# data.y[indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd6878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "84449afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input should be contiguous.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m.\u001b[39mlog_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     accuracy, itr, model \u001b[38;5;241m=\u001b[39m  GCNperformanceSampler(data, dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, train_neighbors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m], test_neighbors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(itr, accuracy)\n\u001b[1;32m      8\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[0;32mIn[78], line 7\u001b[0m, in \u001b[0;36mGCNperformanceSampler\u001b[0;34m(data, dataset, epochs, train_neighbors, test_neighbors)\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GNN(dataset\u001b[38;5;241m.\u001b[39mnum_features, dataset\u001b[38;5;241m.\u001b[39mnum_classes, hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlog_info: \u001b[38;5;28mprint\u001b[39m(model)    \n\u001b[0;32m----> 7\u001b[0m best_acc, num_iteration \u001b[38;5;241m=\u001b[39m train(model, data, epochs, train_neighbors\u001b[38;5;241m=\u001b[39mtrain_neighbors, test_neighbors\u001b[38;5;241m=\u001b[39mtest_neighbors)    \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_acc, num_iteration, model\n",
      "Cell \u001b[0;32mIn[77], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, train_neighbors, test_neighbors)\u001b[0m\n\u001b[1;32m     23\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m'\u001b[39m:{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmodular\u001b[39m\u001b[38;5;124m'\u001b[39m:{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     26\u001b[0m     }\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#                               batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False, \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#                               weight_func=weight_func, params=params, log=args.log_info)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#                               batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#                               weight_func=weight_func, params=params, log=args.log_info)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     loader \u001b[38;5;241m=\u001b[39m NeighborLoader(data, input_nodes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_mask,num_neighbors\u001b[38;5;241m=\u001b[39mtrain_neighbors, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mworker, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m NeighborLoader(data, input_nodes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_mask,num_neighbors\u001b[38;5;241m=\u001b[39mtrain_neighbors, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mworker, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m NeighborLoader(data, input_nodes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mval_mask,num_neighbors\u001b[38;5;241m=\u001b[39mtest_neighbors, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mworker, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/torch_geometric/loader/neighbor_loader.py:198\u001b[0m, in \u001b[0;36mNeighborLoader.__init__\u001b[0;34m(self, data, num_neighbors, input_nodes, input_time, replace, directed, disjoint, temporal_strategy, time_attr, transform, transform_sampler_output, is_sorted, filter_per_worker, neighbor_sampler, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived conflicting \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_time\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m arguments: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_time\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neighbor_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     neighbor_sampler \u001b[38;5;241m=\u001b[39m NeighborSampler(\n\u001b[1;32m    199\u001b[0m         data,\n\u001b[1;32m    200\u001b[0m         num_neighbors\u001b[38;5;241m=\u001b[39mnum_neighbors,\n\u001b[1;32m    201\u001b[0m         replace\u001b[38;5;241m=\u001b[39mreplace,\n\u001b[1;32m    202\u001b[0m         directed\u001b[38;5;241m=\u001b[39mdirected,\n\u001b[1;32m    203\u001b[0m         disjoint\u001b[38;5;241m=\u001b[39mdisjoint,\n\u001b[1;32m    204\u001b[0m         temporal_strategy\u001b[38;5;241m=\u001b[39mtemporal_strategy,\n\u001b[1;32m    205\u001b[0m         time_attr\u001b[38;5;241m=\u001b[39mtime_attr,\n\u001b[1;32m    206\u001b[0m         is_sorted\u001b[38;5;241m=\u001b[39mis_sorted,\n\u001b[1;32m    207\u001b[0m         share_memory\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    211\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    212\u001b[0m     node_sampler\u001b[38;5;241m=\u001b[39mneighbor_sampler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    219\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/torch_geometric/sampler/neighbor_sampler.py:62\u001b[0m, in \u001b[0;36mNeighborSampler.__init__\u001b[0;34m(self, data, num_neighbors, replace, directed, disjoint, temporal_strategy, time_attr, is_sorted, share_memory)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_time \u001b[38;5;241m=\u001b[39m data[time_attr] \u001b[38;5;28;01mif\u001b[39;00m time_attr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Convert the graph data into CSC format for sampling:\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperm \u001b[38;5;241m=\u001b[39m to_csc(\n\u001b[1;32m     63\u001b[0m         data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, share_memory\u001b[38;5;241m=\u001b[39mshare_memory,\n\u001b[1;32m     64\u001b[0m         is_sorted\u001b[38;5;241m=\u001b[39mis_sorted, src_node_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_time)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m==\u001b[39m DataType\u001b[38;5;241m.\u001b[39mheterogeneous:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_types, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_types \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmetadata()\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/torch_geometric/sampler/utils.py:72\u001b[0m, in \u001b[0;36mto_csc\u001b[0;34m(data, device, share_memory, is_sorted, src_node_time)\u001b[0m\n\u001b[1;32m     70\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sorted:\n\u001b[0;32m---> 72\u001b[0m         row, col, perm \u001b[38;5;241m=\u001b[39m sort_csc(row, col, src_node_time)\n\u001b[1;32m     74\u001b[0m     colptr \u001b[38;5;241m=\u001b[39m index2ptr(col, data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/torch_geometric/sampler/utils.py:22\u001b[0m, in \u001b[0;36msort_csc\u001b[0;34m(row, col, src_node_time)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort_csc\u001b[39m(\n\u001b[1;32m     17\u001b[0m     row: Tensor,\n\u001b[1;32m     18\u001b[0m     col: Tensor,\n\u001b[1;32m     19\u001b[0m     src_node_time: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor]:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m src_node_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m         col, perm \u001b[38;5;241m=\u001b[39m index_sort(col)\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m row[perm], col, perm\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# We use `np.lexsort` to sort based on multiple keys.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# TODO There does not seem to exist a PyTorch equivalent yet :(\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/torch_geometric/utils/sort.py:27\u001b[0m, in \u001b[0;36mindex_sort\u001b[0;34m(inputs, max_value)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mWITH_INDEX_SORT:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pyg_lib\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mindex_sort(inputs, max_value\u001b[38;5;241m=\u001b[39mmax_value)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/pyg_lib/ops/__init__.py:257\u001b[0m, in \u001b[0;36mindex_sort\u001b[0;34m(inputs, max_value)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msort(inputs)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mpyg\u001b[38;5;241m.\u001b[39mindex_sort(inputs, max_value)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py311cu117pyg200/lib/python3.11/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input should be contiguous."
     ]
    }
   ],
   "source": [
    "accs=[]\n",
    "itrs =[]\n",
    "\n",
    "for i in range(5):\n",
    "    args.log_info = False\n",
    "    accuracy, itr, model =  GCNperformanceSampler(data, dataset, epochs=150, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "    print(itr, accuracy)\n",
    "    accs.append(accuracy)\n",
    "    itrs.append(itr)\n",
    "    \n",
    "print(int(np.mean(itrs)),np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cab8c",
   "metadata": {},
   "source": [
    "## Get command line interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "361d4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':    \n",
    "    \n",
    "#     n=7\n",
    "#     x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "#     y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "#     edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "#     edge_index = edge_index-1\n",
    "    \n",
    "#     mask = torch.zeros(n, dtype=torch.bool)\n",
    "#     mask[[1,3]] = True\n",
    "    \n",
    "#     test_data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = mask, val_mask = mask)    \n",
    "#     print(test_data)\n",
    "    \n",
    "    \n",
    "#     None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83ca9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48f76e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# #X = model(data.x.to(device),data.edge_index.to(device), data.weight.to(device))\n",
    "# X = model(data.x.to(device),data.edge_index.to(device))\n",
    "# X = X.detach().to('cpu')\n",
    "# y = data.y.to('cpu')\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1e750fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Create a t-SNE model with 2 components and a perplexity of 30\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=42, learning_rate='auto', init='random')\n",
    "\n",
    "# # Fit and transform the data to the 2D t-SNE space\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# # Plot the data in the 2D t-SNE space, colored by class\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c5d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f746ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py311cu117pyg200 Kernel)",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
