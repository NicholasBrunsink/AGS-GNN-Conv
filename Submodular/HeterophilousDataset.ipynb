{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220c565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce76277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def one_hot(\n",
    "    index: Tensor,\n",
    "    num_classes: Optional[int] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Taskes a one-dimensional :obj:`index` tensor and returns a one-hot\n",
    "    encoded representation of it with shape :obj:`[*, num_classes]` that has\n",
    "    zeros everywhere except where the index of last dimension matches the\n",
    "    corresponding value of the input tensor, in which case it will be :obj:`1`.\n",
    "\n",
    "    .. note::\n",
    "        This is a more memory-efficient version of\n",
    "        :meth:`torch.nn.functional.one_hot` as you can customize the output\n",
    "        :obj:`dtype`.\n",
    "\n",
    "    Args:\n",
    "        index (torch.Tensor): The one-dimensional input tensor.\n",
    "        num_classes (int, optional): The total number of classes. If set to\n",
    "            :obj:`None`, the number of classes will be inferred as one greater\n",
    "            than the largest class value in the input tensor.\n",
    "            (default: :obj:`None`)\n",
    "        dtype (torch.dtype, optional): The :obj:`dtype` of the output tensor.\n",
    "    \"\"\"\n",
    "    if index.dim() != 1:\n",
    "        raise ValueError(\"'index' tensor needs to be one-dimensional\")\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = int(index.max()) + 1\n",
    "\n",
    "    out = torch.zeros((index.size(0), num_classes), dtype=dtype,\n",
    "                      device=index.device)\n",
    "    return out.scatter_(1, index.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e2295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import ssl\n",
    "import sys\n",
    "import urllib\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.data.makedirs import makedirs\n",
    "\n",
    "\n",
    "def download_url(url: str, folder: str, log: bool = True,\n",
    "                 filename: Optional[str] = None):\n",
    "    r\"\"\"Downloads the content of an URL to a specific folder.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL.\n",
    "        folder (str): The folder.\n",
    "        log (bool, optional): If :obj:`False`, will not print anything to the\n",
    "            console. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    if filename is None:\n",
    "        filename = url.rpartition('/')[2]\n",
    "        filename = filename if filename[0] == '?' else filename.split('?')[0]\n",
    "\n",
    "    path = osp.join(folder, filename)\n",
    "\n",
    "    if osp.exists(path):  # pragma: no cover\n",
    "        if log and 'pytest' not in sys.modules:\n",
    "            print(f'Using existing file {filename}', file=sys.stderr)\n",
    "        return path\n",
    "\n",
    "    if log and 'pytest' not in sys.modules:\n",
    "        print(f'Downloading {url}', file=sys.stderr)\n",
    "\n",
    "    makedirs(folder)\n",
    "\n",
    "    context = ssl._create_unverified_context()\n",
    "    data = urllib.request.urlopen(url, context=context)\n",
    "\n",
    "    with open(path, 'wb') as f:\n",
    "        # workaround for https://bugs.python.org/issue42853\n",
    "        while True:\n",
    "            chunk = data.read(10 * 1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b7b200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset #download_url\n",
    "#from torch_geometric.utils import one_hot\n",
    "\n",
    "\n",
    "class LINKXDataset(InMemoryDataset):\n",
    "    r\"\"\"A variety of non-homophilous graph datasets from the `\"Large Scale\n",
    "    Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple\n",
    "    Methods\" <https://arxiv.org/abs/2110.14446>`_ paper.\n",
    "\n",
    "    .. note::\n",
    "        Some of the datasets provided in :class:`LINKXDataset` are from other\n",
    "        sources, but have been updated with new features and/or labels.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"penn94\"`, :obj:`\"reed98\"`,\n",
    "            :obj:`\"amherst41\"`, :obj:`\"cornell5\"`, :obj:`\"johnshopkins55\"`,\n",
    "            :obj:`\"genius\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    github_url = ('https://github.com/CUAI/Non-Homophily-Large-Scale/'\n",
    "                  'raw/master/data')\n",
    "    gdrive_url = 'https://drive.google.com/uc?confirm=t&'\n",
    "\n",
    "    facebook_datasets = [\n",
    "        'penn94', 'reed98', 'amherst41', 'cornell5', 'johnshopkins55'\n",
    "    ]\n",
    "\n",
    "    datasets = {\n",
    "        'penn94': {\n",
    "            'data.mat': f'{github_url}/facebook100/Penn94.mat'\n",
    "        },\n",
    "        'reed98': {\n",
    "            'data.mat': f'{github_url}/facebook100/Reed98.mat'\n",
    "        },\n",
    "        'amherst41': {\n",
    "            'data.mat': f'{github_url}/facebook100/Amherst41.mat',\n",
    "        },\n",
    "        'cornell5': {\n",
    "            'data.mat': f'{github_url}/facebook100/Cornell5.mat'\n",
    "        },\n",
    "        'johnshopkins55': {\n",
    "            'data.mat': f'{github_url}/facebook100/Johns%20Hopkins55.mat'\n",
    "        },\n",
    "        'genius': {\n",
    "            'data.mat': f'{github_url}/genius.mat'\n",
    "        },\n",
    "        'wiki': {\n",
    "            'wiki_views2M.pt':\n",
    "            f'{gdrive_url}id=1p5DlVHrnFgYm3VsNIzahSsvCD424AyvP',\n",
    "            'wiki_edges2M.pt':\n",
    "            f'{gdrive_url}id=14X7FlkjrlUgmnsYtPwdh-gGuFla4yb5u',\n",
    "            'wiki_features2M.pt':\n",
    "            f'{gdrive_url}id=1ySNspxbK-snNoAZM7oxiWGvOnTRdSyEK'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    splits = {\n",
    "        'penn94': f'{github_url}/splits/fb100-Penn94-splits.npy',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root: str, name: str,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        assert self.name in self.datasets.keys()\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        names = list(self.datasets[self.name].keys())\n",
    "        if self.name in self.splits:\n",
    "            names += [self.splits[self.name].split('/')[-1]]\n",
    "        return names\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for filename, path in self.datasets[self.name].items():\n",
    "            download_url(path, self.raw_dir, filename=filename)\n",
    "        if self.name in self.splits:\n",
    "            download_url(self.splits[self.name], self.raw_dir)\n",
    "\n",
    "    def _process_wiki(self):\n",
    "\n",
    "        paths = {x.split('/')[-1]: x for x in self.raw_paths}\n",
    "        x = torch.load(paths['wiki_features2M.pt'])\n",
    "        edge_index = torch.load(paths['wiki_edges2M.pt']).t().contiguous()\n",
    "        y = torch.load(paths['wiki_views2M.pt'])\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    def _process_facebook(self):\n",
    "        from scipy.io import loadmat\n",
    "\n",
    "        mat = loadmat(self.raw_paths[0])\n",
    "\n",
    "        A = mat['A'].tocsr().tocoo()\n",
    "        row = torch.from_numpy(A.row).to(torch.long)\n",
    "        col = torch.from_numpy(A.col).to(torch.long)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        metadata = torch.from_numpy(mat['local_info'].astype('int64'))\n",
    "\n",
    "        xs = []\n",
    "        y = metadata[:, 1] - 1  # gender label, -1 means unlabeled\n",
    "        x = torch.cat([metadata[:, :1], metadata[:, 2:]], dim=-1)\n",
    "        for i in range(x.size(1)):\n",
    "            _, out = x[:, i].unique(return_inverse=True)\n",
    "            xs.append(one_hot(out))\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "        if self.name in self.splits:\n",
    "            splits = np.load(self.raw_paths[1], allow_pickle=True)\n",
    "            sizes = (data.num_nodes, len(splits))\n",
    "            data.train_mask = torch.zeros(sizes, dtype=torch.bool)\n",
    "            data.val_mask = torch.zeros(sizes, dtype=torch.bool)\n",
    "            data.test_mask = torch.zeros(sizes, dtype=torch.bool)\n",
    "\n",
    "            for i, split in enumerate(splits):\n",
    "                data.train_mask[:, i][torch.tensor(split['train'])] = True\n",
    "                data.val_mask[:, i][torch.tensor(split['valid'])] = True\n",
    "                data.test_mask[:, i][torch.tensor(split['test'])] = True\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _process_genius(self):\n",
    "        from scipy.io import loadmat\n",
    "\n",
    "        mat = loadmat(self.raw_paths[0])\n",
    "        edge_index = torch.from_numpy(mat['edge_index']).to(torch.long)\n",
    "        x = torch.from_numpy(mat['node_feat']).to(torch.float)\n",
    "        y = torch.from_numpy(mat['label']).squeeze().to(torch.long)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    def process(self):\n",
    "        if self.name in self.facebook_datasets:\n",
    "            data = self._process_facebook()\n",
    "        elif self.name == 'genius':\n",
    "            data = self._process_genius()\n",
    "        elif self.name == 'wiki':\n",
    "            data = self._process_wiki()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"chosen dataset '{self.name}' is not implemented\")\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name.capitalize()}({len(self)})'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8dc746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "#, download_url\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "\n",
    "class HeterophilousGraphDataset(InMemoryDataset):\n",
    "    r\"\"\"The heterophilous graphs :obj:`\"Roman-empire\"`,\n",
    "    :obj:`\"Amazon-ratings\"`, :obj:`\"Minesweeper\"`, :obj:`\"Tolokers\"` and\n",
    "    :obj:`\"Questions\"` from the `\"A Critical Look at the Evaluation of GNNs\n",
    "    under Heterophily: Are We Really Making Progress?\"\n",
    "    <https://arxiv.org/abs/2302.11640>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"Roman-empire\"`,\n",
    "            :obj:`\"Amazon-ratings\"`, :obj:`\"Minesweeper\"`, :obj:`\"Tolokers\"`,\n",
    "            :obj:`\"Questions\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - Name\n",
    "          - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - Roman-empire\n",
    "          - 22,662\n",
    "          - 32,927\n",
    "          - 300\n",
    "          - 18\n",
    "        * - Amazon-ratings\n",
    "          - 24,492\n",
    "          - 93,050\n",
    "          - 300\n",
    "          - 5\n",
    "        * - Minesweeper\n",
    "          - 10,000\n",
    "          - 39,402\n",
    "          - 7\n",
    "          - 2\n",
    "        * - Tolokers\n",
    "          - 11,758\n",
    "          - 519,000\n",
    "          - 10\n",
    "          - 2\n",
    "        * - Questions\n",
    "          - 48,921\n",
    "          - 153,540\n",
    "          - 301\n",
    "          - 2\n",
    "    \"\"\"\n",
    "    url = ('https://github.com/yandex-research/heterophilous-graphs/raw/'\n",
    "           'main/data')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.name = name.lower().replace('-', '_')\n",
    "        assert self.name in [\n",
    "            'roman_empire',\n",
    "            'amazon_ratings',\n",
    "            'minesweeper',\n",
    "            'tolokers',\n",
    "            'questions',\n",
    "        ]\n",
    "\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        download_url(f'{self.url}/{self.name}.npz', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        raw = np.load(self.raw_paths[0], 'r')\n",
    "        x = torch.from_numpy(raw['node_features'])\n",
    "        y = torch.from_numpy(raw['node_labels'])\n",
    "        edge_index = torch.from_numpy(raw['edges']).t().contiguous()\n",
    "        edge_index = to_undirected(edge_index, num_nodes=x.size(0))\n",
    "        train_mask = torch.from_numpy(raw['train_masks']).t().contiguous()\n",
    "        val_mask = torch.from_numpy(raw['val_masks']).t().contiguous()\n",
    "        test_mask = torch.from_numpy(raw['test_masks']).t().contiguous()\n",
    "\n",
    "        data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask,\n",
    "                    val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(name={self.name})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42724f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "\n",
    "class Actor(InMemoryDataset):\n",
    "    r\"\"\"The actor-only induced subgraph of the film-director-actor-writer\n",
    "    network used in the\n",
    "    `\"Geom-GCN: Geometric Graph Convolutional Networks\"\n",
    "    <https://openreview.net/forum?id=S1e2agrFvS>`_ paper.\n",
    "    Each node corresponds to an actor, and the edge between two nodes denotes\n",
    "    co-occurrence on the same Wikipedia page.\n",
    "    Node features correspond to some keywords in the Wikipedia pages.\n",
    "    The task is to classify the nodes into five categories in term of words of\n",
    "    actor's Wikipedia.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - 7,600\n",
    "          - 30,019\n",
    "          - 932\n",
    "          - 5\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master'\n",
    "\n",
    "    def __init__(self, root: str, transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['out1_node_feature_label.txt', 'out1_graph_edges.txt'\n",
    "                ] + [f'film_split_0.6_0.2_{i}.npz' for i in range(10)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for f in self.raw_file_names[:2]:\n",
    "            download_url(f'{self.url}/new_data/film/{f}', self.raw_dir)\n",
    "        for f in self.raw_file_names[2:]:\n",
    "            download_url(f'{self.url}/splits/{f}', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.raw_paths[0], 'r') as f:\n",
    "            data = [x.split('\\t') for x in f.read().split('\\n')[1:-1]]\n",
    "\n",
    "            rows, cols = [], []\n",
    "            for n_id, col, _ in data:\n",
    "                col = [int(x) for x in col.split(',')]\n",
    "                rows += [int(n_id)] * len(col)\n",
    "                cols += col\n",
    "            row, col = torch.tensor(rows), torch.tensor(cols)\n",
    "\n",
    "            x = torch.zeros(int(row.max()) + 1, int(col.max()) + 1)\n",
    "            x[row, col] = 1.\n",
    "\n",
    "            y = torch.empty(len(data), dtype=torch.long)\n",
    "            for n_id, _, label in data:\n",
    "                y[int(n_id)] = int(label)\n",
    "\n",
    "        with open(self.raw_paths[1], 'r') as f:\n",
    "            data = f.read().split('\\n')[1:-1]\n",
    "            data = [[int(v) for v in r.split('\\t')] for r in data]\n",
    "            edge_index = torch.tensor(data, dtype=torch.long).t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        train_masks, val_masks, test_masks = [], [], []\n",
    "        for f in self.raw_paths[2:]:\n",
    "            tmp = np.load(f)\n",
    "            train_masks += [torch.from_numpy(tmp['train_mask']).to(torch.bool)]\n",
    "            val_masks += [torch.from_numpy(tmp['val_mask']).to(torch.bool)]\n",
    "            test_masks += [torch.from_numpy(tmp['test_mask']).to(torch.bool)]\n",
    "        train_mask = torch.stack(train_masks, dim=1)\n",
    "        val_mask = torch.stack(val_masks, dim=1)\n",
    "        test_mask = torch.stack(test_masks, dim=1)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                    val_mask=val_mask, test_mask=test_mask)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850982fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebKB(InMemoryDataset):\n",
    "    r\"\"\"The WebKB datasets used in the\n",
    "    `\"Geom-GCN: Geometric Graph Convolutional Networks\"\n",
    "    <https://openreview.net/forum?id=S1e2agrFvS>`_ paper.\n",
    "    Nodes represent web pages and edges represent hyperlinks between them.\n",
    "    Node features are the bag-of-words representation of web pages.\n",
    "    The task is to classify the nodes into one of the five categories, student,\n",
    "    project, course, staff, and faculty.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"Cornell\"`, :obj:`\"Texas\"`,\n",
    "            :obj:`\"Wisconsin\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - Name\n",
    "          - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - Cornell\n",
    "          - 183\n",
    "          - 298\n",
    "          - 1,703\n",
    "          - 5\n",
    "        * - Texas\n",
    "          - 183\n",
    "          - 325\n",
    "          - 1,703\n",
    "          - 5\n",
    "        * - Wisconsin\n",
    "          - 251\n",
    "          - 515\n",
    "          - 1,703\n",
    "          - 5\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/master'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.name = name.lower()\n",
    "        assert self.name in ['cornell', 'texas', 'wisconsin']\n",
    "\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        out = ['out1_node_feature_label.txt', 'out1_graph_edges.txt']\n",
    "        out += [f'{self.name}_split_0.6_0.2_{i}.npz' for i in range(10)]\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for f in self.raw_file_names[:2]:\n",
    "            download_url(f'{self.url}/new_data/{self.name}/{f}', self.raw_dir)\n",
    "        for f in self.raw_file_names[2:]:\n",
    "            download_url(f'{self.url}/splits/{f}', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.raw_paths[0], 'r') as f:\n",
    "            data = f.read().split('\\n')[1:-1]\n",
    "            x = [[float(v) for v in r.split('\\t')[1].split(',')] for r in data]\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "            y = [int(r.split('\\t')[2]) for r in data]\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        with open(self.raw_paths[1], 'r') as f:\n",
    "            data = f.read().split('\\n')[1:-1]\n",
    "            data = [[int(v) for v in r.split('\\t')] for r in data]\n",
    "            edge_index = torch.tensor(data, dtype=torch.long).t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        train_masks, val_masks, test_masks = [], [], []\n",
    "        for f in self.raw_paths[2:]:\n",
    "            tmp = np.load(f)\n",
    "            train_masks += [torch.from_numpy(tmp['train_mask']).to(torch.bool)]\n",
    "            val_masks += [torch.from_numpy(tmp['val_mask']).to(torch.bool)]\n",
    "            test_masks += [torch.from_numpy(tmp['test_mask']).to(torch.bool)]\n",
    "        train_mask = torch.stack(train_masks, dim=1)\n",
    "        val_mask = torch.stack(val_masks, dim=1)\n",
    "        test_mask = torch.stack(test_masks, dim=1)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                    val_mask=val_mask, test_mask=test_mask)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a25ef160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaNetwork(InMemoryDataset):\n",
    "    r\"\"\"The Wikipedia networks introduced in the\n",
    "    `\"Multi-scale Attributed Node Embedding\"\n",
    "    <https://arxiv.org/abs/1909.13021>`_ paper.\n",
    "    Nodes represent web pages and edges represent hyperlinks between them.\n",
    "    Node features represent several informative nouns in the Wikipedia pages.\n",
    "    The task is to predict the average daily traffic of the web page.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        name (str): The name of the dataset (:obj:`\"chameleon\"`,\n",
    "            :obj:`\"crocodile\"`, :obj:`\"squirrel\"`).\n",
    "        geom_gcn_preprocess (bool): If set to :obj:`True`, will load the\n",
    "            pre-processed data as introduced in the `\"Geom-GCN: Geometric\n",
    "            Graph Convolutional Networks\" <https://arxiv.org/abs/2002.05287>_`,\n",
    "            in which the average monthly traffic of the web page is converted\n",
    "            into five categories to predict.\n",
    "            If set to :obj:`True`, the dataset :obj:`\"crocodile\"` is not\n",
    "            available.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raw_url = 'https://graphmining.ai/datasets/ptg/wiki'\n",
    "    processed_url = ('https://raw.githubusercontent.com/graphdml-uiuc-jlu/'\n",
    "                     'geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f')\n",
    "\n",
    "    def __init__(self, root: str, name: str, geom_gcn_preprocess: bool = True,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        self.geom_gcn_preprocess = geom_gcn_preprocess\n",
    "        assert self.name in ['chameleon', 'crocodile', 'squirrel']\n",
    "        if geom_gcn_preprocess and self.name == 'crocodile':\n",
    "            raise AttributeError(\"The dataset 'crocodile' is not available in \"\n",
    "                                 \"case 'geom_gcn_preprocess=True'\")\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        if self.geom_gcn_preprocess:\n",
    "            return osp.join(self.root, self.name, 'geom_gcn', 'raw')\n",
    "        else:\n",
    "            return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        if self.geom_gcn_preprocess:\n",
    "            return osp.join(self.root, self.name, 'geom_gcn', 'processed')\n",
    "        else:\n",
    "            return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        if self.geom_gcn_preprocess:\n",
    "            return (['out1_node_feature_label.txt', 'out1_graph_edges.txt'] +\n",
    "                    [f'{self.name}_split_0.6_0.2_{i}.npz' for i in range(10)])\n",
    "        else:\n",
    "            return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        if self.geom_gcn_preprocess:\n",
    "            for filename in self.raw_file_names[:2]:\n",
    "                url = f'{self.processed_url}/new_data/{self.name}/{filename}'\n",
    "                download_url(url, self.raw_dir)\n",
    "            for filename in self.raw_file_names[2:]:\n",
    "                url = f'{self.processed_url}/splits/{filename}'\n",
    "                download_url(url, self.raw_dir)\n",
    "        else:\n",
    "            download_url(f'{self.raw_url}/{self.name}.npz', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        if self.geom_gcn_preprocess:\n",
    "            with open(self.raw_paths[0], 'r') as f:\n",
    "                data = f.read().split('\\n')[1:-1]\n",
    "            x = [[float(v) for v in r.split('\\t')[1].split(',')] for r in data]\n",
    "            x = torch.tensor(x, dtype=torch.float)\n",
    "            y = [int(r.split('\\t')[2]) for r in data]\n",
    "            y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "            with open(self.raw_paths[1], 'r') as f:\n",
    "                data = f.read().split('\\n')[1:-1]\n",
    "                data = [[int(v) for v in r.split('\\t')] for r in data]\n",
    "            edge_index = torch.tensor(data, dtype=torch.long).t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "            train_masks, val_masks, test_masks = [], [], []\n",
    "            for filepath in self.raw_paths[2:]:\n",
    "                f = np.load(filepath)\n",
    "                train_masks += [torch.from_numpy(f['train_mask'])]\n",
    "                val_masks += [torch.from_numpy(f['val_mask'])]\n",
    "                test_masks += [torch.from_numpy(f['test_mask'])]\n",
    "            train_mask = torch.stack(train_masks, dim=1).to(torch.bool)\n",
    "            val_mask = torch.stack(val_masks, dim=1).to(torch.bool)\n",
    "            test_mask = torch.stack(test_masks, dim=1).to(torch.bool)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask,\n",
    "                        val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "        else:\n",
    "            data = np.load(self.raw_paths[0], 'r', allow_pickle=True)\n",
    "            x = torch.from_numpy(data['features']).to(torch.float)\n",
    "            edge_index = torch.from_numpy(data['edges']).to(torch.long)\n",
    "            edge_index = edge_index.t().contiguous()\n",
    "            edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "            y = torch.from_numpy(data['target']).to(torch.float)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f8093e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/yandex-research/heterophilous-graphs/raw/main/data/minesweeper.npz\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 110] Connection timed out>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:1354\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1354\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m \n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:1418\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1418\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/http/client.py:922\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mIPPROTO_TCP, socket\u001b[38;5;241m.\u001b[39mTCP_NODELAY, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/socket.py:808\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/socket.py:796\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    795\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 796\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m: \n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#\"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#\"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHeterophilousGraphDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMinesweeper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#dataset = Actor(root=DIR+'Actor')        \u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#(\"Cornell\", \"Texas\", \"Wisconsin\").\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#\"chameleon\", \"crocodile\", \"squirrel\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#dataset = WikipediaNetwork(root=DIR, name = 'squirrel')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m])\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mHeterophilousGraphDataset.__init__\u001b[0;34m(self, root, name, transform, pre_transform)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroman_empire\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon_ratings\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     87\u001b[0m ]\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:56\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m              transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m              pre_transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m              pre_filter: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/torch_geometric/data/dataset.py:84\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices: Optional[Sequence] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/torch_geometric/data/dataset.py:145\u001b[0m, in \u001b[0;36mDataset._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    144\u001b[0m makedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mHeterophilousGraphDataset.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, folder, log, filename)\u001b[0m\n\u001b[1;32m     35\u001b[0m makedirs(folder)\n\u001b[1;32m     37\u001b[0m context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context()\n\u001b[0;32m---> 38\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# workaround for https://bugs.python.org/issue42853\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    522\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    524\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 525\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    528\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:542\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    541\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 542\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    543\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:1397\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/urllib/request.py:1357\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1355\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1358\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 110] Connection timed out>"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    \n",
    "    #\"penn94\", \"reed98\", \"amherst41\", \"cornell5\", \"johnshopkins55\", \"genius\"\n",
    "    \n",
    "#     dataset = LINKXDataset(root=DIR, name = \"reed98\")      \n",
    "    \n",
    "    #\"Roman-empire\", \"Amazon-ratings\", \"Minesweeper\", \"Tolokers\", \"Questions\"\n",
    "    \n",
    "    dataset = HeterophilousGraphDataset(root=DIR, name = \"Minesweeper\")        \n",
    "    \n",
    "    \n",
    "    #dataset = Actor(root=DIR+'Actor')        \n",
    "    \n",
    "    #(\"Cornell\", \"Texas\", \"Wisconsin\").\n",
    "#     dataset = WebKB(root=DIR, name = 'Wisconsin')\n",
    "    \n",
    "    #\"chameleon\", \"crocodile\", \"squirrel\"\n",
    "    #dataset = WikipediaNetwork(root=DIR, name = 'squirrel')\n",
    "    \n",
    "    print(dataset[0])\n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f304545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
