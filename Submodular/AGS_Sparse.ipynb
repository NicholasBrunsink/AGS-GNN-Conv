{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2dcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "ctypes.cdll.LoadLibrary(\"/apps/gilbreth/cuda-toolkit/cuda-11.2.0/lib64/libcusparse.so.11\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71fd74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b49419",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43fed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data, generate_synthetic\n",
    "from ipynb.fs.full.Utils import save_plot\n",
    "\n",
    "import ipynb.fs.full.utils.MoonGraph as MoonGraph\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "#https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from torch_geometric.data import NeighborSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from ipynb.fs.full.AGS_Node_Sampler import WeightedNeighborLoader\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b288b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14450a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "LOG_INFO = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3176206",
   "metadata": {},
   "source": [
    "## Command Line Arguments and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4108fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--pbar', type=bool, default=True)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Moon\", choices=['Reddit','Cora','CiteSeer','PubMed'])\n",
    "        \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b31254",
   "metadata": {},
   "source": [
    "# Sparsification modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6bb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.SubmodularWeights import SubModularWeightFacilityFaster\n",
    "from ipynb.fs.full.KNNWeights import KNNWeight\n",
    "from ipynb.fs.full.PretrainedLink import LinkPred, LinkNN, LinkSub\n",
    "from ipynb.fs.full.RandomSparse import RandomSparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72498e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import test_uniformity, total_entropy, agg_homophily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370f0b6",
   "metadata": {},
   "source": [
    "## GNNmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ipynb.fs.full.SpatialConv import SpatialConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacacb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = AGS_GCN(2, 2)\n",
    "# #print(test)\n",
    "# n=7\n",
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# test(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab977ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import layers\n",
    "# import scipy.sparse as sp\n",
    "# from ipynb.fs.full.ACM.models.Test import GCN, normalize_tensor, sparse_mx_to_torch_sparse_tensor\n",
    "# from torch_geometric.utils.convert import to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88929c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2  = GCN(nfeat=2,\n",
    "#         nhid=2,\n",
    "#         nclass=2,\n",
    "#         nlayers=2,\n",
    "#         nnodes=7,\n",
    "#         dropout=0.2,\n",
    "#         model_type='acmgcn',\n",
    "#         structure_info=0,\n",
    "#         variant=False,\n",
    "#         init_layers_X=1,)\n",
    "\n",
    "# test2 = test2.to(device)\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303503fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ACM.modelgeom.layers as layers\n",
    "from ACM.modelgeom.models import GCN\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils.convert import to_scipy_sparse_matrix\n",
    "from ipynb.fs.full.ACM.models.Test import normalize_tensor, sparse_mx_to_torch_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a599b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2  = GCN(nfeat=2,\n",
    "#         nhid=2,\n",
    "#         nclass=2,\n",
    "#         nlayers=2,\n",
    "#         nnodes=7,\n",
    "#         dropout=0.2,\n",
    "#         model_type='acmgcn',\n",
    "#         structure_info=0,\n",
    "#         variant=False,)\n",
    "\n",
    "# test2 = test2.to(device)\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54de3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_low_unnormalized = to_scipy_sparse_matrix(edge_index)\n",
    "# adj_low = normalize_tensor(sp.identity(n) + adj_low_unnormalized)\n",
    "# adj_high = sp.identity(n) - adj_low\n",
    "# adj_low = sparse_mx_to_torch_sparse_tensor(adj_low).to(device)\n",
    "# adj_high = sparse_mx_to_torch_sparse_tensor(adj_high).to(device)\n",
    "# adj_low_unnormalized = sparse_mx_to_torch_sparse_tensor(adj_low_unnormalized).to(device)\n",
    "\n",
    "# test2(x.to(device), adj_low, adj_high, adj_low_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eb4a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACMtest(model, data, mask, x, adj_low, adj_high, adj_low_unnormalized):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        out = model(x, adj_low, adj_high, adj_low_unnormalized)  \n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        pred =out[mask].argmax(dim=1)            \n",
    "        \n",
    "        correct = pred.eq(data.y[mask].to(device))\n",
    "        total_correct+=correct.sum()            \n",
    "\n",
    "        total_examples += sum(mask)\n",
    "\n",
    "    #print(\"Total tested: \", total_examples,end=', ')\n",
    "\n",
    "    return total_correct/total_examples\n",
    "\n",
    "    \n",
    "\n",
    "def ACMtrain(model, data, epochs=100, train_neighbors=[-1,10], test_neighbors=[-1,10]):\n",
    "    \n",
    "    if LOG_INFO:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    if data.y.ndim == 1:\n",
    "        #criterion = torch.nn.CrossEntropyLoss()\n",
    "        criterion = torch.nn.NLLLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    \n",
    "    row, col = data.edge_index\n",
    "    data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
    "    \n",
    "    train_losses=[]\n",
    "    val_accuracies=[]\n",
    "    train_accuracies=[]\n",
    "    test_accuracies=[]\n",
    "    \n",
    "    \n",
    "    data = data.to(device)\n",
    "    n = data.num_nodes\n",
    "    x = data.x.to(device)\n",
    "    adj_low_unnormalized = to_scipy_sparse_matrix(data.edge_index)\n",
    "    adj_low = normalize_tensor(sp.identity(n) + adj_low_unnormalized)\n",
    "    adj_high = sp.identity(n) - adj_low\n",
    "    adj_low = sparse_mx_to_torch_sparse_tensor(adj_low).to(device)\n",
    "    adj_high = sparse_mx_to_torch_sparse_tensor(adj_high).to(device)\n",
    "    adj_low_unnormalized = sparse_mx_to_torch_sparse_tensor(adj_low_unnormalized).to(device)\n",
    "    \n",
    "    if LOG_INFO:\n",
    "        print(x.device, adj_low.device, adj_high.device, adj_low_unnormalized.device)\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        out = model(x, adj_low, adj_high, adj_low_unnormalized)\n",
    "    \n",
    "        #loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * int(sum(data.train_mask))\n",
    "        total_examples += int(sum(data.train_mask))\n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():            \n",
    "            out = model(x, adj_low, adj_high, adj_low_unnormalized)\n",
    "            #out = F.log_softmax(out,dim=1)        \n",
    "            pred = out.argmax(dim=-1)\n",
    "            correct = pred.eq(data.y)\n",
    "\n",
    "        accs = []\n",
    "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "            accs.append(correct[mask].sum().item() / mask.sum().item()) \n",
    "            \n",
    "        #print(accs)\n",
    "        train_accuracies.append(accs[0])\n",
    "        val_accuracies.append(accs[1])\n",
    "        test_accuracies.append(accs[2])\n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if LOG_INFO:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {accs[0]:.4f}, Val: {accs[1]:.4f}, Test: {accs[2]:.4f}, Std dev: {std_dev:.4f}')\n",
    "    \n",
    "#         train_acc = ACMtest(model, data, data.train_mask, x, adj_low, adj_high, adj_low_unnormalized)\n",
    "#         train_accuracies.append(train_acc.item())\n",
    "\n",
    "#         val_acc = ACMtest(model, data, data.val_mask, x, adj_low, adj_high, adj_low_unnormalized)\n",
    "#         val_accuracies.append(val_acc.item())\n",
    "        \n",
    "#         test_acc = ACMtest(model, data, data.test_mask, x, adj_low, adj_high, adj_low_unnormalized)\n",
    "#         test_accuracies.append(test_acc.item())\n",
    "        \n",
    "#         std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "#         print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "    \n",
    "    if LOG_INFO:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Plots/Validation', yname='Accuracy', xname='Epoch')\n",
    "\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "    \n",
    "    return max(test_accuracies), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215390db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACMperformanceSampler(data, dataset, epochs=1, train_neighbors=[-1,10], test_neighbors=[-1,10]):        \n",
    "    \n",
    "#     model  = GCN(\n",
    "#         nfeat=dataset.num_features,\n",
    "#         nhid=64,\n",
    "#         nclass=dataset.num_classes,\n",
    "#         nlayers=2,\n",
    "#         nnodes=data.num_nodes,\n",
    "#         dropout=0.2,\n",
    "#         model_type='acmsnowball',\n",
    "#         structure_info=0,\n",
    "#         variant=False,\n",
    "#         init_layers_X=1,)\n",
    "\n",
    "    #acmgcnpp acmgcn acmgcnp acmsgc acmsnowball\n",
    "\n",
    "    model  = GCN(\n",
    "        nfeat=dataset.num_features,\n",
    "        nhid=64,\n",
    "        nclass=dataset.num_classes,\n",
    "        nlayers=2,\n",
    "        nnodes=data.num_nodes,\n",
    "        dropout=0.2,\n",
    "        model_type='acmgcnp',\n",
    "        structure_info=1,\n",
    "        variant=True,)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    if LOG_INFO:\n",
    "        print(model)\n",
    "        \n",
    "    test_acc, itr = ACMtrain(model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)\n",
    "    \n",
    "    return test_acc, itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4406b52e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at:  /scratch/gilbreth/das90/Dataset/\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "\n",
      "Dataset: WikipediaNetwork():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 2089\n",
      "Number of classes: 5\n",
      "\n",
      "Data(x=[5201, 2089], edge_index=[2, 217073], y=[5201], train_mask=[5201], val_mask=[5201], test_mask=[5201])\n",
      "===========================================================================================================\n",
      "Number of nodes: 5201\n",
      "Number of edges: 217073\n",
      "Average node degree: 41.74\n",
      "Number of training nodes: 2496\n",
      "Training node label rate: 0.48\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: False\n",
      "0.08891735225915909 0.2239430993795395 0.042996399104595184 0.3737970292568207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[5201, 2089], edge_index=[2, 217073], y=[5201], train_mask=[5201], val_mask=[5201], test_mask=[5201])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, dataset = get_data('Squirrel', DIR, log=True, h_score=True, split_no=2)\n",
    "# data = generate_synthetic(data, d=42, h=0.2, train=0.6, random_state=1, log=True)\n",
    "# data.x = F.one_hot(data.y).float()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484bf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #submodular_weight = LinkPred(data)\n",
    "# nn_weight = LinkSub(data, value='max', selfloop = False, log=True) #min favor similar ones, max disimilar    \n",
    "# #     data.weight = nn_weight.compute_weights()\n",
    "# #     data.X = None\n",
    "# #     nn_weight.# submodular_weight.lazy_greedy_weight(0)\n",
    "# nn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79fda28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify(data, log = True, method = 'NN', metric= None):\n",
    "    data.to('cpu')    \n",
    "    \n",
    "    if metric is None:\n",
    "        metric = 'cosine'\n",
    "    \n",
    "    if method == 'nn':\n",
    "        submodular_weight = KNNWeight(data, metric=metric, log=log)                \n",
    "        data.weight = submodular_weight.compute_weights()        \n",
    "\n",
    "    elif method == 'submodular':\n",
    "        submodular_weight = SubModularWeightFacilityFaster(data, metric=metric, log=log)\n",
    "        data.weight = submodular_weight.compute_weights()        \n",
    "    \n",
    "    elif method == 'link-nn':    \n",
    "        submodular_weight = LinkPred(data, selfloop = True, log=log)\n",
    "        data.weight = submodular_weight.compute_weights()        \n",
    "        nn_weight = LinkNN(data, value='min', log=log) #min favor similar ones, max disimilar\n",
    "        data.weight = nn_weight.compute_weights()\n",
    "    elif method == 'link-sub':    \n",
    "        nn_weight = LinkSub(data, value='max', selfloop = True, log=log) #min favor similar ones, max disimilar    \n",
    "        data.weight = nn_weight.compute_weights()\n",
    "    else:\n",
    "        raise 'Not implemented error'\n",
    "    \n",
    "    cp_data= copy.deepcopy(data)\n",
    "    G = to_networkx(cp_data, to_undirected=False, edge_attrs=['weight'])\n",
    "    to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] < 0.7 ]\n",
    "    G.remove_edges_from(to_remove)\n",
    "    updated_data = from_networkx(G)\n",
    "    \n",
    "    updated_data = from_networkx(G, group_edge_attrs=['weight'])\n",
    "    updated_data.weight = updated_data.edge_attr.view(-1)\n",
    "\n",
    "    row, col = updated_data.edge_index\n",
    "    updated_data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "    updated_data.weight = torch.cat((updated_data.weight, updated_data.weight),dim=0)\n",
    "\n",
    "    \n",
    "    if LOG_INFO:\n",
    "        print(updated_data)\n",
    "        print(\"Node Homophily:\", homophily(updated_data.edge_index, data.y, method='node'))\n",
    "        print(\"Edge Homophily:\", homophily(updated_data.edge_index, data.y, method='edge'))\n",
    "        print(\"Edge_insensitive Homophily:\", homophily(updated_data.edge_index, data.y, method='edge_insensitive'))    \n",
    "        print(\"Degree: \", updated_data.num_edges / updated_data.num_nodes)\n",
    "\n",
    "    data.edge_index = updated_data.edge_index\n",
    "    data.edge_weight = updated_data.weight\n",
    "    data.weight = None\n",
    "\n",
    "    return data\n",
    "\n",
    "# LOG_INFO = True\n",
    "# data = sparsify(data, log = False)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea5a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sparsify(data, K, log = False):    \n",
    "    rand_sparse = RandomSparse(data, K = K, log = log)\n",
    "    edge_index = rand_sparse.sparse()\n",
    "    row, col = edge_index\n",
    "    data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "        print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "        print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "        print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e7b9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_homophily(data, h = 0.1, d = 11, log = False):\n",
    "    data.to('cpu')\n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    edge_index=[]\n",
    "    \n",
    "#     h = 0.1\n",
    "#     d = 11\n",
    "\n",
    "    match = int(round(d*h))\n",
    "    unmatch = int(round(d*(1-h)))\n",
    "    #print(match,unmatch)\n",
    "    \n",
    "    for u in range(N):                \n",
    "        row, col, e_index = adj[u,:].coo()   \n",
    "        \n",
    "        cur_y = data.y[u]\n",
    "        neighbors = data.y[col]\n",
    "        #print(cur_y, neighbors)\n",
    "        \n",
    "        match_indexs = torch.nonzero(neighbors == cur_y).squeeze()\n",
    "        other_indexs = torch.nonzero(neighbors != cur_y).squeeze()\n",
    "        \n",
    "        #print(match_indexs, other_indexs)\n",
    "        \n",
    "        if match_indexs.dim()>0:\n",
    "            m_sel = match_indexs[np.random.choice(len(match_indexs), size=min(match,len(match_indexs)), replace = False)]\n",
    "        else:\n",
    "            m_sel = torch.LongTensor([])\n",
    "        if other_indexs.dim()>0:\n",
    "            um_sel = other_indexs[np.random.choice(len(other_indexs), size=min(unmatch, len(other_indexs)), replace = False)]\n",
    "        else:\n",
    "            um_sel = torch.LongTensor([])\n",
    "            \n",
    "        \n",
    "        #print(m_sel, um_sel)\n",
    "        \n",
    "        indexs = torch.cat((m_sel,um_sel),dim=0)\n",
    "    \n",
    "        e_index = e_index[indexs]            \n",
    "        edge_index.extend(e_index)\n",
    "        \n",
    "        #break        \n",
    "            \n",
    "    edge_index = data.edge_index[:,edge_index]\n",
    "    row, col = edge_index\n",
    "    data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "        print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "        print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "        print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# data = modify_homophily(data, h=0.15, d=11, log = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c371ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_feature(data):\n",
    "    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    edges = data.edge_index.t()\n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    data.x = adj_mat\n",
    "    \n",
    "    return data\n",
    "\n",
    "# adj_feature(data)\n",
    "# data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afc37b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count, score = test_uniformity(data, dataset.num_classes, log=False)\n",
    "# print(count, score, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53d8d7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_hetero():\n",
    "    d = 42\n",
    "    for h in np.array(range(0,21))/20:\n",
    "        DATASET_NAME = 'squirrel'\n",
    "        data, dataset = get_data(DATASET_NAME, log=False)\n",
    "        data = generate_synthetic(data, d=d, h = h, train=0.6, random_state=1, log=False, balance = False)\n",
    "        num_classes = dataset.num_classes\n",
    "        \n",
    "        print('d ', d, ' h', h, end=' ')\n",
    "        count, score = test_uniformity(data, num_classes, log=False)\n",
    "        print(count, score, end = ' ')\n",
    "        total_en, en_score = total_entropy(data, num_classes, log=False)\n",
    "        print(total_en, en_score, end = ' ')\n",
    "        \n",
    "        print('sparse', end = ' ')\n",
    "        data = sparsify(data, log=False)\n",
    "        \n",
    "        count, score = test_uniformity(data, num_classes, log=False)\n",
    "        print(count, score, end = ' ')\n",
    "        total_en, en_score = total_entropy(data, num_classes, log=False)\n",
    "        print(total_en, en_score, end = ' ')\n",
    "        \n",
    "        print(\"Nh \", homophily(data.edge_index, data.y, method='node'), end = ' ')\n",
    "        print(\"Eh \", homophily(data.edge_index, data.y, method='edge'), end = ' ')\n",
    "        print(\"EiH \", homophily(data.edge_index, data.y, method='edge_insensitive'), end = ' ')    \n",
    "        \n",
    "        print(\"Ha \", agg_homophily(data, 'affinity'), end = ' ')\n",
    "        print(\"Hl \", agg_homophily(data, 'laplacian'), end =' ')\n",
    "        \n",
    "        print(\"D \", data.num_edges / data.num_nodes, end = '\\n')\n",
    "\n",
    "\n",
    "# LOG_INFO = False\n",
    "# test_hetero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8caf0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG_INFO = True\n",
    "# data = random_sparsify(data, K = 11, log = True)\n",
    "# data = sparsify(data, method = 'link-nn', metric='cosine', log = True)\n",
    "# data = modify_homophily(data, h = 0.05, d = 11, log = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d39b53d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gcns): ModuleList(\n",
      "    (0): GraphConvolution (2089 -> 64)\n",
      "    (1): GraphConvolution (64 -> 5)\n",
      "  )\n",
      "  (mlps): ModuleList()\n",
      ")\n",
      "Train neighbors:  [-1, -1]\n",
      "Test neighbors:  [-1, -1]\n",
      "cuda:0 cuda:0 cuda:0 cuda:0\n",
      "Epoch: 001, Train Loss: 1.6477, Train: 0.2800, Val: 0.2686, Test: 0.2536, Std dev: 0.0000\n",
      "Epoch: 002, Train Loss: 1.5747, Train: 0.3377, Val: 0.3149, Test: 0.2968, Std dev: 0.0365\n",
      "Epoch: 003, Train Loss: 1.5428, Train: 0.4303, Val: 0.3936, Test: 0.3525, Std dev: 0.0439\n",
      "Epoch: 004, Train Loss: 1.5050, Train: 0.4659, Val: 0.4044, Test: 0.3785, Std dev: 0.0524\n",
      "Epoch: 005, Train Loss: 1.4653, Train: 0.5004, Val: 0.4219, Test: 0.4092, Std dev: 0.0622\n",
      "Epoch: 006, Train Loss: 1.4287, Train: 0.5184, Val: 0.4303, Test: 0.4102, Std dev: 0.0523\n",
      "Epoch: 007, Train Loss: 1.3912, Train: 0.5593, Val: 0.4465, Test: 0.4188, Std dev: 0.0537\n",
      "Epoch: 008, Train Loss: 1.3514, Train: 0.5921, Val: 0.4814, Test: 0.4304, Std dev: 0.0539\n",
      "Epoch: 009, Train Loss: 1.3136, Train: 0.6262, Val: 0.5042, Test: 0.4736, Std dev: 0.0539\n",
      "Epoch: 010, Train Loss: 1.2703, Train: 0.6546, Val: 0.5102, Test: 0.4890, Std dev: 0.0558\n",
      "Epoch: 011, Train Loss: 1.2305, Train: 0.6839, Val: 0.5270, Test: 0.5062, Std dev: 0.0569\n",
      "Epoch: 012, Train Loss: 1.1800, Train: 0.7119, Val: 0.5487, Test: 0.5034, Std dev: 0.0603\n",
      "Epoch: 013, Train Loss: 1.1358, Train: 0.7292, Val: 0.5523, Test: 0.5226, Std dev: 0.0631\n",
      "Epoch: 014, Train Loss: 1.0939, Train: 0.7504, Val: 0.5559, Test: 0.5226, Std dev: 0.0633\n",
      "Epoch: 015, Train Loss: 1.0515, Train: 0.7700, Val: 0.5607, Test: 0.5312, Std dev: 0.0628\n",
      "Epoch: 016, Train Loss: 1.0104, Train: 0.7945, Val: 0.5697, Test: 0.5379, Std dev: 0.0599\n",
      "Epoch: 017, Train Loss: 0.9622, Train: 0.8121, Val: 0.5757, Test: 0.5524, Std dev: 0.0609\n",
      "Epoch: 018, Train Loss: 0.9155, Train: 0.8261, Val: 0.5853, Test: 0.5610, Std dev: 0.0631\n",
      "Epoch: 019, Train Loss: 0.8789, Train: 0.8417, Val: 0.5956, Test: 0.5639, Std dev: 0.0623\n",
      "Epoch: 020, Train Loss: 0.8376, Train: 0.8582, Val: 0.6010, Test: 0.5696, Std dev: 0.0607\n",
      "Epoch: 021, Train Loss: 0.8045, Train: 0.8666, Val: 0.6088, Test: 0.5821, Std dev: 0.0557\n",
      "Epoch: 022, Train Loss: 0.7617, Train: 0.8730, Val: 0.6166, Test: 0.5735, Std dev: 0.0541\n",
      "Epoch: 023, Train Loss: 0.7307, Train: 0.8814, Val: 0.6178, Test: 0.5773, Std dev: 0.0527\n",
      "Epoch: 024, Train Loss: 0.7063, Train: 0.8966, Val: 0.6166, Test: 0.5764, Std dev: 0.0478\n",
      "Epoch: 025, Train Loss: 0.6635, Train: 0.9079, Val: 0.6196, Test: 0.5812, Std dev: 0.0479\n",
      "Epoch: 026, Train Loss: 0.6376, Train: 0.9123, Val: 0.6250, Test: 0.5850, Std dev: 0.0447\n",
      "Epoch: 027, Train Loss: 0.6218, Train: 0.9199, Val: 0.6226, Test: 0.5908, Std dev: 0.0410\n",
      "Epoch: 028, Train Loss: 0.5970, Train: 0.9263, Val: 0.6190, Test: 0.5898, Std dev: 0.0374\n",
      "Epoch: 029, Train Loss: 0.5804, Train: 0.9379, Val: 0.6172, Test: 0.5879, Std dev: 0.0293\n",
      "Epoch: 030, Train Loss: 0.5533, Train: 0.9399, Val: 0.6178, Test: 0.5812, Std dev: 0.0298\n",
      "Epoch: 031, Train Loss: 0.5346, Train: 0.9419, Val: 0.6172, Test: 0.5812, Std dev: 0.0309\n",
      "Epoch: 032, Train Loss: 0.5216, Train: 0.9463, Val: 0.6160, Test: 0.5879, Std dev: 0.0280\n",
      "Epoch: 033, Train Loss: 0.5084, Train: 0.9531, Val: 0.6178, Test: 0.5908, Std dev: 0.0252\n",
      "Epoch: 034, Train Loss: 0.4923, Train: 0.9607, Val: 0.6226, Test: 0.5937, Std dev: 0.0210\n",
      "Epoch: 035, Train Loss: 0.4737, Train: 0.9635, Val: 0.6244, Test: 0.5908, Std dev: 0.0215\n",
      "Epoch: 036, Train Loss: 0.4654, Train: 0.9655, Val: 0.6220, Test: 0.5927, Std dev: 0.0209\n",
      "Epoch: 037, Train Loss: 0.4479, Train: 0.9663, Val: 0.6238, Test: 0.5908, Std dev: 0.0210\n",
      "Epoch: 038, Train Loss: 0.4371, Train: 0.9700, Val: 0.6292, Test: 0.5956, Std dev: 0.0194\n",
      "Epoch: 039, Train Loss: 0.4276, Train: 0.9712, Val: 0.6340, Test: 0.6013, Std dev: 0.0171\n",
      "Epoch: 040, Train Loss: 0.4188, Train: 0.9716, Val: 0.6316, Test: 0.5994, Std dev: 0.0162\n",
      "Epoch: 041, Train Loss: 0.4021, Train: 0.9728, Val: 0.6268, Test: 0.5975, Std dev: 0.0156\n",
      "Epoch: 042, Train Loss: 0.3866, Train: 0.9768, Val: 0.6304, Test: 0.5946, Std dev: 0.0180\n",
      "Epoch: 043, Train Loss: 0.3872, Train: 0.9776, Val: 0.6340, Test: 0.5946, Std dev: 0.0165\n",
      "Epoch: 044, Train Loss: 0.3777, Train: 0.9784, Val: 0.6388, Test: 0.6061, Std dev: 0.0145\n",
      "Epoch: 045, Train Loss: 0.3713, Train: 0.9812, Val: 0.6418, Test: 0.6100, Std dev: 0.0104\n",
      "Epoch: 046, Train Loss: 0.3606, Train: 0.9832, Val: 0.6478, Test: 0.6090, Std dev: 0.0100\n",
      "Epoch: 047, Train Loss: 0.3476, Train: 0.9844, Val: 0.6430, Test: 0.6061, Std dev: 0.0137\n",
      "Epoch: 048, Train Loss: 0.3412, Train: 0.9836, Val: 0.6466, Test: 0.6177, Std dev: 0.0138\n",
      "Epoch: 049, Train Loss: 0.3412, Train: 0.9852, Val: 0.6502, Test: 0.6138, Std dev: 0.0118\n",
      "Epoch: 050, Train Loss: 0.3278, Train: 0.9852, Val: 0.6550, Test: 0.6186, Std dev: 0.0106\n",
      "Epoch: 051, Train Loss: 0.3122, Train: 0.9860, Val: 0.6490, Test: 0.6158, Std dev: 0.0127\n",
      "Epoch: 052, Train Loss: 0.3106, Train: 0.9880, Val: 0.6502, Test: 0.6196, Std dev: 0.0133\n",
      "Epoch: 053, Train Loss: 0.3031, Train: 0.9916, Val: 0.6556, Test: 0.6234, Std dev: 0.0137\n",
      "Epoch: 054, Train Loss: 0.2985, Train: 0.9912, Val: 0.6514, Test: 0.6263, Std dev: 0.0100\n",
      "Epoch: 055, Train Loss: 0.2868, Train: 0.9896, Val: 0.6538, Test: 0.6292, Std dev: 0.0092\n",
      "Epoch: 056, Train Loss: 0.2752, Train: 0.9916, Val: 0.6581, Test: 0.6359, Std dev: 0.0125\n",
      "Epoch: 057, Train Loss: 0.2787, Train: 0.9920, Val: 0.6611, Test: 0.6359, Std dev: 0.0108\n",
      "Epoch: 058, Train Loss: 0.2642, Train: 0.9920, Val: 0.6581, Test: 0.6378, Std dev: 0.0115\n",
      "Epoch: 059, Train Loss: 0.2624, Train: 0.9916, Val: 0.6617, Test: 0.6446, Std dev: 0.0091\n",
      "Epoch: 060, Train Loss: 0.2526, Train: 0.9948, Val: 0.6587, Test: 0.6417, Std dev: 0.0094\n",
      "Epoch: 061, Train Loss: 0.2468, Train: 0.9928, Val: 0.6532, Test: 0.6436, Std dev: 0.0109\n",
      "Epoch: 062, Train Loss: 0.2469, Train: 0.9944, Val: 0.6605, Test: 0.6503, Std dev: 0.0074\n",
      "Epoch: 063, Train Loss: 0.2406, Train: 0.9928, Val: 0.6623, Test: 0.6494, Std dev: 0.0073\n",
      "Epoch: 064, Train Loss: 0.2378, Train: 0.9944, Val: 0.6617, Test: 0.6465, Std dev: 0.0052\n",
      "Epoch: 065, Train Loss: 0.2299, Train: 0.9952, Val: 0.6653, Test: 0.6523, Std dev: 0.0063\n",
      "Epoch: 066, Train Loss: 0.2271, Train: 0.9952, Val: 0.6749, Test: 0.6513, Std dev: 0.0072\n",
      "Epoch: 067, Train Loss: 0.2210, Train: 0.9948, Val: 0.6725, Test: 0.6427, Std dev: 0.0071\n",
      "Epoch: 068, Train Loss: 0.2187, Train: 0.9968, Val: 0.6725, Test: 0.6398, Std dev: 0.0068\n",
      "Epoch: 069, Train Loss: 0.2136, Train: 0.9976, Val: 0.6701, Test: 0.6465, Std dev: 0.0058\n",
      "Epoch: 070, Train Loss: 0.2047, Train: 0.9964, Val: 0.6779, Test: 0.6484, Std dev: 0.0075\n",
      "Epoch: 071, Train Loss: 0.2026, Train: 0.9960, Val: 0.6797, Test: 0.6494, Std dev: 0.0073\n",
      "Epoch: 072, Train Loss: 0.2017, Train: 0.9968, Val: 0.6803, Test: 0.6551, Std dev: 0.0067\n",
      "Epoch: 073, Train Loss: 0.1951, Train: 0.9964, Val: 0.6857, Test: 0.6628, Std dev: 0.0060\n",
      "Epoch: 074, Train Loss: 0.1871, Train: 0.9960, Val: 0.6875, Test: 0.6590, Std dev: 0.0064\n",
      "Epoch: 075, Train Loss: 0.1820, Train: 0.9972, Val: 0.6851, Test: 0.6599, Std dev: 0.0081\n",
      "Epoch: 076, Train Loss: 0.1790, Train: 0.9976, Val: 0.6869, Test: 0.6599, Std dev: 0.0084\n",
      "Epoch: 077, Train Loss: 0.1752, Train: 0.9976, Val: 0.6911, Test: 0.6705, Std dev: 0.0069\n",
      "Epoch: 078, Train Loss: 0.1767, Train: 0.9972, Val: 0.6869, Test: 0.6609, Std dev: 0.0042\n",
      "Epoch: 079, Train Loss: 0.1681, Train: 0.9972, Val: 0.6881, Test: 0.6571, Std dev: 0.0047\n",
      "Epoch: 080, Train Loss: 0.1686, Train: 0.9976, Val: 0.6875, Test: 0.6647, Std dev: 0.0044\n",
      "Epoch: 081, Train Loss: 0.1681, Train: 0.9968, Val: 0.6929, Test: 0.6676, Std dev: 0.0038\n",
      "Epoch: 082, Train Loss: 0.1626, Train: 0.9964, Val: 0.6821, Test: 0.6551, Std dev: 0.0045\n",
      "Epoch: 083, Train Loss: 0.1671, Train: 0.9988, Val: 0.6905, Test: 0.6590, Std dev: 0.0022\n",
      "Epoch: 084, Train Loss: 0.1626, Train: 0.9988, Val: 0.6941, Test: 0.6686, Std dev: 0.0027\n",
      "Epoch: 085, Train Loss: 0.1612, Train: 0.9960, Val: 0.6893, Test: 0.6542, Std dev: 0.0028\n",
      "Epoch: 086, Train Loss: 0.1618, Train: 0.9964, Val: 0.6935, Test: 0.6619, Std dev: 0.0021\n",
      "Epoch: 087, Train Loss: 0.1637, Train: 0.9988, Val: 0.6923, Test: 0.6676, Std dev: 0.0021\n",
      "Epoch: 088, Train Loss: 0.1478, Train: 0.9980, Val: 0.6887, Test: 0.6763, Std dev: 0.0059\n",
      "Epoch: 089, Train Loss: 0.1525, Train: 0.9980, Val: 0.6899, Test: 0.6676, Std dev: 0.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 090, Train Loss: 0.1481, Train: 0.9972, Val: 0.6875, Test: 0.6647, Std dev: 0.0068\n",
      "Epoch: 091, Train Loss: 0.1497, Train: 0.9984, Val: 0.6857, Test: 0.6667, Std dev: 0.0059\n",
      "Epoch: 092, Train Loss: 0.1365, Train: 0.9972, Val: 0.6863, Test: 0.6657, Std dev: 0.0054\n",
      "Epoch: 093, Train Loss: 0.1490, Train: 0.9988, Val: 0.6899, Test: 0.6667, Std dev: 0.0055\n",
      "Epoch: 094, Train Loss: 0.1409, Train: 0.9984, Val: 0.6851, Test: 0.6695, Std dev: 0.0052\n",
      "Epoch: 095, Train Loss: 0.1351, Train: 0.9988, Val: 0.6947, Test: 0.6792, Std dev: 0.0061\n",
      "Epoch: 096, Train Loss: 0.1361, Train: 0.9992, Val: 0.6899, Test: 0.6782, Std dev: 0.0051\n",
      "Epoch: 097, Train Loss: 0.1301, Train: 0.9988, Val: 0.6923, Test: 0.6830, Std dev: 0.0064\n",
      "Epoch: 098, Train Loss: 0.1343, Train: 0.9992, Val: 0.6947, Test: 0.6772, Std dev: 0.0035\n",
      "Epoch: 099, Train Loss: 0.1260, Train: 0.9992, Val: 0.6953, Test: 0.6782, Std dev: 0.0038\n",
      "Epoch: 100, Train Loss: 0.1321, Train: 0.9988, Val: 0.7007, Test: 0.6830, Std dev: 0.0035\n",
      "Epoch: 101, Train Loss: 0.1185, Train: 0.9984, Val: 0.6983, Test: 0.6830, Std dev: 0.0056\n",
      "Epoch: 102, Train Loss: 0.1272, Train: 0.9988, Val: 0.7037, Test: 0.6849, Std dev: 0.0055\n",
      "Epoch: 103, Train Loss: 0.1218, Train: 0.9988, Val: 0.7019, Test: 0.6878, Std dev: 0.0047\n",
      "Epoch: 104, Train Loss: 0.1195, Train: 0.9992, Val: 0.7031, Test: 0.6878, Std dev: 0.0051\n",
      "Epoch: 105, Train Loss: 0.1235, Train: 0.9992, Val: 0.7049, Test: 0.6878, Std dev: 0.0031\n",
      "Epoch: 106, Train Loss: 0.1174, Train: 0.9992, Val: 0.7061, Test: 0.6916, Std dev: 0.0034\n",
      "Epoch: 107, Train Loss: 0.1207, Train: 0.9988, Val: 0.7031, Test: 0.6955, Std dev: 0.0021\n",
      "Epoch: 108, Train Loss: 0.1202, Train: 0.9988, Val: 0.7019, Test: 0.6868, Std dev: 0.0020\n",
      "Epoch: 109, Train Loss: 0.1190, Train: 0.9988, Val: 0.7055, Test: 0.6897, Std dev: 0.0020\n",
      "Epoch: 110, Train Loss: 0.1153, Train: 0.9992, Val: 0.7079, Test: 0.6897, Std dev: 0.0020\n",
      "Epoch: 111, Train Loss: 0.1135, Train: 0.9992, Val: 0.7079, Test: 0.6897, Std dev: 0.0029\n",
      "Epoch: 112, Train Loss: 0.1139, Train: 0.9988, Val: 0.7037, Test: 0.6801, Std dev: 0.0027\n",
      "Epoch: 113, Train Loss: 0.1120, Train: 0.9988, Val: 0.7049, Test: 0.6820, Std dev: 0.0024\n",
      "Epoch: 114, Train Loss: 0.1113, Train: 0.9988, Val: 0.7067, Test: 0.6878, Std dev: 0.0014\n",
      "Epoch: 115, Train Loss: 0.1115, Train: 0.9988, Val: 0.7103, Test: 0.6868, Std dev: 0.0011\n",
      "Epoch: 116, Train Loss: 0.1113, Train: 0.9988, Val: 0.7103, Test: 0.6993, Std dev: 0.0010\n",
      "Epoch: 117, Train Loss: 0.1071, Train: 0.9988, Val: 0.7133, Test: 0.7032, Std dev: 0.0018\n",
      "Epoch: 118, Train Loss: 0.1118, Train: 0.9988, Val: 0.7139, Test: 0.7022, Std dev: 0.0018\n",
      "Epoch: 119, Train Loss: 0.1096, Train: 0.9988, Val: 0.7181, Test: 0.6936, Std dev: 0.0017\n",
      "Epoch: 120, Train Loss: 0.1064, Train: 0.9988, Val: 0.7145, Test: 0.6888, Std dev: 0.0022\n",
      "Epoch: 121, Train Loss: 0.1065, Train: 0.9992, Val: 0.7121, Test: 0.6993, Std dev: 0.0021\n",
      "Epoch: 122, Train Loss: 0.1091, Train: 0.9992, Val: 0.7121, Test: 0.6897, Std dev: 0.0020\n",
      "Epoch: 123, Train Loss: 0.1049, Train: 0.9992, Val: 0.7163, Test: 0.6878, Std dev: 0.0018\n",
      "Epoch: 124, Train Loss: 0.1009, Train: 0.9992, Val: 0.7163, Test: 0.6897, Std dev: 0.0027\n",
      "Epoch: 125, Train Loss: 0.1050, Train: 0.9992, Val: 0.7163, Test: 0.6907, Std dev: 0.0027\n",
      "Epoch: 126, Train Loss: 0.1024, Train: 0.9992, Val: 0.7127, Test: 0.6801, Std dev: 0.0028\n",
      "Epoch: 127, Train Loss: 0.1021, Train: 0.9996, Val: 0.7157, Test: 0.6859, Std dev: 0.0016\n",
      "Epoch: 128, Train Loss: 0.0969, Train: 0.9996, Val: 0.7151, Test: 0.6868, Std dev: 0.0026\n",
      "Epoch: 129, Train Loss: 0.0996, Train: 0.9996, Val: 0.7151, Test: 0.6840, Std dev: 0.0028\n",
      "Epoch: 130, Train Loss: 0.0988, Train: 0.9996, Val: 0.7109, Test: 0.6830, Std dev: 0.0021\n",
      "Epoch: 131, Train Loss: 0.0977, Train: 0.9996, Val: 0.7151, Test: 0.6868, Std dev: 0.0018\n",
      "Epoch: 132, Train Loss: 0.0963, Train: 0.9996, Val: 0.7188, Test: 0.6907, Std dev: 0.0012\n",
      "Epoch: 133, Train Loss: 0.0939, Train: 0.9996, Val: 0.7188, Test: 0.6897, Std dev: 0.0020\n",
      "Epoch: 134, Train Loss: 0.0953, Train: 0.9996, Val: 0.7151, Test: 0.6964, Std dev: 0.0017\n",
      "Epoch: 135, Train Loss: 0.0932, Train: 0.9996, Val: 0.7194, Test: 0.7041, Std dev: 0.0016\n",
      "Epoch: 136, Train Loss: 0.0947, Train: 0.9996, Val: 0.7230, Test: 0.6993, Std dev: 0.0011\n",
      "Epoch: 137, Train Loss: 0.0913, Train: 0.9996, Val: 0.7236, Test: 0.6984, Std dev: 0.0014\n",
      "Epoch: 138, Train Loss: 0.0961, Train: 0.9992, Val: 0.7254, Test: 0.7012, Std dev: 0.0017\n",
      "Epoch: 139, Train Loss: 0.0992, Train: 0.9992, Val: 0.7230, Test: 0.6888, Std dev: 0.0027\n",
      "Epoch: 140, Train Loss: 0.1015, Train: 0.9968, Val: 0.7272, Test: 0.6993, Std dev: 0.0036\n",
      "Epoch: 141, Train Loss: 0.0993, Train: 0.9992, Val: 0.7212, Test: 0.6964, Std dev: 0.0035\n",
      "Epoch: 142, Train Loss: 0.0931, Train: 0.9992, Val: 0.7188, Test: 0.7003, Std dev: 0.0030\n",
      "Epoch: 143, Train Loss: 0.0918, Train: 0.9984, Val: 0.7218, Test: 0.7080, Std dev: 0.0038\n",
      "Epoch: 144, Train Loss: 0.0895, Train: 0.9992, Val: 0.7200, Test: 0.7051, Std dev: 0.0046\n",
      "Epoch: 145, Train Loss: 0.0945, Train: 0.9992, Val: 0.7200, Test: 0.6993, Std dev: 0.0033\n",
      "Epoch: 146, Train Loss: 0.0972, Train: 0.9992, Val: 0.7242, Test: 0.7022, Std dev: 0.0026\n",
      "Epoch: 147, Train Loss: 0.0945, Train: 0.9988, Val: 0.7236, Test: 0.6993, Std dev: 0.0026\n",
      "Epoch: 148, Train Loss: 0.0902, Train: 0.9992, Val: 0.7212, Test: 0.7003, Std dev: 0.0029\n",
      "Epoch: 149, Train Loss: 0.0912, Train: 0.9992, Val: 0.7236, Test: 0.7022, Std dev: 0.0025\n",
      "Epoch: 150, Train Loss: 0.0866, Train: 0.9992, Val: 0.7260, Test: 0.7032, Std dev: 0.0036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy,  0.7271634615384616\n",
      "Best Test Accuracy,  0.7079731027857828\n"
     ]
    }
   ],
   "source": [
    "LOG_INFO = True\n",
    "test_acc, itr = ACMperformanceSampler(data, dataset, epochs=150, train_neighbors=[-1,-1], test_neighbors=[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09955a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def single_experiments(DATASET_NAME, run = 1):\n",
    "\n",
    "    if True:   \n",
    "        d=42; h=0.00\n",
    "    \n",
    "#     for h in np.array(range(0,21))/20:    \n",
    "#     for d in range(110,151,10):            \n",
    "        print('d ', d, ' h', h, end=' ')\n",
    "        num_nodes = 0        \n",
    "        \n",
    "#         if h<0.51:continue\n",
    "        \n",
    "        be_iterations=[];be_accuracies=[];af_iterations=[];af_accuracies=[];before_edge=[];after_edge=[]\n",
    "        \n",
    "        Nh = [];Eh = [];EiH =[];D = []; uni_score = []; en_score =[]; Ha_score = []; Hl_score = [];\n",
    "        \n",
    "        for r in range(run):    \n",
    "            #data, dataset = get_data(DATASET_NAME, log=False, train=0.6, random_state=True)                \n",
    "            data, dataset = get_data(DATASET_NAME, log=False)\n",
    "#             data = generate_synthetic(data, d=d, h=h, train=0.6, random_state=1, log=False)\n",
    "            before_edge.append(data.num_edges)\n",
    "            num_nodes=data.num_nodes\n",
    "\n",
    "            adj_feature(data)\n",
    "#             random_sparsify(data, K=int(d*0.25), log = False)\n",
    "            data = sparsify(data, method = 'nn', metric='cosine', log = False)\n",
    "\n",
    "#             data = modify_homophily(data, h = max(0,h-0.1), d = 11, log = False)\n",
    "\n",
    "            Nh.append(homophily(data.edge_index, data.y, method='node'))\n",
    "            Eh.append(homophily(data.edge_index, data.y, method='edge'))\n",
    "            EiH.append(homophily(data.edge_index, data.y, method='edge_insensitive'))\n",
    "            D.append(data.num_edges / data.num_nodes)\n",
    "            \n",
    "            score, _ = test_uniformity(data, dataset.num_classes, log=False)\n",
    "            ent_score, _ = total_entropy(data, dataset.num_classes, log=False)\n",
    "            uni_score.append(score)\n",
    "            en_score.append(ent_score)\n",
    "            \n",
    "            Ha_score.append(agg_homophily(data, 'affinity'))\n",
    "            Hl_score.append(agg_homophily(data, 'laplacian'))\n",
    "        \n",
    "\n",
    "            accuracy, itr =  ACMperformanceSampler(data, dataset, epochs=150, train_neighbors=[-1,-1], test_neighbors=[-1,-1])\n",
    "            be_iterations.append(itr)\n",
    "            be_accuracies.append(accuracy)\n",
    "        \n",
    "        print('Nh ', np.mean(Nh), end=' ')\n",
    "        print('Eh ', np.mean(Eh), end=' ')\n",
    "        print('Eih ', np.mean(EiH), end=' ')\n",
    "        print('uni ',np.mean(uni_score), end=' ')\n",
    "        print('en ',np.mean(en_score), end=' ')\n",
    "        print('ha ',np.mean(Ha_score), end=' ')\n",
    "        print('hl ',np.mean(Hl_score), end=' ')\n",
    "        print('D ', np.mean(D), end=' ')        \n",
    "        print(f\"{num_nodes} {int(np.mean(before_edge))} {int(np.mean(be_iterations))} {np.mean(be_accuracies):0.4f}\")\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "def batch_experiments():\n",
    "    \n",
    "    datasets = [\n",
    "#         'cora', \n",
    "#         'Cora',\n",
    "#         'citeseer',\n",
    "#         'CiteSeer',\n",
    "#         'PubMed',\n",
    "#         'dblp',\n",
    "#         'Reddit','Reddit2',\n",
    "#         'Computers',\n",
    "#         'Photo',\n",
    "#         'CS','Physics',\n",
    "#         'Flickr',\n",
    "#         'film',\n",
    "        'cornell',\n",
    "        'texas',\n",
    "        'wisconsin',\n",
    "        'squirrel',\n",
    "        'chameleon',\n",
    "#         'Fake',\n",
    "#         'Moon',\n",
    "#         'AmazonProducts',\n",
    "#         'Yelp'\n",
    "    ]\n",
    "    \n",
    "    for DATASET_NAME in datasets:\n",
    "        print(DATASET_NAME, end='\\t')\n",
    "        before_edge, after_edge, iterations, accuracies = single_experiments(DATASET_NAME, run = 5)\n",
    "        \n",
    "        #print(before_edge, after_edge, iterations, accuracies)\n",
    "        \n",
    "    \n",
    "    return \n",
    "\n",
    "# start_time = time.time()\n",
    "# LOG_INFO = False\n",
    "# batch_experiments()\n",
    "# LOG_INFO = True\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(\"Time spent: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0820ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2fae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d61340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
