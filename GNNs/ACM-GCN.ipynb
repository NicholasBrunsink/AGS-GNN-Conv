{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f2dcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../Submodular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aca7628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cabd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bc954a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71fd74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61250713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b49419",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c54682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#from torch_geometric.data import NeighborSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from ipynb.fs.full.AGSNodeSampler import WeightedNeighborLoader\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b288b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370f0b6",
   "metadata": {},
   "source": [
    "## GNNmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c0852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ipynb.fs.full.SpatialConv import SpatialConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bacacb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = AGS_GCN(2, 2)\n",
    "# #print(test)\n",
    "# n=7\n",
    "# x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "# y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "# edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "# edge_index = edge_index-1\n",
    "# test(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eab977ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import layers\n",
    "# import scipy.sparse as sp\n",
    "# from ipynb.fs.full.ACM.models.Test import GCN, normalize_tensor, sparse_mx_to_torch_sparse_tensor\n",
    "# from torch_geometric.utils.convert import to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88929c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2  = GCN(nfeat=2,\n",
    "#         nhid=2,\n",
    "#         nclass=2,\n",
    "#         nlayers=2,\n",
    "#         nnodes=7,\n",
    "#         dropout=0.2,\n",
    "#         model_type='acmgcn',\n",
    "#         structure_info=0,\n",
    "#         variant=False,\n",
    "#         init_layers_X=1,)\n",
    "\n",
    "# test2 = test2.to(device)\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "303503fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ACM.modelgeom.layers as layers\n",
    "from ACM.modelgeom.models import GCN\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils.convert import to_scipy_sparse_matrix\n",
    "from ipynb.fs.full.ACM.models.Test import normalize_tensor, sparse_mx_to_torch_sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a599b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2  = GCN(nfeat=2,\n",
    "#         nhid=2,\n",
    "#         nclass=2,\n",
    "#         nlayers=2,\n",
    "#         nnodes=7,\n",
    "#         dropout=0.2,\n",
    "#         model_type='acmgcn',\n",
    "#         structure_info=0,\n",
    "#         variant=False,)\n",
    "\n",
    "# test2 = test2.to(device)\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54de3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_low_unnormalized = to_scipy_sparse_matrix(edge_index)\n",
    "# adj_low = normalize_tensor(sp.identity(n) + adj_low_unnormalized)\n",
    "# adj_high = sp.identity(n) - adj_low\n",
    "# adj_low = sparse_mx_to_torch_sparse_tensor(adj_low).to(device)\n",
    "# adj_high = sparse_mx_to_torch_sparse_tensor(adj_high).to(device)\n",
    "# adj_low_unnormalized = sparse_mx_to_torch_sparse_tensor(adj_low_unnormalized).to(device)\n",
    "\n",
    "# test2(x.to(device), adj_low, adj_high, adj_low_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3eb4a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACMtest(model, data, mask, x, adj_low, adj_high, adj_low_unnormalized):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        out = model(x, adj_low, adj_high, adj_low_unnormalized)  \n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        pred =out[mask].argmax(dim=1)            \n",
    "        \n",
    "        correct = pred.eq(data.y[mask].to(device))\n",
    "        total_correct+=correct.sum()            \n",
    "\n",
    "        total_examples += sum(mask)\n",
    "\n",
    "    #print(\"Total tested: \", total_examples,end=', ')\n",
    "\n",
    "    return total_correct/total_examples\n",
    "\n",
    "    \n",
    "\n",
    "def ACMtrain(model, data, epochs=100, train_neighbors=[-1,10], test_neighbors=[-1,10]):\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    if data.y.ndim == 1:\n",
    "        #criterion = torch.nn.CrossEntropyLoss()\n",
    "        criterion = torch.nn.NLLLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    \n",
    "    row, col = data.edge_index\n",
    "    data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
    "    \n",
    "    train_losses=[]\n",
    "    val_accuracies=[]\n",
    "    train_accuracies=[]\n",
    "    test_accuracies=[]\n",
    "    \n",
    "    \n",
    "    data = data.to(device)\n",
    "    n = data.num_nodes\n",
    "    x = data.x.to(device)\n",
    "    adj_low_unnormalized = to_scipy_sparse_matrix(data.edge_index)\n",
    "    adj_low = normalize_tensor(sp.identity(n) + adj_low_unnormalized)\n",
    "    adj_high = sp.identity(n) - adj_low\n",
    "    adj_low = sparse_mx_to_torch_sparse_tensor(adj_low).to(device)\n",
    "    adj_high = sparse_mx_to_torch_sparse_tensor(adj_high).to(device)\n",
    "    adj_low_unnormalized = sparse_mx_to_torch_sparse_tensor(adj_low_unnormalized).to(device)\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(x.device, adj_low.device, adj_high.device, adj_low_unnormalized.device)\n",
    "    \n",
    "    num_iteration = epochs\n",
    "    best_acc = 0\n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        out = model(x, adj_low, adj_high, adj_low_unnormalized)\n",
    "    \n",
    "        #loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "        out = F.log_softmax(out,dim=1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * int(sum(data.train_mask))\n",
    "        total_examples += int(sum(data.train_mask))\n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():            \n",
    "            out = model(x, adj_low, adj_high, adj_low_unnormalized)\n",
    "            #out = F.log_softmax(out,dim=1)        \n",
    "            pred = out.argmax(dim=-1)\n",
    "            correct = pred.eq(data.y)\n",
    "\n",
    "        accs = []\n",
    "        for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "            accs.append(correct[mask].sum().item() / mask.sum().item()) \n",
    "            \n",
    "        #print(accs)\n",
    "        train_accuracies.append(accs[0])\n",
    "        val_accuracies.append(accs[1])\n",
    "        test_accuracies.append(accs[2])\n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {accs[0]:.4f}, Val: {accs[1]:.4f}, Test: {accs[2]:.4f}, Std dev: {std_dev:.4f}')\n",
    "        \n",
    "        else:\n",
    "            if accs[2]>best_acc:\n",
    "                best_acc=accs[2]\n",
    "                \n",
    "#             if epoch>=5 and std_dev<=1e-3:\n",
    "#                 num_iteration = epoch            \n",
    "#                 if args.log_info:                \n",
    "#                     print(\"Iteration for convergence: \", epoch)\n",
    "#                 break\n",
    "                \n",
    "    \n",
    "#         train_acc = ACMtest(model, data, data.train_mask, x, adj_low, adj_high, adj_low_unnormalized)\n",
    "#         train_accuracies.append(train_acc.item())\n",
    "\n",
    "#         val_acc = ACMtest(model, data, data.val_mask, x, adj_low, adj_high, adj_low_unnormalized)\n",
    "#         val_accuracies.append(val_acc.item())\n",
    "        \n",
    "#         test_acc = ACMtest(model, data, data.test_mask, x, adj_low, adj_high, adj_low_unnormalized)\n",
    "#         test_accuracies.append(test_acc.item())\n",
    "        \n",
    "#         std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "#         print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "    \n",
    "    if args.log_info:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Results/ACMVal', yname='Accuracy', xname='Epoch')\n",
    "\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))    \n",
    "        return max(test_accuracies), 0\n",
    "    else:\n",
    "        return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "215390db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACMperformanceSampler(data, dataset, num_classes, epochs=1, train_neighbors=[-1,10], test_neighbors=[-1,10]):        \n",
    "    \n",
    "#     model  = GCN(\n",
    "#         nfeat=dataset.num_features,\n",
    "#         nhid=64,\n",
    "#         nclass=num_classes,\n",
    "#         nlayers=2,\n",
    "#         nnodes=data.num_nodes,\n",
    "#         dropout=0.2,\n",
    "#         model_type='acmsnowball',\n",
    "#         structure_info=0,\n",
    "#         variant=False,\n",
    "#         init_layers_X=1,)\n",
    "\n",
    "    #acmgcnpp acmgcn acmgcnp acmsgc acmsnowball\n",
    "\n",
    "    model  = GCN(\n",
    "        nfeat=dataset.num_features,\n",
    "        nhid=64,\n",
    "        nclass=num_classes,\n",
    "        nlayers=2,\n",
    "        nnodes=data.num_nodes,\n",
    "        dropout=0.2,\n",
    "        model_type='acmgcnp',\n",
    "        structure_info=1,\n",
    "        variant=True,)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(model)\n",
    "        \n",
    "    best_acc, num_iteration = ACMtrain(model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)\n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712bc10",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d39b53d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# args.log_info = False\n",
    "\n",
    "# data, dataset = get_data('karate', DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "# # # data = generate_synthetic(data, d=42, h=0.2, train=0.6, random_state=1, log=True)\n",
    "# # # data.x = F.one_hot(data.y).float()\n",
    "# # data\n",
    "# best_acc, num_iteration, _ = ACMperformanceSampler(data, dataset, dataset.num_classes, epochs=150, train_neighbors=[-1,-1], test_neighbors=[-1,-1])\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d971b8",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82d61340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        \"Cornell\",\n",
    "        \"Texas\",\n",
    "        \"Wisconsin\",\n",
    "        \"reed98\",\n",
    "        \"amherst41\",\n",
    "        \"penn94\",\n",
    "        \"Roman-empire\",\n",
    "        \"cornell5\",\n",
    "        \"Squirrel\",\n",
    "        \"johnshopkins55\",\n",
    "        \"AmazonProducts\",\n",
    "        \"Actor\",\n",
    "        \"Minesweeper\",\n",
    "        \"Questions\",\n",
    "        \"Chameleon\",\n",
    "        \"Tolokers\",\n",
    "        \"Flickr\",\n",
    "        \"Yelp\",\n",
    "        \"Amazon-ratings\",\n",
    "        \"genius\",\n",
    "        \"cora\",\n",
    "        \"CiteSeer\",\n",
    "        \"dblp\",\n",
    "        \"Computers\",\n",
    "        \"PubMed\",\n",
    "        \"pubmed\",\n",
    "        \"Reddit\",\n",
    "        \"cora_ml\",\n",
    "        \"Cora\",\n",
    "        \"Reddit2\",\n",
    "        \"CS\",\n",
    "        \"Photo\",\n",
    "        \"Physics\",\n",
    "        \"citeseer\"\n",
    "    ]\n",
    " \n",
    "    \n",
    "    #ALL_DATASETs= [\"karate\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        result_file = open(\"Results/ACMgcnp.txt\",'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        \n",
    "        \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            \n",
    "            if data.num_nodes>100000:\n",
    "                accuracy, itr = -1, -1\n",
    "            else:\n",
    "                accuracy, itr, _ =  ACMperformanceSampler(data, dataset, num_classes, epochs=150, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f07ff6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import generate_synthetic2homophily\n",
    "import torch_geometric.utils.homophily as homophily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2e0f8",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42f287e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora - 10 loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "[0.3597285067873303, 0.3597285067873303, 0.3438914027149321, 0.34615384615384615, 0.3574660633484163] [250, 250, 250, 250, 250]\n",
      "acc 35.3394 \\pm 0.6922 itr 250 sd 0\n",
      "Runtime:  32.03099513053894\n"
     ]
    }
   ],
   "source": [
    "def ablation(num_run = 1):\n",
    "    \n",
    "    #SYN_NAME = random.randint(0,1000)\n",
    "\n",
    "    ALL_DATASETs= [\n",
    "        \"Tolokers\",\"Computers\",\"Photo\"\n",
    "    ]\n",
    "    \n",
    "    ALL_DATASETs= [\"Cora\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/AGM-GCNablation.txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        \n",
    "        random_state = 10\n",
    "        \n",
    "        print(DATASET_NAME,\"-\",random_state, end=' ')\n",
    "        \n",
    "        \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "            d = 100\n",
    "            h =0.05\n",
    "            train=0.3\n",
    "            balance=True\n",
    "            h2 = 0.25\n",
    "            ratio = 0.50\n",
    "                                    \n",
    "#             global data_filename_extension\n",
    "#             data_filename_extension = str(d)+str(h)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "#             data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "#             if os.path.exists(data_filename):\n",
    "#                 data = torch.load(data_filename)                \n",
    "#                 print(\"loaded \"+data_filename)\n",
    "#             else:\n",
    "#                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False, balance=balance)\n",
    "# #                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False)\n",
    "#                 torch.save(data,data_filename)\n",
    "#                 print(\"saved \"+data_filename)\n",
    "        \n",
    "            global data_filename_extension\n",
    "            data_filename_extension = str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "            data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "            if os.path.exists(data_filename):\n",
    "                data = torch.load(data_filename)                \n",
    "                print(\"loaded \"+data_filename)\n",
    "            else:\n",
    "                data = generate_synthetic2homophily(data, d=d, h1=h, h2=h2, ratio=ratio, train=train, random_state=random_state, log=False, balance=balance)                 \n",
    "                torch.save(data,data_filename)\n",
    "                print(\"saved \"+data_filename)\n",
    "    \n",
    "            ##Sparsifiy\n",
    "            #data = random_sparsify(data, 13, log = True)\n",
    "#             data = sparsify(data, log = True, method = 'submodular', metric= 'cosine')\n",
    "                        \n",
    "#             data1 = sparsify(copy.deepcopy(data), log = True, method = 'submodular', metric= 'cosine')\n",
    "#             data = sparsify(data, log = True, method = 'nn', metric= 'cosine')                         \n",
    "#             data.edge_index = torch.cat((data.edge_index, data1.edge_index), dim=1)\n",
    "            \n",
    "            #optional for making undirected graph\n",
    "            (row, col) = data.edge_index\n",
    "            data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "            data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "            \n",
    "            if True:\n",
    "                print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "                print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "                print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "                print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 250\n",
    "            else:\n",
    "                max_epochs = 20\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon','cornell5','penn94','johnshopkins55','amherst41']:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "                              \n",
    "            accuracy, itr, _ = ACMperformanceSampler(data, dataset, num_classes, epochs=250, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "\n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs)*100:0.4f} \\pm {np.std(accs)*100:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs)*100:0.4f} \\pm {np.std(accs)*10:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "    return \n",
    "\n",
    "st_time = time.time()\n",
    "ablation(num_run=5)\n",
    "en_time = time.time()\n",
    "\n",
    "print(\"Runtime: \", en_time-st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c926a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
