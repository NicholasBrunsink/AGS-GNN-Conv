{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e16d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf7a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed18bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected, sort_edge_index\n",
    "from torch_geometric.data import NeighborSampler, ClusterData, ClusterLoader, Data, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler, RandomNodeSampler\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from logger import Logger, SimpleLogger\n",
    "from dataset import load_nc_dataset, NCDataset\n",
    "from data_utils import normalize, gen_normalized_adjs, evaluate, eval_acc, eval_rocauc, to_sparse_tensor\n",
    "from parse import parse_method, parser_add_main_args\n",
    "from batch_utils import nc_dataset_to_torch_geo, torch_geo_to_nc_dataset, AdjRowLoader, make_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1286618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    \n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    ### Parse args ###\n",
    "    parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
    "    parser_add_main_args(parser)\n",
    "    parser.add_argument('--train_batch', type=str, default='cluster', help='type of mini batch loading scheme for training GNN')\n",
    "    parser.add_argument('--no_mini_batch_test', action='store_true', help='whether to test on mini batches as well')\n",
    "    parser.add_argument('--batch_size', type=int, default=10000)\n",
    "    parser.add_argument('--num_parts', type=int, default=100, help='number of partitions for partition batching')\n",
    "    parser.add_argument('--cluster_batch_size', type=int, default=1, help='number of clusters to use per cluster-gcn step')\n",
    "    parser.add_argument('--saint_num_steps', type=int, default=5, help='number of steps for graphsaint')\n",
    "    parser.add_argument('--test_num_parts', type=int, default=10, help='number of partitions for testing')\n",
    "    \n",
    "    #parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    #parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    #parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    #parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('--use_normalization', action='store_true')    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e649364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.datasets import LINKXDataset\n",
    "# from torch_geometric.nn import LINKX\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn import GCNConv, SGConv, GATConv, JumpingKnowledge, APPNP, GCN2Conv, MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import scipy.sparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d9729",
   "metadata": {},
   "source": [
    "# LINKX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc9b94",
   "metadata": {},
   "source": [
    "### Available models\n",
    "LINK, GCN, MLP, SGC, GAT, SGCMem, MultiLP, MixHop, \n",
    "\n",
    "GCNJK, GATJK, H2GCN, APPNP_Net, LINK_Concat, LINKX, GPRGNN, GCNII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc9485",
   "metadata": {},
   "source": [
    "### Available Sampler\n",
    "\n",
    "-- NeighborSampler, \n",
    "\n",
    "-- ClusterData, ClusterLoader, \n",
    "\n",
    "-- GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler\n",
    "\n",
    "-- RandomNodeSampler\n",
    "\n",
    "### AGS-GNN: our work\n",
    "-- KNNsampler\n",
    "\n",
    "-- SubmodularSampler\n",
    "\n",
    "-- LinkSampler\n",
    "\n",
    "-- JointSampler\n",
    "\n",
    "-- DisjointEdgeSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76acea",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d57ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    \n",
    "#     print(sum(mask))\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    model.eval()            \n",
    "    with torch.no_grad():\n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=sum(mask).item())\n",
    "            pbar.set_description(f'Evaluating {name}')\n",
    "        \n",
    "        for tg_batch in loader:\n",
    "            node_ids = tg_batch.node_ids\n",
    "            batch_dataset = torch_geo_to_nc_dataset(tg_batch, device=device)\n",
    "            out = model(batch_dataset)\n",
    "            \n",
    "            batch_test_idx = tg_batch.mask.to(torch.bool)\n",
    "            pred = out[batch_test_idx].argmax(dim=-1)\n",
    "            correct = pred.eq(batch_dataset.label[batch_test_idx])\n",
    "            \n",
    "            total_correct+=correct.sum().cpu().item()\n",
    "            total_examples+=sum(tg_batch.mask).item()            \n",
    "            \n",
    "#             pred = out[tg_batch.mask].argmax(dim=-1).cpu()\n",
    "#             correct = pred.eq(tg_batch.y[tg_batch.mask])\n",
    "\n",
    "#             total_correct+=correct.sum().item()\n",
    "#             total_examples+=sum(tg_batch.mask).item()\n",
    "    \n",
    "            if args.log_info:\n",
    "                pbar.update(sum(tg_batch.mask).item())\n",
    "    \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "    \n",
    "#     print(total_correct)\n",
    "#     print(total_examples)\n",
    "\n",
    "    return total_correct/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d390b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_pred):    \n",
    "    \n",
    "    y_true = y_true.detach().cpu()\n",
    "    y_pred = y_pred.argmax(dim=-1).detach().cpu()\n",
    "    \n",
    "    correct = y_pred.eq(y_true)\n",
    "    acc = correct.sum().item()/len(y_true)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63f7424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkxtest(model, data, nc_dataset, dataset, loader, num_classes):   \n",
    "    # needs a loader that includes every node in the graph\n",
    "    model.eval()\n",
    "    \n",
    "    full_out = torch.zeros(data.num_nodes, num_classes, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tg_batch in loader:\n",
    "            node_ids = tg_batch.node_ids\n",
    "            batch_dataset = torch_geo_to_nc_dataset(tg_batch, device=device)\n",
    "            out = model(batch_dataset)\n",
    "            full_out[node_ids] = out\n",
    "    \n",
    "\n",
    "    train_acc = eval_accuracy(data.y[data.train_mask], out[data.train_mask])\n",
    "    valid_acc = eval_accuracy(data.y[data.val_mask], out[data.val_mask])\n",
    "    test_acc = eval_accuracy(data.y[data.test_mask], out[data.test_mask])\n",
    "        \n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e200f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, nc_dataset, dataset, epochs, num_classes):\n",
    "    \n",
    "    #model.reset_parameters()\n",
    "    \n",
    "    if args.rocauc or args.dataset in ('yelp-chi', 'twitch-e', 'ogbn-proteins'):\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.NLLLoss()        \n",
    "        \n",
    "    if args.log_info:    \n",
    "        print('making train loader')    \n",
    "    \n",
    "    train_idx = torch.nonzero(data.train_mask).squeeze()\n",
    "    val_idx = torch.nonzero(data.val_mask).squeeze()\n",
    "    test_idx = torch.nonzero(data.test_mask).squeeze()\n",
    "    \n",
    "    worker = 0\n",
    "    if data.num_nodes>100000:\n",
    "        worker=8\n",
    "        \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "\n",
    "    sample_batch_size=512\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                            batch_size=sample_batch_size, shuffle=False, num_workers=num_workers)\n",
    "    val_loader = NeighborLoader(data,input_nodes=data.val_mask,num_neighbors=test_neighbors, \n",
    "                                batch_size=sample_batch_size,shuffle=False, num_workers=num_workers)\n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "                                 batch_size=sample_batch_size,shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = make_loader(args, nc_dataset, train_idx, mini_batch = True, num_workers=worker)\n",
    "    val_loader = make_loader(args, nc_dataset, val_idx, mini_batch = True, test=True, num_workers=worker)\n",
    "    test_loader = make_loader(args, nc_dataset, test_idx, mini_batch = True, test=True, num_workers=worker)\n",
    "    \n",
    "    model.reset_parameters()\n",
    "    if args.adam:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(train_loader)\n",
    "        print(val_loader)\n",
    "    \n",
    "    \n",
    "    train_losses=[]\n",
    "    best_acc = 0 \n",
    "    num_iteration = epochs    \n",
    "    print_once = True\n",
    "    \n",
    "    val_accuracies = []; train_accuracies = []; test_accuracies = [];training_times = [];\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_examples = 0\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=len(train_loader))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "                \n",
    "        for tg_batch in train_loader:\n",
    "            \n",
    "            if args.log_info and print_once:\n",
    "                print_once = False\n",
    "                print(tg_batch)\n",
    "                print(tg_batch.mask)\n",
    "            \n",
    "            batch_train_idx = tg_batch.mask.to(torch.bool)\n",
    "            batch_dataset = torch_geo_to_nc_dataset(tg_batch, device=device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(batch_dataset)\n",
    "            if args.rocauc or args.dataset in ('yelp-chi', 'twitch-e', 'ogbn-proteins'):\n",
    "                if dataset.label.shape[1] == 1:\n",
    "                    # change -1 instances to 0 for one-hot transform\n",
    "                    # dataset.label[dataset.label==-1] = 0\n",
    "                    true_label = F.one_hot(batch_dataset.label, batch_dataset.label.max() + 1).squeeze(1)\n",
    "                else:\n",
    "                    true_label = batch_dataset.label\n",
    "\n",
    "                loss = criterion(out[batch_train_idx], true_label[batch_train_idx].to(out.dtype))\n",
    "            else:                                            \n",
    "#                 loss = F.cross_entropy(out[batch_train_idx], batch_dataset.label[batch_train_idx])\n",
    "                \n",
    "                out = F.log_softmax(out, dim=1)\n",
    "#                 #loss = criterion(out[batch_train_idx], batch_dataset.label.squeeze(1)[batch_train_idx])\n",
    "                loss = criterion(out[batch_train_idx], batch_dataset.label[batch_train_idx])\n",
    "#                 loss = F.nll_loss(out[batch_train_idx], batch_dataset.label[batch_train_idx])\n",
    "            num_examples                \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss+=loss.item()\n",
    "            num_examples+= len(batch_train_idx)\n",
    "        \n",
    "            if args.log_info:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close() \n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        training_times.append(epoch_end-epoch_start)\n",
    "        \n",
    "        train_losses.append(total_loss/num_examples)   \n",
    "        \n",
    "#         train_acc, val_acc, test_acc = linkxtest(model, data, nc_dataset, dataset, val_loader, num_classes)\n",
    "        if args.log_info:      \n",
    "            train_acc=test(model, train_loader,data.train_mask,'Train')\n",
    "        else:\n",
    "            train_acc=0\n",
    "            \n",
    "        if args.log_info:\n",
    "            val_acc = test(model, val_loader, data.val_mask,'Validation')\n",
    "        else:\n",
    "            val_acc=0\n",
    "            \n",
    "        test_acc = test(model, test_loader, data.test_mask,'Test')\n",
    "        \n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "                \n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "        \n",
    "        std_dev = np.std(train_losses[-5:])        \n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "                \n",
    "#         if epoch>=5 and std_dev<=1e-4:\n",
    "#             num_iteration = epoch\n",
    "            \n",
    "#             if args.log_info:                \n",
    "#                 print(\"Iteration for convergence: \", epoch)\n",
    "#             break\n",
    "            \n",
    "    if args.log_info:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='../Results/AGSGSValidation', yname='Accuracy', xname='Epoch')\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "    \n",
    "    acc_file = open(\"../Runtime/LINKX.txt\",'a+') \n",
    "    acc_file.write(str(train_losses))\n",
    "    acc_file.write(str(train_accuracies))\n",
    "    acc_file.write(str(val_accuracies))\n",
    "    acc_file.write(str(test_accuracies))\n",
    "    acc_file.write(str(training_times))\n",
    "    acc_file.write(str(np.mean(training_times)))\n",
    "    acc_file.write(f'\\nworker {worker:1d} avg epoch runtime {np.mean(training_times):0.8f}')\n",
    "    acc_file.close()     \n",
    "                \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AllperformanceSampler(data, dataset, num_classes, epochs=1, train_neighbors=[8,4], test_neighbors=[8,4]):        \n",
    "    \n",
    "    n = data.num_nodes\n",
    "    c = num_classes\n",
    "    d = data.x.shape[1]\n",
    "        \n",
    "    nc_dataset = torch_geo_to_nc_dataset(data)    \n",
    "    model = parse_method(args, nc_dataset, n, c, d, device)\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(model) \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, nc_dataset, dataset, epochs, num_classes)        \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dae9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['link', 'gcn', 'mlp', 'cs', 'sgc', \n",
    "           'gprgnn', 'appnp', 'gat', 'lp', \n",
    "           'mixhop','gcnjk','gatjk','h2gcn',\n",
    "           'link_concat','linkx','gcn2']\n",
    "\n",
    "#others = ['gsage','gsaint','acmgcn','clustergcn','gcn','gin','gat','linkx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_methods = ['cluster', 'graphsaint-node', 'graphsaint-edge', 'graphsaint-rw',\n",
    "            'random','full-batch', 'row']\n",
    "\n",
    "# train_batch cluster\n",
    "# batch_size 10000\n",
    "# num_parts 100\n",
    "# cluster_batch_size 1\n",
    "# saint_num_steps 5\n",
    "# test_num_parts 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113ff79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.log_info = True\n",
    "\n",
    "DATASET_NAME = 'Cora'\n",
    "args.dataset = DATASET_NAME\n",
    "gnn_name = 'linkx'\n",
    "\n",
    "args.method = gnn_name\n",
    "args.train_batch = 'random'\n",
    "args.num_parts = 100\n",
    "\n",
    "data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "print(data)\n",
    "\n",
    "# if data.num_nodes>100000:\n",
    "#     args.num_parts = 100\n",
    "#     #args.batch_size = 4096\n",
    "# else:\n",
    "#     args.num_parts = 1\n",
    "\n",
    "best_acc, num_iteration, _ = AllperformanceSampler(data, dataset, dataset.num_classes, epochs=50)\n",
    "print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20bc0",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\"reed98\",\"amherst41\",\n",
    "#         \"genius\",\n",
    "#         #\"AmazonProducts\",\n",
    "#         \"cornell5\",\"penn94\",\"johnshopkins55\",\n",
    "#         #\"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "#         #\"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "#         #\"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",     \n",
    "#         \"Cora\",\n",
    "        \"Reddit\",\n",
    "        \"genius\",\n",
    "        \"Yelp\",\n",
    "    ]\n",
    " \n",
    "    \n",
    "#     ALL_DATASETs= [\"karate\"]\n",
    "    \n",
    "    \n",
    "    gnn_name = 'linkx'\n",
    "    args.method = gnn_name\n",
    "    args.train_batch = 'random'\n",
    "    args.num_parts = 100\n",
    "    \n",
    "    \n",
    "    result_file = open(\"../Results/LINKXscale.txt\",'a+')        \n",
    "    result_file.write(f'{gnn_name} ')\n",
    "    result_file.write(f'{args.train_batch} ')\n",
    "    result_file.write(f'{args.num_parts} ')\n",
    "    result_file.close()\n",
    "    \n",
    "    \n",
    "    runtime_filename = \"../Runtime/LINKX.txt\"\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        result_file = open(\"../Results/LINKXscale.txt\",'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        \n",
    "        acc_file = open(runtime_filename,'a+') \n",
    "        acc_file.write(f'{DATASET_NAME}\\n')\n",
    "        acc_file.close()     \n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "            if data.num_nodes<100000:\n",
    "                accs.append(-1)\n",
    "                itrs.append(-1)\n",
    "                break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 20\n",
    "                              \n",
    "            accuracy, itr, _ = AllperformanceSampler(data, dataset, num_classes, epochs=max_epochs)\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07532558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311cu117pyg200",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
