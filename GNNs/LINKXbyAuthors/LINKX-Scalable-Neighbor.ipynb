{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e16d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf7a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c97254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected, sort_edge_index\n",
    "from torch_geometric.data import NeighborSampler, ClusterData, ClusterLoader, Data, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler, RandomNodeSampler\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from logger import Logger, SimpleLogger\n",
    "from dataset import load_nc_dataset, NCDataset\n",
    "from data_utils import normalize, gen_normalized_adjs, evaluate, eval_acc, eval_rocauc, to_sparse_tensor\n",
    "from parse import parse_method, parser_add_main_args\n",
    "from batch_utils import nc_dataset_to_torch_geo, torch_geo_to_nc_dataset, AdjRowLoader, make_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1286618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    \n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    ### Parse args ###\n",
    "    parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
    "    parser_add_main_args(parser)\n",
    "    parser.add_argument('--train_batch', type=str, default='cluster', help='type of mini batch loading scheme for training GNN')\n",
    "    parser.add_argument('--no_mini_batch_test', action='store_true', help='whether to test on mini batches as well')\n",
    "    parser.add_argument('--batch_size', type=int, default=10000)\n",
    "    parser.add_argument('--num_parts', type=int, default=100, help='number of partitions for partition batching')\n",
    "    parser.add_argument('--cluster_batch_size', type=int, default=1, help='number of clusters to use per cluster-gcn step')\n",
    "    parser.add_argument('--saint_num_steps', type=int, default=5, help='number of steps for graphsaint')\n",
    "    parser.add_argument('--test_num_parts', type=int, default=10, help='number of partitions for testing')\n",
    "    \n",
    "    #parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    #parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    #parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    #parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('--use_normalization', action='store_true')    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e649364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.datasets import LINKXDataset\n",
    "# from torch_geometric.nn import LINKX\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573fbd9",
   "metadata": {},
   "source": [
    "# LINKX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4c3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn import GCNConv, SGConv, GATConv, JumpingKnowledge, APPNP, GCN2Conv, MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5c16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout=.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        if num_layers == 1:\n",
    "            # just linear layer i.e. logistic regression\n",
    "            self.lins.append(nn.Linear(in_channels, out_channels))\n",
    "        else:\n",
    "            self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "            self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, batch_data, input_tensor=False):\n",
    "        if not input_tensor:\n",
    "            x = batch_data.x\n",
    "        else:\n",
    "            x = batch_data\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = F.relu(x, inplace=True)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a08036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINKXcustom(nn.Module):\t\n",
    "    \"\"\" our LINKX method with skip connections \n",
    "        a = MLP_1(A), x = MLP_2(X), MLP_3(sigma(W_1[a, x] + a + x))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, num_nodes, dropout=.5, cache=False, inner_activation=False, inner_dropout=False, init_layers_A=1, init_layers_X=1):\n",
    "        super(LINKXcustom, self).__init__()\t\n",
    "        self.mlpA = MLP(num_nodes, hidden_channels, hidden_channels, init_layers_A, dropout=0)\n",
    "        self.mlpX = MLP(in_channels, hidden_channels, hidden_channels, init_layers_X, dropout=0)\n",
    "        self.W = nn.Linear(2*hidden_channels, hidden_channels)\n",
    "        self.mlp_final = MLP(hidden_channels, hidden_channels, out_channels, num_layers, dropout=dropout)\n",
    "        self.in_channels = in_channels\n",
    "        self.num_nodes = num_nodes\n",
    "        self.A = None\n",
    "        self.inner_activation = inner_activation\n",
    "        self.inner_dropout = inner_dropout\n",
    "\n",
    "    def reset_parameters(self):\t\n",
    "        self.mlpA.reset_parameters()\t\n",
    "        self.mlpX.reset_parameters()\n",
    "        self.W.reset_parameters()\n",
    "        self.mlp_final.reset_parameters()\t\n",
    "\n",
    "    def forward(self, batch_data):\t\n",
    "        m = batch_data.num_nodes\t\n",
    "        feat_dim = batch_data.x\n",
    "        row, col = batch_data.edge_index\n",
    "        row = row-row.min()\n",
    "        A = SparseTensor(row=row, col=col,\t\n",
    "                 sparse_sizes=(m, self.num_nodes)\n",
    "                        ).to_torch_sparse_coo_tensor()\n",
    "\n",
    "        xA = self.mlpA(A, input_tensor=True)\n",
    "        xX = self.mlpX(batch_data.x, input_tensor=True)\n",
    "        x = torch.cat((xA, xX), axis=-1)\n",
    "        x = self.W(x)\n",
    "        if self.inner_dropout:\n",
    "            x = F.dropout(x)\n",
    "        if self.inner_activation:\n",
    "            x = F.relu(x)\n",
    "        x = F.relu(x + xA + xX)\n",
    "        x = self.mlp_final(x, input_tensor=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682b013",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d57ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            #out = model(batch_data.x, batch_data.edge_index,batch_data.edge_weight)\n",
    "            out = model(batch_data)\n",
    "            out=out[:batch_data.batch_size,:]\n",
    "            pred = out.argmax(dim=-1)            \n",
    "            correct = pred.eq(batch_data.y[:batch_data.batch_size].to(device))\n",
    "\n",
    "            total_correct+=correct.sum()\n",
    "            total_examples+=batch_data.batch_size\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_data.batch_size)\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "\n",
    "    return total_correct.item()/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fcc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, train_neighbors=[8,4], test_neighbors=[8,4]):\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train Neighbors: \", train_neighbors)\n",
    "        print(\"Test Neighbors: \", test_neighbors)\n",
    "    \n",
    " \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)    \n",
    "    batch_size=1024\n",
    "    loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                            batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = NeighborLoader(data,input_nodes=data.val_mask, num_neighbors=test_neighbors, \n",
    "                                batch_size=batch_size,shuffle=False, num_workers=0)\n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "                                 batch_size=batch_size,shuffle=False, num_workers=0)    \n",
    "    \n",
    "    train_losses=[]\n",
    "    best_acc = 0 \n",
    "    num_iteration = epochs\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=int(sum(data.train_mask)))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        for i,batch_data in enumerate(loader):                \n",
    "            batch_data = batch_data.to(device)\n",
    "            \n",
    "            print(batch_data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #out = model(batch_data.x, batch_data.edge_index)\n",
    "            out = model(batch_data)\n",
    "            \n",
    "            #loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "            loss = F.cross_entropy(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * sum(batch_data.train_mask).item()\n",
    "            total_examples += sum(batch_data.train_mask).item()\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_size)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "        \n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)     \n",
    "        \n",
    "        #train_acc=test(model, train_loader,data.train_mask,'Train')\n",
    "        train_acc=0\n",
    "        val_acc = test(model, val_loader, data.val_mask,'Validation')\n",
    "        test_acc = test(model, test_loader, data.test_mask,'Test')\n",
    "                \n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "        \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "                \n",
    "        if epoch>=5 and std_dev<=1e-4:\n",
    "            num_iteration = epoch\n",
    "            \n",
    "            if args.log_info:                \n",
    "                print(\"Iteration for convergence: \", epoch)\n",
    "            break\n",
    "                \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52fdabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LINKXperformanceSampler(data, dataset, num_classes, epochs=1, train_neighbors=[8,4], test_neighbors=[8,4]):        \n",
    "    model = LINKXcustom(in_channels=data.x.shape[1], hidden_channels=64, out_channels = dataset.num_classes, \n",
    "                        num_layers=1, num_nodes=data.num_nodes).to(device)\n",
    "    if args.log_info:\n",
    "        print(model) \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)    \n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7113ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 \n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "LINKXcustom(\n",
      "  (mlpA): MLP(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=34, out_features=64, bias=True)\n",
      "    )\n",
      "    (bns): ModuleList()\n",
      "  )\n",
      "  (mlpX): MLP(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=34, out_features=64, bias=True)\n",
      "    )\n",
      "    (bns): ModuleList()\n",
      "  )\n",
      "  (W): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (mlp_final): MLP(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "    (bns): ModuleList()\n",
      "  )\n",
      ")\n",
      "Train Neighbors:  [8, 4]\n",
      "Test Neighbors:  [8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: : 1024it [00:00, 168522.61it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[24, 34], edge_index=[2, 74], y=[24], train_mask=[24], val_mask=[24], test_mask=[24], n_id=[24], e_id=[74], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 13786.47it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 14543.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.4029, Train: 0.0000, Val: 0.2667, Test: 0.3333, Std dev: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: : 1024it [00:00, 258608.34it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[26, 34], edge_index=[2, 66], y=[26], train_mask=[26], val_mask=[26], test_mask=[26], n_id=[26], e_id=[66], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15147.36it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15194.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Loss: 1.2962, Train: 0.0000, Val: 0.3333, Test: 0.3000, Std dev: 0.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: : 1024it [00:00, 249910.82it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28, 34], edge_index=[2, 65], y=[28], train_mask=[28], val_mask=[28], test_mask=[28], n_id=[28], e_id=[65], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14836.59it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15218.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 1.2574, Train: 0.0000, Val: 0.2333, Test: 0.2667, Std dev: 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: : 1024it [00:00, 264338.21it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[29, 34], edge_index=[2, 64], y=[29], train_mask=[29], val_mask=[29], test_mask=[29], n_id=[29], e_id=[64], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14632.99it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15167.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Loss: 1.0965, Train: 0.0000, Val: 0.2667, Test: 0.2667, Std dev: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: : 1024it [00:00, 260190.66it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[24, 34], edge_index=[2, 69], y=[24], train_mask=[24], val_mask=[24], test_mask=[24], n_id=[24], e_id=[69], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14689.37it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15261.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 0.9062, Train: 0.0000, Val: 0.2000, Test: 0.1667, Std dev: 0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: : 1024it [00:00, 259201.41it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[25, 34], edge_index=[2, 74], y=[25], train_mask=[25], val_mask=[25], test_mask=[25], n_id=[25], e_id=[74], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14597.35it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15239.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Loss: 0.8546, Train: 0.0000, Val: 0.1333, Test: 0.1333, Std dev: 0.1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: : 1024it [00:00, 258219.64it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[27, 34], edge_index=[2, 65], y=[27], train_mask=[27], val_mask=[27], test_mask=[27], n_id=[27], e_id=[65], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14789.51it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15324.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Loss: 0.6584, Train: 0.0000, Val: 0.1333, Test: 0.1333, Std dev: 0.2058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: : 1024it [00:00, 257122.08it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28, 34], edge_index=[2, 73], y=[28], train_mask=[28], val_mask=[28], test_mask=[28], n_id=[28], e_id=[73], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14540.00it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15094.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Train Loss: 0.5849, Train: 0.0000, Val: 0.2000, Test: 0.1667, Std dev: 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: : 1024it [00:00, 258235.17it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28, 34], edge_index=[2, 71], y=[28], train_mask=[28], val_mask=[28], test_mask=[28], n_id=[28], e_id=[71], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14672.24it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15298.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Train Loss: 0.6046, Train: 0.0000, Val: 0.1667, Test: 0.1667, Std dev: 0.1328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: : 1024it [00:00, 260759.35it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28, 34], edge_index=[2, 68], y=[28], train_mask=[28], val_mask=[28], test_mask=[28], n_id=[28], e_id=[68], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[4], batch_size=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14815.63it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 14643.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Loss: 0.3749, Train: 0.0000, Val: 0.1667, Test: 0.2000, Std dev: 0.1536\n",
      "0.3333333333333333 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.log_info = True\n",
    "DATASET_NAME = 'karate'\n",
    "data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "print(data)\n",
    "best_acc, num_iteration, _ = LINKXperformanceSampler(data, dataset, dataset.num_classes, epochs=10, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e252d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from torch_geometric.loader import ClusterData, ClusterLoader, NeighborSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42fd4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_dir = DIR+'ClusterGCNtest/'+DATASET_NAME\n",
    "# if not os.path.exists(sampler_dir):\n",
    "#     os.makedirs(sampler_dir)\n",
    "\n",
    "# num_parts=2\n",
    "\n",
    "# start_time = time.time()\n",
    "# cluster_data = ClusterData(data, num_parts=num_parts, recursive=False,save_dir=sampler_dir)\n",
    "# train_loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True,num_workers=0)\n",
    "# subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024,shuffle=False, num_workers=0)\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(cluster_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20bc0",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af59b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\"reed98\",\"amherst41\",\"genius\",\n",
    "        \"AmazonProducts\",\n",
    "#         \"cornell5\",\"penn94\",\"johnshopkins55\",\n",
    "        \"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "        \"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "        \"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",                \n",
    "    ]\n",
    " \n",
    "    \n",
    "#     ALL_DATASETs= [\"karate\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        result_file = open(\"Results/LINKX.txt\",'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                              \n",
    "            accuracy, itr, _ = LINKXperformanceSampler(data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07532558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311cu117pyg200",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
