{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21e16d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdf7a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79f940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0aa9fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected, sort_edge_index\n",
    "from torch_geometric.data import NeighborSampler, ClusterData, ClusterLoader, Data, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler, RandomNodeSampler\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from logger import Logger, SimpleLogger\n",
    "from dataset import load_nc_dataset, NCDataset\n",
    "from data_utils import normalize, gen_normalized_adjs, evaluate, eval_acc, eval_rocauc, to_sparse_tensor\n",
    "from parse import parse_method, parser_add_main_args\n",
    "from batch_utils import nc_dataset_to_torch_geo, torch_geo_to_nc_dataset, AdjRowLoader, make_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1286618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    \n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    ### Parse args ###\n",
    "    parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
    "    parser_add_main_args(parser)\n",
    "    parser.add_argument('--train_batch', type=str, default='cluster', help='type of mini batch loading scheme for training GNN')\n",
    "    parser.add_argument('--no_mini_batch_test', action='store_true', help='whether to test on mini batches as well')\n",
    "    parser.add_argument('--batch_size', type=int, default=10000)\n",
    "    parser.add_argument('--num_parts', type=int, default=100, help='number of partitions for partition batching')\n",
    "    parser.add_argument('--cluster_batch_size', type=int, default=1, help='number of clusters to use per cluster-gcn step')\n",
    "    parser.add_argument('--saint_num_steps', type=int, default=5, help='number of steps for graphsaint')\n",
    "    parser.add_argument('--test_num_parts', type=int, default=10, help='number of partitions for testing')\n",
    "    \n",
    "    #parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    #parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    #parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    #parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('--use_normalization', action='store_true')    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e649364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.datasets import LINKXDataset\n",
    "# from torch_geometric.nn import LINKX\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn import GCNConv, SGConv, GATConv, JumpingKnowledge, APPNP, GCN2Conv, MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d4e689",
   "metadata": {},
   "source": [
    "# LINKX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506ee05",
   "metadata": {},
   "source": [
    "### Available models\n",
    "LINK, GCN, MLP, SGC, GAT, SGCMem, MultiLP, MixHop, \n",
    "\n",
    "GCNJK, GATJK, H2GCN, APPNP_Net, LINK_Concat, LINKX, GPRGNN, GCNII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2088b",
   "metadata": {},
   "source": [
    "### Available Sampler\n",
    "\n",
    "NeighborSampler, \n",
    "\n",
    "ClusterData, ClusterLoader, \n",
    "\n",
    "GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler, \n",
    "\n",
    "RandomNodeSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c35249",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d57ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_dataset = torch_geo_to_nc_dataset(batch_data, device=device)\n",
    "            out = model(batch_dataset)\n",
    "            out=out[:batch_data.batch_size,:]\n",
    "            pred = out.argmax(dim=-1)            \n",
    "            correct = pred.eq(batch_data.y[:batch_data.batch_size].to(device))\n",
    "\n",
    "            total_correct+=correct.sum()\n",
    "            total_examples+=batch_data.batch_size\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_data.batch_size)\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "\n",
    "    return total_correct.item()/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5fcc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs, train_neighbors=[8,4], test_neighbors=[8,4]):\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train Neighbors: \", train_neighbors)\n",
    "        print(\"Test Neighbors: \", test_neighbors)\n",
    "     \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)    \n",
    "    batch_size=1024\n",
    "    loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                            batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = NeighborLoader(data,input_nodes=data.val_mask, num_neighbors=test_neighbors, \n",
    "                                batch_size=batch_size,shuffle=False, num_workers=0)\n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "                                 batch_size=batch_size,shuffle=False, num_workers=0)    \n",
    "    \n",
    "    train_losses=[]\n",
    "    best_acc = 0 \n",
    "    num_iteration = epochs\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=int(sum(data.train_mask)))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        for i,batch_data in enumerate(loader):                \n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_dataset = torch_geo_to_nc_dataset(batch_data, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            out = model(batch_dataset)\n",
    "            \n",
    "            #loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "            loss = F.cross_entropy(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * sum(batch_data.train_mask).item()\n",
    "            total_examples += sum(batch_data.train_mask).item()\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_size)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "        \n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)     \n",
    "        \n",
    "        #train_acc=test(model, train_loader,data.train_mask,'Train')\n",
    "        train_acc=0\n",
    "        val_acc = test(model, val_loader, data.val_mask,'Validation')\n",
    "        test_acc = test(model, test_loader, data.test_mask,'Test')\n",
    "                \n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "        \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "                \n",
    "        if epoch>=5 and std_dev<=1e-4:\n",
    "            num_iteration = epoch\n",
    "            \n",
    "            if args.log_info:                \n",
    "                print(\"Iteration for convergence: \", epoch)\n",
    "            break\n",
    "                \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52fdabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LINKXperformanceSampler(data, dataset, num_classes, epochs=1, train_neighbors=[8,4], test_neighbors=[8,4]):        \n",
    "    from models import LINKX\n",
    "    \n",
    "    model = LINKX(in_channels=data.x.shape[1], hidden_channels=64, out_channels = dataset.num_classes, \n",
    "                        num_layers=1, num_nodes=data.num_nodes).to(device)\n",
    "    if args.log_info:\n",
    "        print(model) \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)    \n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7113ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 \n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n",
      "LINKX(\n",
      "  (mlpA): MLP(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=34, out_features=64, bias=True)\n",
      "    )\n",
      "    (bns): ModuleList()\n",
      "  )\n",
      "  (mlpX): MLP(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=34, out_features=64, bias=True)\n",
      "    )\n",
      "    (bns): ModuleList()\n",
      "  )\n",
      "  (W): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (mlp_final): MLP(\n",
      "    (lins): ModuleList(\n",
      "      (0): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "    (bns): ModuleList()\n",
      "  )\n",
      ")\n",
      "Train Neighbors:  [8, 4]\n",
      "Test Neighbors:  [8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: : 1024it [00:00, 160679.66it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15925.72it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15791.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.4216, Train: 0.0000, Val: 0.3000, Test: 0.3333, Std dev: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: : 1024it [00:00, 257507.48it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15272.38it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 14359.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Loss: 1.3428, Train: 0.0000, Val: 0.3667, Test: 0.3333, Std dev: 0.0394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: : 1024it [00:00, 262160.00it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14898.07it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15738.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 1.1308, Train: 0.0000, Val: 0.4333, Test: 0.4000, Std dev: 0.1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: : 1024it [00:00, 260901.91it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15311.40it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15526.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Loss: 0.9916, Train: 0.0000, Val: 0.6000, Test: 0.5667, Std dev: 0.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: : 1024it [00:00, 263139.77it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15051.33it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15766.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 1.0011, Train: 0.0000, Val: 0.5000, Test: 0.4667, Std dev: 0.1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: : 1024it [00:00, 267249.54it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15272.38it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15348.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Loss: 0.8015, Train: 0.0000, Val: 0.4667, Test: 0.4667, Std dev: 0.1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: : 1024it [00:00, 261266.94it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15062.14it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15632.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Loss: 0.6766, Train: 0.0000, Val: 0.4333, Test: 0.4333, Std dev: 0.1609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: : 1024it [00:00, 269564.26it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15196.75it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15867.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Train Loss: 0.4365, Train: 0.0000, Val: 0.4333, Test: 0.4333, Std dev: 0.2111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: : 1024it [00:00, 242680.94it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15807.68it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15797.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Train Loss: 0.5563, Train: 0.0000, Val: 0.4333, Test: 0.4667, Std dev: 0.1957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: : 1024it [00:00, 267866.24it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15831.55it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 16057.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Loss: 0.3771, Train: 0.0000, Val: 0.4667, Test: 0.4333, Std dev: 0.1550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: : 1024it [00:00, 272800.26it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15994.55it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15875.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Train Loss: 0.1669, Train: 0.0000, Val: 0.4667, Test: 0.4000, Std dev: 0.1720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: : 1024it [00:00, 264843.52it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15538.30it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 14406.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Train Loss: 0.1288, Train: 0.0000, Val: 0.4333, Test: 0.4000, Std dev: 0.1624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: : 1024it [00:00, 270839.15it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15636.77it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 16229.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Train Loss: 0.1282, Train: 0.0000, Val: 0.4000, Test: 0.4000, Std dev: 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: : 1024it [00:00, 270140.72it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 16225.55it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 16364.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Train Loss: 0.0233, Train: 0.0000, Val: 0.2333, Test: 0.2667, Std dev: 0.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: : 1024it [00:00, 276043.92it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 16167.17it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 16041.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Train Loss: 0.0230, Train: 0.0000, Val: 0.2000, Test: 0.1333, Std dev: 0.0596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: : 1024it [00:00, 273390.66it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15650.39it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15161.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Train Loss: 0.0180, Train: 0.0000, Val: 0.1667, Test: 0.1667, Std dev: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: : 1024it [00:00, 271885.00it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15065.75it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15382.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Train Loss: 0.0052, Train: 0.0000, Val: 0.1667, Test: 0.1667, Std dev: 0.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: : 1024it [00:00, 268889.21it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15169.27it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15594.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Train Loss: 0.0371, Train: 0.0000, Val: 0.1667, Test: 0.1667, Std dev: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: : 1024it [00:00, 261123.98it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14684.22it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 13900.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Train Loss: 0.0015, Train: 0.0000, Val: 0.1333, Test: 0.1667, Std dev: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: : 1024it [00:00, 268452.23it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15266.82it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15346.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Train Loss: 0.0013, Train: 0.0000, Val: 0.1333, Test: 0.2000, Std dev: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: : 1024it [00:00, 264957.88it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15467.62it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15416.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Train Loss: 0.0012, Train: 0.0000, Val: 0.2000, Test: 0.1667, Std dev: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: : 1024it [00:00, 268133.81it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15699.20it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15654.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Train Loss: 0.0004, Train: 0.0000, Val: 0.2000, Test: 0.2000, Std dev: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: : 1024it [00:00, 266586.02it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15570.98it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15475.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Train Loss: 0.0008, Train: 0.0000, Val: 0.2000, Test: 0.2000, Std dev: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: : 1024it [00:00, 269886.09it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15346.89it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15638.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Train Loss: 0.0008, Train: 0.0000, Val: 0.2000, Test: 0.2000, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: : 1024it [00:00, 269057.65it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15320.73it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15901.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Train Loss: 0.0006, Train: 0.0000, Val: 0.1667, Test: 0.1667, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: : 1024it [00:00, 272333.23it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15764.11it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 16096.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Train Loss: 0.0002, Train: 0.0000, Val: 0.2000, Test: 0.2000, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: : 1024it [00:00, 260379.95it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15322.59it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15760.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Train Loss: 0.0003, Train: 0.0000, Val: 0.2000, Test: 0.2333, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: : 1024it [00:00, 266768.16it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 15395.71it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15576.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Train Loss: 0.0001, Train: 0.0000, Val: 0.2333, Test: 0.2000, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 1024it [00:00, 264908.86it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14944.08it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15395.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Train Loss: 0.0001, Train: 0.0000, Val: 0.2000, Test: 0.2333, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: : 1024it [00:00, 256293.55it/s]      \n",
      "Evaluating Validation: 100%|██████████| 30/30 [00:00<00:00, 14841.84it/s]\n",
      "Evaluating Test: 100%|██████████| 30/30 [00:00<00:00, 15501.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Train Loss: 0.0001, Train: 0.0000, Val: 0.2000, Test: 0.2333, Std dev: 0.0001\n",
      "Iteration for convergence:  30\n",
      "0.5666666666666667 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.log_info = True\n",
    "DATASET_NAME = 'karate'\n",
    "data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "print(data)\n",
    "best_acc, num_iteration, _ = LINKXperformanceSampler(data, dataset, dataset.num_classes, epochs=100, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e252d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from torch_geometric.loader import ClusterData, ClusterLoader, NeighborSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42fd4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler_dir = DIR+'ClusterGCNtest/'+DATASET_NAME\n",
    "# if not os.path.exists(sampler_dir):\n",
    "#     os.makedirs(sampler_dir)\n",
    "\n",
    "# num_parts=2\n",
    "\n",
    "# start_time = time.time()\n",
    "# cluster_data = ClusterData(data, num_parts=num_parts, recursive=False,save_dir=sampler_dir)\n",
    "# train_loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True,num_workers=0)\n",
    "# subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024,shuffle=False, num_workers=0)\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(cluster_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20bc0",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af59b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\"reed98\",\"amherst41\",\"genius\",\n",
    "        \"AmazonProducts\",\n",
    "#         \"cornell5\",\"penn94\",\"johnshopkins55\",\n",
    "        \"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "        \"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "        \"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",                \n",
    "    ]\n",
    " \n",
    "    \n",
    "#     ALL_DATASETs= [\"karate\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        result_file = open(\"Results/LINKX.txt\",'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                              \n",
    "            accuracy, itr, _ = LINKXperformanceSampler(data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07532558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311cu117pyg200",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
