{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e16d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdf7a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a90e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/gilbreth/das90/Dataset/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed18bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected, sort_edge_index\n",
    "from torch_geometric.data import NeighborSampler, ClusterData, ClusterLoader, Data, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler, RandomNodeSampler\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from logger import Logger, SimpleLogger\n",
    "from dataset import load_nc_dataset, NCDataset\n",
    "from data_utils import normalize, gen_normalized_adjs, evaluate, eval_acc, eval_rocauc, to_sparse_tensor\n",
    "from parse import parse_method, parser_add_main_args\n",
    "from batch_utils import nc_dataset_to_torch_geo, torch_geo_to_nc_dataset, AdjRowLoader, make_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1286618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    \n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    ### Parse args ###\n",
    "    parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
    "    parser_add_main_args(parser)\n",
    "    parser.add_argument('--train_batch', type=str, default='cluster', help='type of mini batch loading scheme for training GNN')\n",
    "    parser.add_argument('--no_mini_batch_test', action='store_true', help='whether to test on mini batches as well')\n",
    "    parser.add_argument('--batch_size', type=int, default=10000)\n",
    "    parser.add_argument('--num_parts', type=int, default=100, help='number of partitions for partition batching')\n",
    "    parser.add_argument('--cluster_batch_size', type=int, default=1, help='number of clusters to use per cluster-gcn step')\n",
    "    parser.add_argument('--saint_num_steps', type=int, default=5, help='number of steps for graphsaint')\n",
    "    parser.add_argument('--test_num_parts', type=int, default=10, help='number of partitions for testing')\n",
    "    \n",
    "    #parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    #parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    #parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    #parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('--use_normalization', action='store_true')    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e649364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.datasets import LINKXDataset\n",
    "# from torch_geometric.nn import LINKX\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn import GCNConv, SGConv, GATConv, JumpingKnowledge, APPNP, GCN2Conv, MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import scipy.sparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d9729",
   "metadata": {},
   "source": [
    "# LINKX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc9b94",
   "metadata": {},
   "source": [
    "### Available models\n",
    "LINK, GCN, MLP, SGC, GAT, SGCMem, MultiLP, MixHop, \n",
    "\n",
    "GCNJK, GATJK, H2GCN, APPNP_Net, LINK_Concat, LINKX, GPRGNN, GCNII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc9485",
   "metadata": {},
   "source": [
    "### Available Sampler\n",
    "\n",
    "-- NeighborSampler, \n",
    "\n",
    "-- ClusterData, ClusterLoader, \n",
    "\n",
    "-- GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTRandomWalkSampler\n",
    "\n",
    "-- RandomNodeSampler\n",
    "\n",
    "### AGS-GNN: our work\n",
    "-- KNNsampler\n",
    "\n",
    "-- SubmodularSampler\n",
    "\n",
    "-- LinkSampler\n",
    "\n",
    "-- JointSampler\n",
    "\n",
    "-- DisjointEdgeSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76acea",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d57ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    model.eval()            \n",
    "    with torch.no_grad():\n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=sum(mask).item())\n",
    "            pbar.set_description(f'Evaluating {name}')\n",
    "        \n",
    "        for tg_batch in loader:\n",
    "            node_ids = tg_batch.node_ids\n",
    "            batch_dataset = torch_geo_to_nc_dataset(tg_batch, device=device)\n",
    "            out = model(batch_dataset)\n",
    "            \n",
    "            batch_test_idx = tg_batch.mask.to(torch.bool)\n",
    "            pred = out[batch_test_idx].argmax(dim=-1)\n",
    "            correct = pred.eq(batch_dataset.label[batch_test_idx])\n",
    "            \n",
    "            total_correct+=correct.sum().cpu().item()\n",
    "            total_examples+=sum(tg_batch.mask).item()            \n",
    "            \n",
    "#             pred = out[tg_batch.mask].argmax(dim=-1).cpu()\n",
    "#             correct = pred.eq(tg_batch.y[tg_batch.mask])\n",
    "\n",
    "#             total_correct+=correct.sum().item()\n",
    "#             total_examples+=sum(tg_batch.mask).item()\n",
    "    \n",
    "            if args.log_info:\n",
    "                pbar.update(sum(tg_batch.mask).item())\n",
    "    \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "    \n",
    "#     print(total_correct)\n",
    "#     print(total_examples)\n",
    "\n",
    "    return total_correct/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d390b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(y_true, y_pred):    \n",
    "    \n",
    "    y_true = y_true.detach().cpu()\n",
    "    y_pred = y_pred.argmax(dim=-1).detach().cpu()\n",
    "    \n",
    "    correct = y_pred.eq(y_true)\n",
    "    acc = correct.sum().item()/len(y_true)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63f7424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkxtest(model, data, nc_dataset, dataset, loader, num_classes):   \n",
    "    # needs a loader that includes every node in the graph\n",
    "    model.eval()\n",
    "    \n",
    "    full_out = torch.zeros(data.num_nodes, num_classes, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tg_batch in loader:\n",
    "            node_ids = tg_batch.node_ids\n",
    "            batch_dataset = torch_geo_to_nc_dataset(tg_batch, device=device)\n",
    "            out = model(batch_dataset)\n",
    "            full_out[node_ids] = out\n",
    "    \n",
    "\n",
    "    train_acc = eval_accuracy(data.y[data.train_mask], out[data.train_mask])\n",
    "    valid_acc = eval_accuracy(data.y[data.val_mask], out[data.val_mask])\n",
    "    test_acc = eval_accuracy(data.y[data.test_mask], out[data.test_mask])\n",
    "        \n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5fcc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, nc_dataset, dataset, epochs, num_classes):\n",
    "    \n",
    "    #model.reset_parameters()\n",
    "    \n",
    "    if args.rocauc or args.dataset in ('yelp-chi', 'twitch-e', 'ogbn-proteins'):\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.NLLLoss()        \n",
    "        \n",
    "    if args.log_info:    \n",
    "        print('making train loader')    \n",
    "    \n",
    "    train_idx = torch.nonzero(data.train_mask).squeeze()\n",
    "    val_idx = torch.nonzero(data.val_mask).squeeze()\n",
    "    test_idx = torch.nonzero(data.test_mask).squeeze()\n",
    "    \n",
    "    worker = 0\n",
    "    if data.num_nodes>100000:\n",
    "        worker=8\n",
    "    \n",
    "    train_loader = make_loader(args, nc_dataset, train_idx, mini_batch = True, num_workers=worker)\n",
    "    t_loader = make_loader(args, nc_dataset, train_idx, mini_batch = True, test=True, num_workers=worker)\n",
    "    val_loader = make_loader(args, nc_dataset, val_idx, mini_batch = True, test=True, num_workers=worker)\n",
    "    test_loader = make_loader(args, nc_dataset, test_idx, mini_batch = True, test=True, num_workers=worker)\n",
    "    \n",
    "    model.reset_parameters()\n",
    "    if args.adam:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(train_loader)\n",
    "        print(val_loader)\n",
    "    \n",
    "    \n",
    "    train_losses=[]\n",
    "    best_acc = 0 \n",
    "    num_iteration = epochs    \n",
    "    print_once = True\n",
    "    \n",
    "    val_accuracies = []; train_accuracies = []; test_accuracies = [];training_times = [];\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_examples = 0\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=len(train_loader))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        for i in range(args.num_parts):\n",
    "            for tg_batch in train_loader:\n",
    "\n",
    "    #             if args.log_info and print_once:\n",
    "    #                 print_once = False\n",
    "    #                 print(tg_batch)\n",
    "    #                 print(tg_batch.mask)\n",
    "    #             print(tg_batch.mask.sum())\n",
    "\n",
    "                if int(tg_batch.mask.sum().item()) == 0:\n",
    "                    continue\n",
    "\n",
    "                batch_train_idx = tg_batch.mask.to(torch.bool)\n",
    "                batch_dataset = torch_geo_to_nc_dataset(tg_batch, device=device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = model(batch_dataset)\n",
    "                if args.rocauc or args.dataset in ('yelp-chi', 'twitch-e', 'ogbn-proteins'):\n",
    "                    if dataset.label.shape[1] == 1:\n",
    "                        # change -1 instances to 0 for one-hot transform\n",
    "                        # dataset.label[dataset.label==-1] = 0\n",
    "                        true_label = F.one_hot(batch_dataset.label, batch_dataset.label.max() + 1).squeeze(1)\n",
    "                    else:\n",
    "                        true_label = batch_dataset.label\n",
    "\n",
    "                    loss = criterion(out[batch_train_idx], true_label[batch_train_idx].to(out.dtype))\n",
    "                else:                                            \n",
    "    #                 loss = F.cross_entropy(out[batch_train_idx], batch_dataset.label[batch_train_idx])\n",
    "\n",
    "                    out = F.log_softmax(out, dim=1)\n",
    "    #                 #loss = criterion(out[batch_train_idx], batch_dataset.label.squeeze(1)[batch_train_idx])\n",
    "                    loss = criterion(out[batch_train_idx], batch_dataset.label[batch_train_idx])\n",
    "    #                 loss = F.nll_loss(out[batch_train_idx], batch_dataset.label[batch_train_idx])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss+=loss.item()\n",
    "                num_examples+= len(batch_train_idx)\n",
    "        \n",
    "                if args.log_info:\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close() \n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        training_times.append(epoch_end-epoch_start)\n",
    "        \n",
    "        \n",
    "#         train_acc, val_acc, test_acc = linkxtest(model, data, nc_dataset, dataset, val_loader, num_classes)\n",
    "        \n",
    "        train_acc=0; val_acc=0; test_acc = 0;loss = 0;\n",
    "        if epoch%10 == 0:\n",
    "            \n",
    "            if num_examples == 0:\n",
    "                print(\"Something went wrong, training examples cannot be zero\")\n",
    "                continue\n",
    "            else:\n",
    "                loss = total_loss/num_examples\n",
    "            \n",
    "            train_losses.append(loss)\n",
    "                \n",
    "            if args.log_info:   \n",
    "                train_acc=test(model, t_loader,data.train_mask,'Train')\n",
    "                val_acc = test(model, val_loader, data.val_mask,'Validation')            \n",
    "            \n",
    "            test_acc = test(model, test_loader, data.test_mask,'Test')                \n",
    "\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "            if test_acc>best_acc:\n",
    "                best_acc=test_acc\n",
    "\n",
    "            std_dev = np.std(train_losses[-5:])        \n",
    "\n",
    "            if args.log_info:\n",
    "                print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "    #         if epoch>=5 and std_dev<=1e-4:\n",
    "    #             num_iteration = epoch\n",
    "\n",
    "    #             if args.log_info:                \n",
    "    #                 print(\"Iteration for convergence: \", epoch)\n",
    "    #             break\n",
    "            \n",
    "    if args.log_info:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='../Results/AGSGSValidation', yname='Accuracy', xname='Epoch')\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "    \n",
    "    acc_file = open(\"../Runtime/LINKX.txt\",'a+') \n",
    "    acc_file.write(str(train_losses))\n",
    "    acc_file.write(str(train_accuracies))\n",
    "    acc_file.write(str(val_accuracies))\n",
    "    acc_file.write(str(test_accuracies))\n",
    "    acc_file.write(str(training_times))\n",
    "    acc_file.write(str(np.mean(training_times)))\n",
    "    acc_file.write(f'\\nworker {worker:1d} avg epoch runtime {np.mean(training_times):0.8f}')\n",
    "    acc_file.close()     \n",
    "                \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52fdabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AllperformanceSampler(data, dataset, num_classes, epochs=1, train_neighbors=[8,4], test_neighbors=[8,4]):        \n",
    "    \n",
    "    n = data.num_nodes\n",
    "    c = num_classes\n",
    "    d = data.x.shape[1]\n",
    "        \n",
    "    nc_dataset = torch_geo_to_nc_dataset(data)    \n",
    "    model = parse_method(args, nc_dataset, n, c, d, device)\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(model) \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, nc_dataset, dataset, epochs, num_classes)        \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3dae9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['link', 'gcn', 'mlp', 'cs', 'sgc', \n",
    "           'gprgnn', 'appnp', 'gat', 'lp', \n",
    "           'mixhop','gcnjk','gatjk','h2gcn',\n",
    "           'link_concat','linkx','gcn2']\n",
    "\n",
    "#others = ['gsage','gsaint','acmgcn','clustergcn','gcn','gin','gat','linkx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddaf802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_methods = ['cluster', 'graphsaint-node', 'graphsaint-edge', 'graphsaint-rw',\n",
    "            'random','full-batch', 'row']\n",
    "\n",
    "# train_batch cluster\n",
    "# batch_size 10000\n",
    "# num_parts 100\n",
    "# cluster_batch_size 1\n",
    "# saint_num_steps 5\n",
    "# test_num_parts 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ddbe364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/bin/bash\n",
    "\n",
    "# dataset=$1\n",
    "# sub_dataset=${2:-''}\n",
    "\n",
    "# hidden_channels_lst=(16 32 128 256)\n",
    "# num_layers_lst=(1 2 3)\n",
    "\n",
    "\n",
    "# for num_layers in \"${num_layers_lst[@]}\"; do\n",
    "#     for hidden_channels in \"${hidden_channels_lst[@]}\"; do\n",
    "#         if [ \"$dataset\" = \"snap-patents\" ] || [ \"$dataset\" = \"arxiv-year\" ]; then\n",
    "#             echo \"Running $dataset \"\n",
    "#             python main_scalable.py --dataset $dataset --sub_dataset ${sub_dataset:-''} --method linkx  --num_layers $num_layers --hidden_channels $hidden_channels --display_step 25 --runs 5 --directed --train_batch row --num_parts 10\n",
    "#         else\n",
    "#             python main_scalable.py --dataset $dataset --sub_dataset ${sub_dataset:-''} --method linkx  --num_layers $num_layers --hidden_channels $hidden_channels --display_step 25 --runs 5 --train_batch row --num_parts 10\n",
    "#         fi\n",
    "#     done\n",
    "# done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf83ec9",
   "metadata": {},
   "source": [
    "## LINKX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7113ff79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# args.log_info = True\n",
    "# DATASET_NAME = 'Reddit0.525'\n",
    "# args.dataset = DATASET_NAME\n",
    "# gnn_name = 'linkx'\n",
    "\n",
    "# args.method = gnn_name\n",
    "# args.train_batch = 'row'\n",
    "# args.num_parts = 10\n",
    "\n",
    "# data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "# print(data)\n",
    "\n",
    "# # best_acc, num_iteration, _ = AllperformanceSampler(data, dataset, dataset.num_classes, epochs=50)\n",
    "# # print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81f54",
   "metadata": {},
   "source": [
    "## Other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93d39e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 \n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34], val_mask=[34], test_mask=[34])\n"
     ]
    }
   ],
   "source": [
    "args.log_info = True\n",
    "DATASET_NAME = 'karate'\n",
    "args.dataset = DATASET_NAME\n",
    "gnn_name = 'link'\n",
    "\n",
    "args.method = gnn_name\n",
    "args.train_batch = 'full-batch'\n",
    "\n",
    "data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "print(data)\n",
    "\n",
    "# best_acc, num_iteration, _ = AllperformanceSampler(data, dataset, dataset.num_classes, epochs=50)\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20bc0",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af59b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\"reed98\",\"amherst41\",\n",
    "#         \"genius\",\n",
    "#         \"AmazonProducts\",\n",
    "#         \"cornell5\",\n",
    "#         \"penn94\",\n",
    "#         \"johnshopkins55\",\n",
    "#         \"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "#         \"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "#         \"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",     \n",
    "#         \"Cora\",\n",
    "#         \"Reddit\",\"genius\",\"Yelp\",\n",
    "#         'pokec','twitch-gamer',\n",
    "#         'wiki', \n",
    "#         'arxiv-year','snap-patents'\n",
    "    ]\n",
    " \n",
    "    \n",
    "    #ALL_DATASETs= [\"karate\"]\n",
    "    ALL_DATASETs= [\"Reddit0.525\",\"Reddit0.425\",\"Reddit0.325\"]\n",
    "    \n",
    "    gnn_name = 'linkx'\n",
    "\n",
    "    args.method = gnn_name\n",
    "    args.train_batch = 'row'\n",
    "    args.num_parts = 10\n",
    "    \n",
    "    \n",
    "    result_file = open(\"../Results/LINKXscale.txt\",'a+')        \n",
    "    result_file.write(f'{gnn_name} ')\n",
    "    result_file.write(f'{args.train_batch} ')\n",
    "    result_file.write(f'{args.num_parts} ')\n",
    "    result_file.close()\n",
    "    \n",
    "    \n",
    "    runtime_filename = \"../Runtime/LINKX.txt\"\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs: \n",
    "        \n",
    "#         if DATASET_NAME in ['arxiv-year','snap-patents']:\n",
    "#             args.directed = True     \n",
    "            \n",
    "        if DATASET_NAME in ['wiki','snap-patents','AmazonProducts']:\n",
    "            args.num_parts = 50\n",
    "        else:\n",
    "            args.num_parts = 10\n",
    "        \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        args.dataset = DATASET_NAME\n",
    "        \n",
    "        result_file = open(\"../Results/LINKXscale.txt\",'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        \n",
    "        acc_file = open(runtime_filename,'a+') \n",
    "        acc_file.write(f'{DATASET_NAME}\\n')\n",
    "        acc_file.close()     \n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state=5)   \n",
    "            \n",
    "#             if data.num_nodes<100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            \n",
    "            max_epochs = 50\n",
    "                              \n",
    "            accuracy, itr, _ = AllperformanceSampler(data, dataset, num_classes, epochs=max_epochs)\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7a03e",
   "metadata": {},
   "source": [
    "## Batch processing of all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07532558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "            \"Cornell\",\n",
    "            \"Texas\",\n",
    "            \"Wisconsin\",\n",
    "            \"reed98\",\n",
    "            \"amherst41\",\n",
    "            \"penn94\",\n",
    "            \"Roman-empire\",\n",
    "            \"cornell5\",\n",
    "            \"Squirrel\",\n",
    "            \"johnshopkins55\",\n",
    "            \"Actor\",\n",
    "            \"Minesweeper\",\n",
    "            \"Questions\",\n",
    "            \"Chameleon\",\n",
    "            \"Tolokers\",\n",
    "            \"Flickr\",\n",
    "            \"Amazon-ratings\",\n",
    "    ]\n",
    " \n",
    "    #'lp' error\n",
    "    #'h2gcn'\n",
    "    methods = [\n",
    "        'link', 'mlp', 'cs', 'sgc', 'gprgnn', 'appnp',\n",
    "        'mixhop','gcnjk','gatjk',\n",
    "        'link_concat','linkx','gcn2']\n",
    "    \n",
    "    \n",
    "#     ALL_DATASETs= [\"karate\"]\n",
    "#     methods=['linkx']\n",
    "    \n",
    "    \n",
    "    for gnn_name in methods:\n",
    "        #gnn_name = 'linkx'\n",
    "        \n",
    "        print(gnn_name)\n",
    "        print(\"-\"*100)\n",
    "\n",
    "        args.method = gnn_name\n",
    "        args.train_batch = 'full-batch'\n",
    "    \n",
    "    \n",
    "        filename = \"../Results/\"+gnn_name+\"scale.txt\"\n",
    "\n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{gnn_name} ')\n",
    "        result_file.write(f'{args.train_batch} ')\n",
    "        result_file.write(f'{args.num_parts} ')\n",
    "        result_file.close()\n",
    "\n",
    "\n",
    "        runtime_filename = \"../Runtime/\"+gnn_name+\".txt\"\n",
    "\n",
    "        args.log_info = False\n",
    "\n",
    "        for DATASET_NAME in ALL_DATASETs: \n",
    "\n",
    "    #         if DATASET_NAME in ['arxiv-year','snap-patents']:\n",
    "    #             args.directed = True     \n",
    "\n",
    "            if DATASET_NAME in ['wiki','snap-patents','AmazonProducts']:\n",
    "                args.num_parts = 50\n",
    "            else:\n",
    "                args.num_parts = 10\n",
    "\n",
    "            print(DATASET_NAME, end=' ')\n",
    "\n",
    "            args.dataset = DATASET_NAME\n",
    "\n",
    "            result_file = open(filename,'a+')        \n",
    "            result_file.write(f'{DATASET_NAME} ')\n",
    "\n",
    "            acc_file = open(runtime_filename,'a+') \n",
    "            acc_file.write(f'{DATASET_NAME}\\n')\n",
    "            acc_file.close()     \n",
    "\n",
    "            accs = []\n",
    "            itrs = []\n",
    "\n",
    "            for i in range(num_run):\n",
    "                data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state=5)   \n",
    "\n",
    "    #             if data.num_nodes<100000:\n",
    "    #                 accs.append(-1)\n",
    "    #                 itrs.append(-1)\n",
    "    #                 break\n",
    "\n",
    "                if len(data.y.shape) > 1:\n",
    "                    data.y = data.y.argmax(dim=1)        \n",
    "                    num_classes = torch.max(data.y).item()+1\n",
    "                else:\n",
    "                    num_classes = dataset.num_classes\n",
    "\n",
    "                if num_classes!= torch.max(data.y)+1:\n",
    "                    num_classes = torch.max(data.y).item()+1\n",
    "\n",
    "                max_epochs = 150\n",
    "\n",
    "                accuracy, itr, _ = AllperformanceSampler(data, dataset, num_classes, epochs=max_epochs)\n",
    "\n",
    "                accs.append(accuracy)\n",
    "                itrs.append(itr)\n",
    "                #print(itr, accuracy)\n",
    "\n",
    "            #print(accs, itrs)\n",
    "            print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "            result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "            result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311cu117pyg200",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
