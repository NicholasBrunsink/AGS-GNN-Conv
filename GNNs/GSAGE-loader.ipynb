{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44415f5a",
   "metadata": {},
   "source": [
    "## Get Cuda and Processor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ac319c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Submodular')\n",
    "\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "782d8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d11e7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d0c7f",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38ae0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f2deccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f832",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbd4df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, ChebConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from ipynb.fs.full.SpatialConv import SpatialConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007e8b8",
   "metadata": {},
   "source": [
    "### GNN option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5744498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNNconv = SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd0e048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSGSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "             \n",
    "        self.conv1 = GNNconv(num_features, hidden_channels)\n",
    "        #self.conv2 = GNNconv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = GNNconv(hidden_channels,num_classes)\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        x = batch_data.x\n",
    "        edge_index = batch_data.edge_index\n",
    "        edge_weight = batch_data.edge_weight\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b17b8e",
   "metadata": {},
   "source": [
    "## GNN Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e11e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "# from ipynb.fs.full.a1AGS_Node_Sampler_Fast import WeightedNeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ac415e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):    \n",
    "    if args.log_info:    \n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()    \n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_data.edge_weight=None\n",
    "            used = batch_data.batch_size\n",
    "            \n",
    "            out = model(batch_data)\n",
    "                   \n",
    "            out=out[:used,:]\n",
    "            pred = out.argmax(dim=1)            \n",
    "\n",
    "            y_true.append(batch_data.y[:used].detach().cpu().numpy())\n",
    "            y_pred.append(pred.detach().cpu().numpy())\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(used)\n",
    "              \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    #acc = f1_score(y_true, y_pred, average='micro')\n",
    "                    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85fa0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(DATASET_NAME, model, data, epochs=100, train_neighbors=[-1,10], test_neighbors=[-1,10]):\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "        \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    if data.y.ndim == 1:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    batch_size=4096         \n",
    "    worker = 0\n",
    "    \n",
    "    if data.num_nodes>=100000:\n",
    "        worker = 8\n",
    "    \n",
    "#     if data.num_nodes>=10000:\n",
    "#         worker = min(8,int(sum(data.train_mask)/batch_size))\n",
    "        \n",
    "    if args.log_info:\n",
    "        print(\"Worker: \", worker)\n",
    "        \n",
    "# #     weight_func=['knn','submodular']; \n",
    "#     weight_func=['knn']; \n",
    "# #     weight_func=['random', 'random'];  worker = 0;\n",
    "# #     weight_func=['link-nn', 'link-sub'];  worker = 2;\n",
    "#     params={\n",
    "#         'knn':{'metric':'cosine'},\n",
    "#         'submodular':{'metric':'cosine'},\n",
    "#         'link-nn':{'value':'min'},\n",
    "#         'link-sub':{'value':'max'},\n",
    "#     }    \n",
    "    \n",
    "#     sampler_dir = DIR+'AGSGSAGE/'+DATASET_NAME\n",
    "# #     if not os.path.exists(sampler_dir):\n",
    "# #         os.makedirs(sampler_dir)\n",
    "    \n",
    "#     start = time.time()    \n",
    "#     loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info,\n",
    "#                                     directed=True, replace = False,\n",
    "#                                     save_dir = sampler_dir,recompute = False)\n",
    "\n",
    "#     train_loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info,\n",
    "#                                           directed=True, replace = False,\n",
    "#                                           save_dir = sampler_dir,recompute = False)\n",
    "    \n",
    "#     val_loader = WeightedNeighborLoader(data, input_nodes=data.val_mask,num_neighbors=test_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=False, num_workers=min(8,int(sum(data.val_mask)/batch_size)), drop_last=False, \n",
    "#                               weight_func=weight_func, params=params,log=args.log_info, directed=True, replace = False,\n",
    "#                                         save_dir = sampler_dir,recompute = False)\n",
    "    \n",
    "#     test_loader = WeightedNeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=False, num_workers=min(8,int(sum(data.test_mask)/batch_size)), drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info, directed=True, replace = False,\n",
    "#                                          save_dir = sampler_dir,recompute = False)\n",
    "    \n",
    "    start = time.time()    \n",
    "    loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False, \n",
    "                              directed=True, replace = False)\n",
    "\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                              batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "                              directed=True, replace = False)\n",
    "    \n",
    "    val_loader =  NeighborLoader(data, input_nodes=data.val_mask,num_neighbors=test_neighbors, \n",
    "                              batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "                              directed=True, replace = False)\n",
    "    \n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "                              batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "                              directed=True, replace = False)\n",
    "    \n",
    "    \n",
    "    top_k_accs = []    \n",
    "    best_acc=0  \n",
    "    \n",
    "    train_losses=[]\n",
    "    val_accuracies=[]\n",
    "    train_accuracies=[]\n",
    "    test_accuracies=[]\n",
    "    training_times = []\n",
    "    \n",
    "    num_iteration = epochs\n",
    "    \n",
    "    end = time.time()\n",
    "    if args.log_info:\n",
    "        print(\"Total initialization time: \", end-start)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=int(sum(data.train_mask)))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for i,batch_data in enumerate(loader):            \n",
    "            #print(batch_data)\n",
    "            \n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_data.edge_weight=None\n",
    "            used = batch_data.batch_size #int(sum(batch_data.train_mask))       \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch_data)\n",
    "            #out = F.log_softmax(out, dim=1)                 \n",
    "            #loss = F.nll_loss(out[batch_data[0].train_mask], batch_data[0].y[batch_data[0].train_mask])\n",
    "            #loss = F.cross_entropy(out[:used], batch_data[0].y[:used])\n",
    "            loss = criterion(out[:used], batch_data.y[:used])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            total_loss += loss.item() * used\n",
    "            total_examples += used\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(used)\n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "            \n",
    "        epoch_end = time.time()\n",
    "        training_times.append(epoch_end-epoch_start)\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            loss=total_loss / total_examples\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            #print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}', end = ', ')                \n",
    "\n",
    "            if args.log_info:\n",
    "                train_acc=test(model, train_loader,data.train_mask,'Train')            \n",
    "                train_accuracies.append(train_acc.item())        \n",
    "            else:\n",
    "                train_acc = 0 ; train_accuracies.append(train_acc)\n",
    "\n",
    "            if args.log_info:\n",
    "                val_acc = test(model, val_loader,data.val_mask,'Validation')\n",
    "                val_accuracies.append(val_acc.item())\n",
    "            else:\n",
    "                val_acc = 0 ; val_accuracies.append(val_acc)\n",
    "\n",
    "            test_acc = test(model, test_loader,data.test_mask,'Test')\n",
    "            test_accuracies.append(test_acc.item())\n",
    "            #print(f'Epoch: {epoch:03d}, Test: {test_acc:.4f}')\n",
    "\n",
    "            std_dev = np.std(train_losses[-5:])\n",
    "            #print(f'Epoch: {epoch:03d}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "            if args.log_info:\n",
    "                print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "    #         if epoch>=5 and std_dev<=1e-3:\n",
    "    #             num_iteration = epoch\n",
    "\n",
    "    #             if args.log_info:                \n",
    "    #                 print(\"Iteration for convergence: \", epoch)\n",
    "    #             break\n",
    "\n",
    "        if args.log_info:\n",
    "            #save_plot([val_accuracies], labels=['Validation'], name='Plots/Validation', yname='Accuracy', xname='Epoch')    \n",
    "            save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Results/AGSNSVal', yname='Accuracy', xname='Epoch')\n",
    "\n",
    "            print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "            print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "        \n",
    "    best_acc = max(test_accuracies)\n",
    "    \n",
    "    end = time.time()\n",
    "    if args.log_info:\n",
    "        print(\"Total epoch time: \", end-start) \n",
    "        \n",
    "    \n",
    "    acc_file = open(\"Runtime/GSAGE_loader.txt\",'a+') \n",
    "    acc_file.write(str(train_losses))\n",
    "    acc_file.write(str(train_accuracies))\n",
    "    acc_file.write(str(val_accuracies))\n",
    "    acc_file.write(str(test_accuracies))\n",
    "    acc_file.write(str(training_times))\n",
    "    acc_file.write(str(np.mean(training_times)))\n",
    "    acc_file.write(f'\\nworker {worker:1d} avg epoch runtime {np.mean(training_times):0.8f}')\n",
    "    acc_file.close()     \n",
    "    \n",
    "    \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34bee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=1, train_neighbors=[-1,-1], test_neighbors=[-1,-1]):        \n",
    "    \n",
    "    model = AGSGSAGE(data.x.shape[1], num_classes, hidden_channels=256).to(device)\n",
    "    \n",
    "    if args.log_info: print(model)    \n",
    "    \n",
    "    best_acc, num_iteration = train(DATASET_NAME, model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)    \n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9140fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_feature(data):    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    edges = data.edge_index.t()\n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    return adj_mat\n",
    "\n",
    "# adj_feature(data)\n",
    "# data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "586728b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# args.log_info = True\n",
    "\n",
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME,DIR=None, log=False, h_score=True, split_no=0)\n",
    "# print(data)\n",
    "\n",
    "# # (row, col) = data.edge_index\n",
    "# # data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "# # data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "# # print(data)\n",
    "\n",
    "# # if DATASET_NAME in ['Squirrel', 'Chameleon']:\n",
    "# #     data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "# #     if args.log_info == True:\n",
    "# #         print(data.x.shape)\n",
    "    \n",
    "# best_acc, num_iteration, _ =  AGSNSperformanceSampler(DATASET_NAME, data, dataset, dataset.num_classes, epochs=150, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f89d3",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7a2677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit2 acc 0.8894 sd 0.0032 itr 50 sd 0\n"
     ]
    }
   ],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\n",
    "#         \"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\n",
    "#         \"reed98\",\n",
    "#         \"amherst41\",\n",
    "#         \"genius\",\n",
    "#         \"AmazonProducts\",\n",
    "#         \"cornell5\",\n",
    "#         \"penn94\",\"johnshopkins55\",\n",
    "#         \"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "#         \"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "        \"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",\n",
    "#         \"Squirrel\",\"Chameleon\",\n",
    "#         \"Cora\",\"Reddit\",\"genius\",\"Yelp\",\n",
    "#         'pokec','arxiv-year',\n",
    "#         'snap-patents','twitch-gamer',\n",
    "#         'wiki'\n",
    "    ]\n",
    "    \n",
    "#     ALL_DATASETs= [\"karate\"]\n",
    "\n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/GSAGE_loader.txt\"\n",
    "    runtime_filename = \"Runtime/GSAGE_loader.txt\"\n",
    "        \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')    \n",
    "        \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "        \n",
    "        acc_file = open(runtime_filename,'a+') \n",
    "        acc_file.write(f'{DATASET_NAME}\\n')\n",
    "        acc_file.close()     \n",
    "\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state=i)   \n",
    "            \n",
    "#             #optional for making undirected graph\n",
    "#             (row, col) = data.edge_index\n",
    "#             data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "#             data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon', \n",
    "                                #'cornell5','penn94','johnshopkins55'\n",
    "                               ]:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "                              \n",
    "            accuracy, itr, _ = AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cab8c",
   "metadata": {},
   "source": [
    "## View Learned Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "361d4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':    \n",
    "    \n",
    "#     n=7\n",
    "#     x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "#     y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "#     edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "#     edge_index = edge_index-1\n",
    "    \n",
    "#     mask = torch.zeros(n, dtype=torch.bool)\n",
    "#     mask[[1,3]] = True\n",
    "    \n",
    "#     test_data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = mask, val_mask = mask)    \n",
    "#     print(test_data)\n",
    "    \n",
    "    \n",
    "#     None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83ca9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48f76e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# #X = model(data.x.to(device),data.edge_index.to(device), data.weight.to(device))\n",
    "# X = model(data.x.to(device),data.edge_index.to(device))\n",
    "# X = X.detach().to('cpu')\n",
    "# y = data.y.to('cpu')\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1e750fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Create a t-SNE model with 2 components and a perplexity of 30\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=42, learning_rate='auto', init='random')\n",
    "\n",
    "# # Fit and transform the data to the 2D t-SNE space\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# # Plot the data in the 2D t-SNE space, colored by class\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f746ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ca0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c2712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
