{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44415f5a",
   "metadata": {},
   "source": [
    "# AGS-GNN Graph Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac319c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Submodular')\n",
    "\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782d8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11e7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--recompute', type=bool, default=False)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ae0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f832",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd4df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, ChebConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ipynb.fs.full.SpatialConv import SpatialConv\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb2011",
   "metadata": {},
   "source": [
    "## Homophilic GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2402dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomophilicNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels, end_hidden):\n",
    "        super().__init__()\n",
    "        in_channels = num_features\n",
    "        out_channels = num_classes\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "#         self.lin = torch.nn.Linear(3 * hidden_channels, end_hidden)\n",
    "#         self.lin2 = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, end_hidden)\n",
    "        self.lin2 = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
    "\n",
    "\n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "#         self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "#         x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "#         x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "#         x = torch.cat([x1, x2, x3], dim=-1)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        \n",
    "        c1 = self.lin(x)\n",
    "        c2 = self.lin2(x)\n",
    "        \n",
    "        return F.relu(c1), c2.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ef962",
   "metadata": {},
   "source": [
    "## Combination Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24c3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=256, dropout=0.5, N = 0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        hidden = int(hidden_channels/2)        \n",
    "        self.gnn1 = HomophilicNet(num_features, num_classes, hidden_channels, hidden)\n",
    "        self.gnn2 = HomophilicNet(num_features, num_classes, hidden_channels, hidden)\n",
    "        self.p = dropout\n",
    "        self.com_lin = nn.Linear(hidden*2, num_classes)      \n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "        #out = model(batch_data.x, batch_data.edge_index, batch_data.edge_weight)\n",
    "        #out = model(batch_data.x, batch_data.edge_index)\n",
    "        \n",
    "        x1, x1c1 = self.gnn1(batch_data[0].x, batch_data[0].edge_index, batch_data[0].weight)\n",
    "        x2, x2c2 = self.gnn2(batch_data[1].x, batch_data[1].edge_index, batch_data[1].weight)\n",
    "\n",
    "#         x1 = self.gnn1(batch_data[0].x, batch_data[0].edge_index)\n",
    "#         x2 = self.gnn2(batch_data[1].x, batch_data[1].edge_index)\n",
    "\n",
    "        a1 = F.relu(x1)\n",
    "        a1 = F.dropout(a1, p=self.p, training=self.training)\n",
    "        \n",
    "        s1 = F.relu(x2)        \n",
    "        s1 = F.dropout(s1, p=self.p, training=self.training)\n",
    "        \n",
    "        batch_size = batch_data[0].batch_size        \n",
    "        x = torch.cat([a1[:batch_size,:], s1[:batch_size,:]], dim=-1)\n",
    "        x = self.com_lin(x)\n",
    "        \n",
    "        #return x\n",
    "    \n",
    "        return x.log_softmax(dim=-1), x1c1, x2c2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b17b8e",
   "metadata": {},
   "source": [
    "## GNN Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.AGSGraphSampler import AGSGraphSampler\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83409b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def prediction(y_pred_seed, y_pred_hm, y_pred_ht):\n",
    "    \n",
    "    all_tensors = [y_pred_seed, y_pred_hm, y_pred_ht]\n",
    "    final_predictions = []\n",
    "\n",
    "    for i in range(len(y_pred_seed)):\n",
    "        values_at_index = [tensor[i].item() for tensor in all_tensors]\n",
    "        counter = Counter(values_at_index)\n",
    "        most_common_values = counter.most_common()\n",
    "\n",
    "        # Check if there's a tie\n",
    "        if len(most_common_values) > 1 and most_common_values[0][1] == most_common_values[1][1]:\n",
    "            #selected_value = random.choice([value for value, count in most_common_values[:2]])\n",
    "            selected_value = y_pred_seed[i].item()\n",
    "        else:\n",
    "            selected_value = most_common_values[0][0]\n",
    "\n",
    "        final_predictions.append(selected_value)\n",
    "            \n",
    "    return torch.LongTensor(final_predictions)\n",
    "\n",
    "y_pred_seed = torch.tensor([0, 1, 2, 2, 1])\n",
    "y_pred_hm = torch.tensor([2, 1, 1, 0, 2])\n",
    "y_pred_ht = torch.tensor([0, 2, 1, 1, 0])\n",
    "\n",
    "\n",
    "# prediction(y_pred_seed,y_pred_hm,y_pred_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac415e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train', channel='all'):\n",
    "    if args.log_info:\n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "\n",
    "    model.eval()\n",
    "    model.gnn1.set_aggr('add' if args.use_normalization else 'mean')\n",
    "    model.gnn2.set_aggr('add' if args.use_normalization else 'mean')\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            batch_data = [batch_data,batch_data]\n",
    "            \n",
    "            batch_data = [b_data.to(device) for b_data in batch_data]\n",
    "            batch_size = batch_data[0].batch_size\n",
    "            \n",
    "            \n",
    "#             if args.use_normalization:\n",
    "#                 batch_data[0].weight = batch_data[0].edge_norm * batch_data[0].edge_weight\n",
    "#                 batch_data[1].weight = batch_data[1].edge_norm * batch_data[1].edge_weight\n",
    "#             else:\n",
    "#                 batch_data[0].weight = batch_data[0].edge_weight\n",
    "#                 batch_data[1].weight = batch_data[1].edge_weight\n",
    "            \n",
    "            batch_data[0].weight = batch_data[0].edge_weight\n",
    "            batch_data[1].weight = batch_data[1].edge_weight\n",
    "        \n",
    "            \n",
    "            out, out1, out2 = model(batch_data)\n",
    "                \n",
    "            #print(out.shape, out1.shape, out2.shape)\n",
    "            \n",
    "            y_pred_seed = out[:batch_size].argmax(dim=-1).cpu()\n",
    "            y_pred_hm = out1[:batch_size].argmax(dim=-1).cpu()\n",
    "            y_pred_ht = out2[:batch_size].argmax(dim=-1).cpu()\n",
    "            \n",
    "            #print(y_pred_seed.shape,y_pred_hm.shape, y_pred_ht.shape)\n",
    "            \n",
    "            \n",
    "            y_true = batch_data[0].y[:batch_size]\n",
    "            \n",
    "            if name == 'Train':\n",
    "                t_mask = batch_data[0].train_mask[:batch_size]\n",
    "            elif name == 'Validation':\n",
    "                t_mask = batch_data[0].val_mask[:batch_size]\n",
    "            else:\n",
    "                t_mask = batch_data[0].test_mask[:batch_size]\n",
    "\n",
    "            if channel=='sd':\n",
    "                y_pred = y_pred_seed\n",
    "            elif channel=='hm':\n",
    "                y_pred = y_pred_hm\n",
    "            elif channel=='ht':\n",
    "                y_pred = y_pred_ht\n",
    "            else:\n",
    "                y_pred = prediction(y_pred_seed, y_pred_hm, y_pred_ht)\n",
    "\n",
    "            #print(y_pred)                \n",
    "            correct = y_pred.eq(y_true.cpu())\n",
    "            total_correct+= correct[t_mask].sum().item()\n",
    "            items = t_mask.sum().item()\n",
    "            total_examples+= items           \n",
    "            if args.log_info:\n",
    "                pbar.update(items)\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "\n",
    "    return total_correct/total_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85fa0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/GraphSAINT/GraphSAINT/issues/11\n",
    "    \n",
    "def train(DATASET_NAME,model, data, dataset, epochs=1, channel='all', BATCH_SIZE=1024): #'all', 'hm', 'ht', 'sd'\n",
    "        \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    \n",
    "    optimizer = torch.optim.Adam(model.parameters())    \n",
    "    \n",
    "    if data.y.ndim == 1:\n",
    "        criterion = torch.nn.CrossEntropyLoss() #regular logits as output\n",
    "#         criterion = torch.nn.NLLLoss() ## if log softmax used as activation\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()     #multillabel\n",
    "    \n",
    "    row, col = data.edge_index\n",
    "    data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
    "    \n",
    "    #minibatch_size= 2048\n",
    "    minibatch_size = BATCH_SIZE\n",
    "    \n",
    "    sampler_dir = DIR+'AGSGSAINTII/'+DATASET_NAME\n",
    "    if not os.path.exists(sampler_dir):\n",
    "        os.makedirs(sampler_dir)\n",
    "        \n",
    "    #batch_size= min(1024, data.num_nodes)\n",
    "    #num_steps=math.ceil(sum (data.train_mask)/batch_size)    \n",
    "    num_steps=math.ceil(data.num_nodes/minibatch_size)\n",
    "    num_workers = 8  if data.num_nodes>50000 else 0\n",
    "    \n",
    "    \n",
    "    sample_func =['wrw', 'wrw']\n",
    "    weight_func =[\n",
    "        {'exact':False,'weight':'fastlink'}, #exact for exact size to the batch\n",
    "        {'exact':False,'weight':'fastlink'}\n",
    "    ]\n",
    "\n",
    "    params={'knn':{'metric':'cosine'},\n",
    "            'submodular':{'metric':'cosine'},\n",
    "            'link-nn':{'value':'min'},\n",
    "            'link-sub':{'value':'max'},\n",
    "            'disjoint':{'value':'mst'},\n",
    "           }\n",
    "    \n",
    "    loader = AGSGraphSampler(\n",
    "        data, batch_size=minibatch_size, walk_length=2, num_steps=num_steps, sample_coverage=100,\n",
    "        num_workers=num_workers,log=args.log_info,save_dir=sampler_dir,recompute = args.recompute, shuffle = False,\n",
    "        sample_func = sample_func, weight_func=weight_func, params=params)\n",
    "        \n",
    "    \n",
    "    #for evaluation\n",
    "    train_num_steps = int(torch.ceil(sum(data.train_mask)/minibatch_size))\n",
    "    val_num_steps = int(torch.ceil(sum(data.val_mask)/minibatch_size))\n",
    "    test_num_steps = int(torch.ceil(sum(data.test_mask)/minibatch_size))\n",
    " \n",
    "    \n",
    "    sample_batch_size=2048\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=[8,4], \n",
    "                            batch_size=sample_batch_size, shuffle=False, num_workers=num_workers)\n",
    "    val_loader = NeighborLoader(data,input_nodes=data.val_mask,num_neighbors=[8,4], \n",
    "                                batch_size=sample_batch_size,shuffle=False, num_workers=num_workers)\n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=[8,4], \n",
    "                                 batch_size=sample_batch_size,shuffle=False, num_workers=num_workers)\n",
    "\n",
    "            \n",
    "    best_acc=0    \n",
    "    train_losses = []; val_accuracies = []; train_accuracies = []; test_accuracies = []    \n",
    "    max_iteration = epochs    \n",
    "    \n",
    "    \n",
    "    th_node = 10000\n",
    "    \n",
    "    if data.num_nodes<th_node:\n",
    "        test_data = data.clone()\n",
    "        test_data.weight = loader.edge_norm * test_data.edge_weight\n",
    "        test_data.seed_node=torch.arange(test_data.num_nodes)  \n",
    "        test_data.batch_size=test_data.seed_node.shape[0]\n",
    "        all_data = [test_data.to(device), test_data.to(device)]    \n",
    "        \n",
    "        if args.log_info:\n",
    "            print(all_data)\n",
    "            \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=num_steps)\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        model.gnn1.set_aggr('add' if args.use_normalization else 'mean')\n",
    "        model.gnn2.set_aggr('add' if args.use_normalization else 'mean')\n",
    "\n",
    "        total_loss = total_examples = 0\n",
    "        total_loss_seed = total_loss_hm = total_loss_ht = 0\n",
    "        total_seed_example = total_hm_example = total_ht_example = 0\n",
    "                    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "#             print(batch_data);\n",
    "#             print(\"*\"*50)            \n",
    "            \n",
    "            batch_data = [b_data.to(device) for b_data in batch_data]\n",
    "            batch_size = batch_data[0].batch_size\n",
    "            mask = batch_data[0].train_mask[:batch_size]\n",
    "            y_true = batch_data[0].y[:batch_size]\n",
    "            \n",
    "            if torch.sum(mask) == 0:\n",
    "                print(\"no training mask in seed node\")\n",
    "#                 continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if args.use_normalization:                 \n",
    "                \n",
    "                batch_data[0].weight = batch_data[0].edge_norm * batch_data[0].edge_weight\n",
    "                batch_data[1].weight = batch_data[1].edge_norm * batch_data[1].edge_weight\n",
    "                \n",
    "                out, out1, out2 = model(batch_data)                            \n",
    "                #print(out.shape, out1.shape, out2.shape)\n",
    "                \n",
    "                                \n",
    "#                 print(out[:batch_size][mask].shape, y_true[mask].shape)                \n",
    "#                 print(loss.shape)\n",
    "#                 print(batch_data[0].node_norm.shape)\n",
    "#                 print(batch_data[0].node_norm[:batch_size].shape)\n",
    "#                 loss = criterion(out, batch_data.y, reduction='none')\n",
    "\n",
    "                #--------- loss computation seed nodes --------- #                \n",
    "                loss_seed = F.nll_loss(out[:batch_size][mask], y_true[mask], reduction='none')\n",
    "                loss_seed = (loss_seed * batch_data[0].node_norm[:batch_size][mask]).sum()\n",
    "        \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "            \n",
    "                loss_hm = F.nll_loss(out1[batch_data[0].train_mask], batch_data[0].y[batch_data[0].train_mask], reduction='none')\n",
    "                loss_hm = (loss_hm * batch_data[0].node_norm[batch_data[0].train_mask]).sum()\n",
    "                \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "                loss_ht = F.nll_loss(out2[batch_data[1].train_mask], batch_data[1].y[batch_data[1].train_mask], reduction='none')\n",
    "                loss_ht = (loss_ht * batch_data[1].node_norm[batch_data[1].train_mask]).sum()\n",
    "                \n",
    "                if channel=='sd':loss = loss_seed\n",
    "                elif channel=='hm':loss = loss_hm\n",
    "                elif channel=='ht':loss = loss_ht\n",
    "                else:loss = loss_seed + loss_hm + loss_ht                \n",
    "\n",
    "            else:\n",
    "                batch_data[0].weight = batch_data[0].edge_weight\n",
    "                batch_data[1].weight = batch_data[1].edge_weight\n",
    "                    \n",
    "                out, out1, out2 = model(batch_data)                            \n",
    "                #print(out.shape, out1.shape, out2.shape)\n",
    "                \n",
    "                mask = batch_data[0].train_mask[:batch_size]\n",
    "                y_true = batch_data[0].y[:batch_size]\n",
    "                \n",
    "                #--------- loss computation seed nodes --------- #                \n",
    "                loss_seed = F.nll_loss(out[:batch_size][mask], y_true[mask])\n",
    "                \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "                loss_hm = F.nll_loss(out1[batch_data[0].train_mask], batch_data[0].y[batch_data[0].train_mask])\n",
    "                \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "                loss_ht = F.nll_loss(out2[batch_data[1].train_mask], batch_data[1].y[batch_data[1].train_mask])\n",
    "                #loss = criterion(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "                \n",
    "                if channel=='sd':loss = loss_seed\n",
    "                elif channel=='hm':loss = loss_hm\n",
    "                elif channel=='ht':loss = loss_ht\n",
    "                else:loss = loss_seed + loss_hm + loss_ht\n",
    "                                                        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss_seed+= loss_seed.item()*batch_size\n",
    "            total_loss_hm+= loss_hm.item()*sum(batch_data[0].train_mask).item()\n",
    "            total_loss_ht+= loss_ht.item()*sum(batch_data[1].train_mask).item()\n",
    "            \n",
    "            total_seed_example+= sum(mask).item()\n",
    "            total_hm_example+=sum(batch_data[0].train_mask).item()\n",
    "            total_ht_example+=sum(batch_data[1].train_mask).item()\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "            \n",
    "        total_loss_seed /= max(total_seed_example,1)\n",
    "        total_loss_hm /= max(total_hm_example,1)\n",
    "        total_loss_ht /= max(total_ht_example,1)\n",
    "        \n",
    "        total_loss=total_loss_seed+total_loss_hm+total_loss_ht\n",
    "        total_examples = total_seed_example + total_hm_example + total_ht_example\n",
    "        \n",
    "        \n",
    "        if epoch%10==0:\n",
    "        \n",
    "            if args.log_info:\n",
    "                print(f'Loss:{total_loss:0.4f} seed:{total_loss_seed:0.4f} hm:{total_loss_hm:0.4f} ht:{total_loss_ht:0.4f}')\n",
    "                print(f'Example:{total_examples:1d} seed:{total_seed_example:1d} hm:{total_hm_example:1d} ht:{total_ht_example:1d}')\n",
    "\n",
    "            if channel=='sd':train_losses.append(total_loss_seed)\n",
    "            elif channel=='hm':train_losses.append(total_loss_hm)\n",
    "            elif channel=='ht':train_losses.append(total_loss_ht)\n",
    "            else:train_losses.append(total_loss)\n",
    "\n",
    "\n",
    "            model.eval()       \n",
    "            model.gnn1.set_aggr('add' if args.use_normalization else 'mean')\n",
    "    #         model.gnn2.set_aggr('add' if args.use_normalization else 'mean')\n",
    "            accs = [0,0,0]\n",
    "\n",
    "            if data.num_nodes<th_node:\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out, out1, out2 = model(all_data)\n",
    "\n",
    "                    #print(out.shape, out1.shape, out2.shape)\n",
    "                    y_pred_seed = out.argmax(dim=-1).cpu()\n",
    "                    y_pred_hm = out1.argmax(dim=-1).cpu()\n",
    "                    y_pred_ht = out2.argmax(dim=-1).cpu()\n",
    "\n",
    "                    if channel=='sd':\n",
    "                        y_pred = y_pred_seed\n",
    "                    elif channel=='hm':\n",
    "                        y_pred = y_pred_hm\n",
    "                    elif channel=='ht':\n",
    "                        y_pred = y_pred_ht\n",
    "                    else:\n",
    "                        y_pred = prediction(y_pred_seed, y_pred_hm, y_pred_ht)\n",
    "\n",
    "                    #print(y_pred)                \n",
    "                    correct = y_pred.eq(data.y.cpu())\n",
    "                    accs = []\n",
    "                    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "                        accs.append(correct[mask].sum().item() / mask.sum().item()) \n",
    "\n",
    "            else:\n",
    "                accs[0] = test(model, train_loader, data.train_mask, name='Train',channel=channel)\n",
    "                accs[1] = test(model, val_loader, data.val_mask, name='Validation',channel=channel)\n",
    "                accs[2] = test(model, test_loader, data.test_mask, name='Test',channel=channel)\n",
    "\n",
    "            train_accuracies.append(accs[0])\n",
    "            val_accuracies.append(accs[1])\n",
    "            test_accuracies.append(accs[2])\n",
    "            std_dev = np.std(train_losses[-5:])\n",
    "\n",
    "            if args.log_info:\n",
    "                print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {accs[0]:.4f}, Val: {accs[1]:.4f}, Test: {accs[2]:.4f}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "#             if epoch>=5 and std_dev<=1e-3:\n",
    "#                 if args.log_info:\n",
    "#                     print(\"Iteration for convergence: \", epoch)\n",
    "#                 max_iteration = epoch\n",
    "#                 break\n",
    "    \n",
    "    if args.log_info:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Results/AGSGSValidation', yname='Accuracy', xname='Epoch')\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "    \n",
    "    if (minibatch_size == data.num_nodes) and data.num_nodes<th_node:\n",
    "        del all_data\n",
    "    \n",
    "    return max(test_accuracies), max_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGSGSperformanceSampler(DATASET_NAME,data, dataset, num_classes, epochs=1,channel='all'):\n",
    "    \n",
    "    ###\n",
    "    #BATCH_SIZE = min(data.num_nodes, 1024)\n",
    "    BATCH_SIZE = 6000\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"BATCH SIZE: \", BATCH_SIZE)\n",
    "        \n",
    "    model = AGSGNN(data.x.shape[1], num_classes, hidden_channels=256, N = BATCH_SIZE).to(device)            \n",
    "    \n",
    "    if args.log_info:\n",
    "        print(model)\n",
    "    \n",
    "    itr, accuracy = train(DATASET_NAME, model, data, dataset, epochs, channel, BATCH_SIZE)\n",
    "    \n",
    "    return itr, accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce2248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "def adj_feature(data):    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    edges = data.edge_index.t()\n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    adj_mat[edges[:,1], edges[:,0]] = 1\n",
    "    \n",
    "#     n_components = data.x.shape[1]\n",
    "    n_components = min(256, data.x.shape[1], data.num_nodes)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    x = svd.fit_transform(adj_mat)\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    x.shape    \n",
    "    \n",
    "    return x\n",
    "\n",
    "# x = adj_feature(data)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "586728b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_info = False\n",
    "args.recompute = False\n",
    "\n",
    "# DATASET_NAME = 'Cora'\n",
    "# data, dataset = get_data(DATASET_NAME,DIR=None, log=False, h_score=True, split_no=0)\n",
    "# print(data)\n",
    "\n",
    "# channel = 'all' #'all', 'hm', 'ht', 'sd'\n",
    "\n",
    "# # if DATASET_NAME in ['Squirrel', 'Chameleon']:\n",
    "# #     data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "# #     if args.log_info == True:\n",
    "# #         print(data.x.shape)\n",
    "    \n",
    "# best_acc, num_iteration, _ =  AGSGSperformanceSampler(DATASET_NAME, data, dataset, dataset.num_classes,\n",
    "#                                                       epochs=150, channel=channel)\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72e3b1",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd76e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genius acc 0.8003 sd 0.0000 itr 50 sd 0\n",
      "pokec "
     ]
    }
   ],
   "source": [
    "def batch_experiments(num_run=1, channel='all'):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        'genius',\n",
    "        'pokec',\n",
    "        'arxiv-year',\n",
    "        'snap-patents',\n",
    "        'twitch-gamers',\n",
    "        'wiki',\n",
    "        'AmazonProducts',\n",
    "        'Yelp',\n",
    "        'Reddit',\n",
    "        'Reddit2',        \n",
    "    ]\n",
    "    \n",
    "    #ALL_DATASETs= [\"Cora\"]\n",
    "#     ALL_DATASETs= [\"Squirrel\"]\n",
    "#     ALL_DATASETs= [\"Texas\", \"Cornell\", \"Wisconsin\"]\n",
    "#     ALL_DATASETs= [\"cornell5\", \"penn94\", \"johnshopkins55\"]\n",
    "\n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/AGSGNN-SAINT-II-GS-\"+channel+\".txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "                \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i,random_state=i)  \n",
    "\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                \n",
    "#             if DATASET_NAME in ['Squirrel', 'Chameleon', \n",
    "#                                 #'cornell5','penn94','johnshopkins55'\n",
    "#                                ]:\n",
    "#                 data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "#                 if args.log_info == True:\n",
    "#                     print(data.x.shape)\n",
    "                              \n",
    "            accuracy, itr, _ = AGSGSperformanceSampler(DATASET_NAME, data, dataset, num_classes,epochs=max_epochs, channel=channel)\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "start = time.time()\n",
    "batch_experiments(num_run=1, channel='all')\n",
    "end = time.time()\n",
    "print(\"Time spent:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cab8c",
   "metadata": {},
   "source": [
    "## Visualize representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    \n",
    "#     n=7\n",
    "#     x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "#     y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "#     edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "#     edge_index = edge_index-1\n",
    "    \n",
    "#     mask = torch.zeros(n, dtype=torch.bool)\n",
    "#     mask[[1,3]] = True\n",
    "    \n",
    "#     test_data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = mask, val_mask = mask)    \n",
    "#     print(test_data)\n",
    "    \n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f76e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AGS_layer(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.T = 3\n",
    "#         self.p = dropout\n",
    "#         #self.Aconv1 = GCNConv(input_channels, output_channels)        \n",
    "#         #self.Sconv1 = SpatialConv(input_channels, output_channels)\n",
    "        \n",
    "#         self.Aconv1 = GCNConv(input_channels, output_channels)\n",
    "#         self.Sconv1 = SpatialConv(input_channels, output_channels)\n",
    "        \n",
    "#         self.I1 = nn.Linear(input_channels, output_channels)\n",
    "        \n",
    "#         self.layer_norm_a1 =  nn.LayerNorm(output_channels)\n",
    "#         self.layer_norm_s1 =  nn.LayerNorm(output_channels)\n",
    "#         self.layer_norm_i1 =  nn.LayerNorm(output_channels)\n",
    "        \n",
    "#         self.alpha_a1 = nn.Linear(output_channels, 1)\n",
    "#         self.alpha_s1 = nn.Linear(output_channels, 1)\n",
    "#         self.alpha_i1 = nn.Linear(output_channels, 1)\n",
    "#         self.w1 = nn.Linear(3, 3)\n",
    "        \n",
    "#         #self.reset_parameters()\n",
    "            \n",
    "#     def reset_parameters(self):\n",
    "        \n",
    "#         stdv = 1. / math.sqrt(self.I1.weight.size(1))\n",
    "#         std_att = 1. / math.sqrt(self.w1.weight.size(1))\n",
    "#         std_att_vec = 1. / math.sqrt( self.alpha_a1.weight.size(1))\n",
    "        \n",
    "#         self.I1.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "#         self.alpha_a1.weight.data.uniform_(-std_att, std_att)\n",
    "#         self.alpha_s1.weight.data.uniform_(-std_att, std_att)\n",
    "#         self.alpha_i1.weight.data.uniform_(-std_att, std_att)\n",
    "        \n",
    "#         self.w1.weight.data.uniform_(-std_att_vec, std_att_vec)\n",
    "        \n",
    "#         self.layer_norm_a1.reset_parameters()\n",
    "#         self.layer_norm_s1.reset_parameters()\n",
    "#         self.layer_norm_i1.reset_parameters()\n",
    "        \n",
    "\n",
    "#     def forward(self, x0, edge_index, edge_weight=None):\n",
    "#         a1 = F.relu(self.Aconv1(x0, edge_index, edge_weight))\n",
    "#         a1 = self.layer_norm_a1(a1)\n",
    "#         a1 = F.dropout(a1, p=self.p, training=self.training)\n",
    "        \n",
    "#         s1 = F.relu(self.Sconv1(x0, edge_index, edge_weight))\n",
    "#         s1 = self.layer_norm_s1(s1)\n",
    "#         s1 = F.dropout(s1, p=self.p, training=self.training)\n",
    "\n",
    "#         i1 = F.relu(self.I1(x0))\n",
    "#         i1 = self.layer_norm_i1(i1)\n",
    "#         i1 = F.dropout(i1, p=self.p, training=self.training)\n",
    "        \n",
    "#         ala1 = torch.sigmoid(self.alpha_a1(a1))\n",
    "#         als1 = torch.sigmoid(self.alpha_s1(s1))\n",
    "#         ali1 = torch.sigmoid(self.alpha_i1(i1))        \n",
    "#         alpha1 = F.softmax(self.w1(torch.cat([ala1, als1, ali1],dim=-1)/self.T), dim=1)        \n",
    "        \n",
    "#         x1 = torch.mm(torch.diag(alpha1[:,0]),a1) + torch.mm(torch.diag(alpha1[:,1]),s1) + torch.mm(torch.diag(alpha1[:,2]),i1)                \n",
    "        \n",
    "#         return x1\n",
    "        \n",
    "# class AGS_GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_features, num_classes, hidden_channels=16, dropout=0.2):\n",
    "#         super().__init__()        \n",
    "#         self.num_classes = num_classes\n",
    "#         self.p = dropout\n",
    "        \n",
    "#         self.ags_layer1 = AGS_layer(num_features, hidden_channels)\n",
    "#         self.ags_layer2 = AGS_layer(hidden_channels, hidden_channels)\n",
    "#         #self.ags_layer2 = AGS_layer(hidden_channels, num_classes)\n",
    "                \n",
    "#         self.CombineW = nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "#         self.PredW = nn.Linear(1*hidden_channels, num_classes)\n",
    "        \n",
    "    \n",
    "#     def forward(self, x0, edge_index, edge_weight=None):\n",
    "        \n",
    "#         #x0 = F.dropout(x0, p=self.p, training=self.training)\n",
    "#         x1 = self.ags_layer1(x0, edge_index, edge_weight)\n",
    "#         x1 = F.dropout(x1, p=self.p, training=self.training)\n",
    "        \n",
    "#         x2 = self.ags_layer2(x1, edge_index, edge_weight)\n",
    "#         x2 = F.dropout(x2, p=self.p, training=self.training)        \n",
    "        \n",
    "#         #x = self.PredW(torch.cat([x1, x2], dim=-1))\n",
    "#         x = self.PredW(x2)\n",
    "         \n",
    "#         #return x\n",
    "#         return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
