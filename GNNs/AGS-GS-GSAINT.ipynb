{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--recompute', type=bool, default=False)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_true', default=True)    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSAINT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler, GraphSAINTEdgeSampler, GraphSAINTSampler\n",
    "from ipynb.fs.full.AGSGraphSampler import AGSGraphSampler\n",
    "\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels):\n",
    "        super().__init__()        \n",
    "        in_channels = num_features\n",
    "        out_channels = num_classes\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "#         self.lin = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
    "\n",
    "\n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "#         self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "#         x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "#         x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "#         x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    \n",
    "    #graphsage\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, device, subgraph_loader):\n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=x_all.size(0))\n",
    "            pbar.set_description('Evaluating')\n",
    "\n",
    "        xs = []\n",
    "        for batch_size, n_id, adj in subgraph_loader:\n",
    "            edge_index, _, size = adj.to(device)\n",
    "            x = x_all[n_id].to(device)                \n",
    "            x = self.forward(x, edge_index)\n",
    "            x_target = x[:size[1]]\n",
    "\n",
    "            xs.append(x_target.cpu())\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "        x_all = torch.cat(xs, dim=0)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    if args.log_info:\n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "\n",
    "    model.eval()\n",
    "    model.set_aggr('add' if args.use_normalization else 'mean')\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            out = model(batch_data.x.to(device), batch_data.edge_index.to(device))\n",
    "            out=out[:batch_data.batch_size,:]\n",
    "            pred = out.argmax(dim=-1)            \n",
    "            correct = pred.eq(batch_data.y[:batch_data.batch_size].to(device))\n",
    "\n",
    "            total_correct+=correct.sum()\n",
    "            total_examples+=batch_data.batch_size\n",
    "\n",
    "            if args.log_info:                \n",
    "                pbar.update(batch_data.batch_size)\n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "\n",
    "    return total_correct.item()/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(DATASET_NAME, model, data, dataset, epochs=10,train_neighbors=[8,4],test_neighbors=[8,4]):\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    row, col = data.edge_index\n",
    "    data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
    "\n",
    "    \n",
    "    sampler_dir = DIR+'AGSGSAINTfast/'+DATASET_NAME\n",
    "    if not os.path.exists(sampler_dir):\n",
    "        os.makedirs(sampler_dir)\n",
    "        \n",
    "    #batch_size=min(data.num_nodes,2048)\n",
    "    batch_size= 2048\n",
    "    \n",
    "    num_steps=math.ceil(data.num_nodes/batch_size) #num_steps=5    \n",
    "    num_workers = 0 if data.num_nodes <100000 else 8\n",
    "    \n",
    "    worker = num_workers\n",
    "    \n",
    "    sample_func =['wrw']\n",
    "    weight_func =[\n",
    "        {'exact':False,'weight':'fastlink'}, #exact for exact size to the batch\n",
    "       #{'exact':False,'weight':'knn'}\n",
    "    ]\n",
    "\n",
    "    params={'knn':{'metric':'cosine'},\n",
    "            'submodular':{'metric':'cosine'},\n",
    "            'link-nn':{'value':'min'},\n",
    "            'link-sub':{'value':'max'},\n",
    "            'disjoint':{'value':'mst'},\n",
    "           }\n",
    "    \n",
    "    loader = AGSGraphSampler(\n",
    "        data, batch_size=batch_size, walk_length=2, num_steps=num_steps, sample_coverage=100,\n",
    "        num_workers=num_workers,log=args.log_info,save_dir=sampler_dir,recompute = args.recompute, shuffle = False,\n",
    "        sample_func = sample_func, weight_func=weight_func, params=params)\n",
    "        \n",
    "#     #### original loader\n",
    "#     loader = GraphSAINTRandomWalkSampler(data, batch_size=batch_size, walk_length=2,\n",
    "#                                      num_steps=num_steps, sample_coverage=100,\n",
    "#                                      save_dir=sampler_dir,num_workers=num_workers)\n",
    "    \n",
    "#     #----\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "\n",
    "    sample_batch_size=1024\n",
    "    train_loader = NeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                            batch_size=sample_batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    val_loader = NeighborLoader(data,input_nodes=data.val_mask,num_neighbors=test_neighbors, \n",
    "                                batch_size=sample_batch_size,shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    test_loader = NeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "                                 batch_size=sample_batch_size,shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "#         subgraph_loader = NeighborSampler(data.edge_index, node_idx=None,\n",
    "#                                       sizes=[-1], batch_size=2048,\n",
    "#                                       shuffle=False, num_workers=4)\n",
    "\n",
    "    \n",
    "    best_acc=0    \n",
    "    num_iteration = epochs\n",
    "    train_losses = []; val_accuracies = []; train_accuracies = []; test_accuracies = [];\n",
    "    training_times = []\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        if args.log_info:\n",
    "            #pbar = tqdm(total=int(sum(data.train_mask)))\n",
    "            pbar = tqdm(total=batch_size*num_steps)\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        model.set_aggr('add' if args.use_normalization else 'mean')\n",
    "\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "            #print(batch_data);print(\"*\"*50)            \n",
    "            batch_data = batch_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if args.use_normalization:                \n",
    "                edge_weight = batch_data.edge_norm * batch_data.edge_weight\n",
    "                out = model(batch_data.x, batch_data.edge_index, edge_weight)\n",
    "                loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask], reduction='none')                \n",
    "                loss = (loss * batch_data.node_norm[batch_data.train_mask]).sum()\n",
    "            else:\n",
    "                out = model(batch_data.x, batch_data.edge_index)\n",
    "                loss = F.nll_loss(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "                #loss = criterion(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch_data.num_nodes\n",
    "            total_examples += batch_data.num_nodes\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_size)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        training_times.append(epoch_end-epoch_start)\n",
    "        \n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(\"Training Loss: \",loss)                             \n",
    "        \n",
    "        if data.num_nodes<10000:\n",
    "            model.eval()\n",
    "            #model.set_aggr('mean')\n",
    "            model.set_aggr('add' if args.use_normalization else 'mean')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x.to(device), data.edge_index.to(device))\n",
    "                pred = out.argmax(dim=-1)\n",
    "                correct = pred.eq(data.y.to(device))\n",
    "\n",
    "            accs = []\n",
    "            for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "                accs.append(correct[mask].sum().item() / mask.sum().item())\n",
    "            \n",
    "            if args.log_info:                \n",
    "                print(accs)\n",
    "\n",
    "            if accs[2]>best_acc:\n",
    "                best_acc=accs[2]\n",
    "                \n",
    "            train_acc=accs[0]\n",
    "            val_acc=accs[1]\n",
    "            test_acc=accs[2]\n",
    "\n",
    "        else:\n",
    "            if args.log_info==True:\n",
    "                train_acc=test(model, train_loader,data.train_mask,'Train')\n",
    "                val_acc = test(model, val_loader,data.val_mask,'Validation')\n",
    "            else:\n",
    "                train_acc=0\n",
    "                val_acc = 0            \n",
    "            test_acc = test(model, test_loader,data.test_mask,'Test')\n",
    "            \n",
    "            accs=[train_acc,val_acc,test_acc]\n",
    "            \n",
    "            if args.log_info:\n",
    "                print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "            if test_acc>best_acc:\n",
    "                best_acc=test_acc\n",
    "                \n",
    "        train_accuracies.append(accs[0])\n",
    "        val_accuracies.append(accs[1])\n",
    "        test_accuracies.append(accs[2])\n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "                \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        if args.log_info:\n",
    "            print('std_dev: ', std_dev)\n",
    "        \n",
    "#         if epoch>=5 and std_dev<=1e-3:\n",
    "#             num_iteration = epoch            \n",
    "#             if args.log_info:                \n",
    "#                 print(\"Iteration for convergence: \", epoch)\n",
    "#             break\n",
    "            \n",
    "    if args.log_info:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Results/AGSGSValidation', yname='Accuracy', xname='Epoch')\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "    \n",
    "    acc_file = open(\"Runtime/AGSGSAINT.txt\",'a+') \n",
    "    acc_file.write(str(train_losses))\n",
    "    acc_file.write(str(train_accuracies))\n",
    "    acc_file.write(str(val_accuracies))\n",
    "    acc_file.write(str(test_accuracies))\n",
    "    acc_file.write(str(training_times))\n",
    "    acc_file.write(str(np.mean(training_times)))\n",
    "    acc_file.write(f'\\nworker {worker:1d} avg epoch runtime {np.mean(training_times):0.8f}')\n",
    "    acc_file.close()     \n",
    " \n",
    "                \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def GSAINTperformance(DATASET_NAME, data, dataset, num_classes, epochs=20, train_neighbors=[8,4],test_neighbors=[8,4]):\n",
    "    model = Net(data.x.shape[1], num_classes, hidden_channels=128).to(device)        \n",
    "    \n",
    "    if args.log_info:\n",
    "        print(model)\n",
    "    \n",
    "    best_acc, num_iteration = train(DATASET_NAME, model, data, dataset, epochs, train_neighbors, test_neighbors)\n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import add_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "def adj_feature(data):    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    edges = data.edge_index.t()\n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    adj_mat[edges[:,1], edges[:,0]] = 1\n",
    "    \n",
    "#     n_components = data.x.shape[1]\n",
    "    n_components = min(256, data.x.shape[1], data.num_nodes)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    x = svd.fit_transform(adj_mat)\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    x.shape    \n",
    "    \n",
    "    return x\n",
    "\n",
    "# x = adj_feature(data)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your adjacency matrix (replace this with your actual adjacency matrix)\n",
    "# adj = np.array([[0, 1, 0, 1],\n",
    "#                 [1, 0, 1, 0],\n",
    "#                 [0, 1, 0, 1],\n",
    "#                 [1, 0, 1, 0]], dtype=np.float32)\n",
    "\n",
    "# n_components = 2\n",
    "\n",
    "# # Perform SVD dimensionality reduction\n",
    "# svd = TruncatedSVD(n_components=n_components)\n",
    "# low_dimensional_matrix = svd.fit_transform(adj_feature(data))\n",
    "\n",
    "# low_dimensional_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# args.log_info = True\n",
    "# args.recompute = False\n",
    "# args.use_normalization = True\n",
    "\n",
    "# DATASET_NAME = 'wiki'\n",
    "# data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0, random_state=0); print(\"\")\n",
    "\n",
    "# if len(data.y.shape) > 1:\n",
    "#     data.y = data.y.argmax(dim=1)        \n",
    "#     num_classes = torch.max(data.y).item()+1\n",
    "# else:\n",
    "#     num_classes = dataset.num_classes\n",
    "\n",
    "# if num_classes!= torch.max(data.y)+1:\n",
    "#     num_classes = torch.max(data.y).item()+1\n",
    "\n",
    "\n",
    "# data.edge_index, _ = add_self_loops(data.edge_index)\n",
    "    \n",
    "# if DATASET_NAME in ['Cornell', 'cornell5']:\n",
    "#     data.edge_index, _ = add_self_loops(data.edge_index)\n",
    "    \n",
    "# if DATASET_NAME in ['Squirrel', 'Chameleon', 'amherst41',\n",
    "#                     'Cornell','cornell5']:\n",
    "#     data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "#     if args.log_info == True:\n",
    "#         print(data.x.shape)\n",
    "\n",
    "# best_acc, num_iteration, _ = GSAINTperformance(DATASET_NAME, data, dataset, num_classes, epochs=50,\n",
    "#                              train_neighbors=[8,4],test_neighbors=[8,4])    \n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Cornell\",\n",
    "#         \"Texas\",\n",
    "#         \"Wisconsin\",\n",
    "#         \"reed98\",\n",
    "#         \"amherst41\",\n",
    "#         \"penn94\",\n",
    "#         \"Roman-empire\",\n",
    "#         \"cornell5\",\n",
    "#         \"Squirrel\",\n",
    "#         \"johnshopkins55\",\n",
    "#         \"AmazonProducts\",\n",
    "#         \"Actor\",\n",
    "#         \"Minesweeper\",\n",
    "#         \"Questions\",\n",
    "#         \"Chameleon\",\n",
    "#         \"Tolokers\",\n",
    "#         \"Flickr\",\n",
    "#         \"Yelp\",\n",
    "#         \"Amazon-ratings\",\n",
    "#         \"genius\",\n",
    "#         \"cora\",\n",
    "#         \"CiteSeer\",\n",
    "#         \"dblp\",\n",
    "#         \"Computers\",\n",
    "#         \"pubmed\",\n",
    "#         \"Reddit\",\n",
    "#         \"cora_ml\",\n",
    "#         \"Cora\",\n",
    "#         \"Reddit2\",\n",
    "#         \"CS\",\n",
    "#         \"Photo\",\n",
    "#         \"Physics\",\n",
    "#         \"citeseer\",\n",
    "        'wiki',\n",
    "        'pokec',\n",
    "        'arxiv-year',\n",
    "        'snap-patents',\n",
    "        'twitch-gamer',\n",
    "    ]     \n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        'wiki',\n",
    "        'pokec',\n",
    "        'arxiv-year',\n",
    "        'snap-patents',\n",
    "        'twitch-gamer',\n",
    "        'genius'\n",
    "    ]\n",
    "    \n",
    "    args.log_info = False\n",
    "    runtime_filename = \"Runtime/AGSGSAINT.txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')        \n",
    "        result_file = open(\"Results/AGSGSAINT.txt\",'a+')                \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        \n",
    "        acc_file = open(runtime_filename,'a+') \n",
    "        acc_file.write(f'{DATASET_NAME}\\n')\n",
    "        acc_file.close()     \n",
    "\n",
    "        \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state = i)            \n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)                \n",
    "#                 continue\n",
    "            \n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                \n",
    "            \n",
    "#             data.edge_index, _ = add_self_loops(data.edge_index) \n",
    "#             data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "#             if args.log_info == True:\n",
    "#                 print(data.x.shape)\n",
    "                \n",
    "            \n",
    "#             if DATASET_NAME in ['Cornell', 'cornell5']:\n",
    "#                 data.edge_index, _ = add_self_loops(data.edge_index)            \n",
    "\n",
    "            \n",
    "#             if DATASET_NAME in [\n",
    "#                 'Squirrel', 'Chameleon', 'Cornell'\n",
    "#                 'amherst41', 'cornell5',\n",
    "#             ]:\n",
    "#                 data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "#                 if args.log_info == True:\n",
    "#                     print(data.x.shape)\n",
    "                                \n",
    "            accuracy, itr, _ = GSAINTperformance(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs,train_neighbors=[8,4],test_neighbors=[8,4])\n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "\n",
    "# start = time.time()\n",
    "# batch_experiments(num_run = 1)\n",
    "# end = time.time()\n",
    "# print(\"Time spent:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import generate_synthetic2homophily\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wisconsin - 10 [0.6078431372549019, 0.5686274509803921, 0.7450980392156863, 0.6666666666666666, 0.6470588235294118] [150, 150, 150, 150, 150]\n",
      "Wisconsin - 10 acc 64.7059 \\pm 5.9474 itr 150 sd 0\n",
      "reed98 - 10 [0.5751295336787565, 0.5492227979274611, 0.5284974093264249, 0.6010362694300518, 0.5181347150259067] [150, 150, 150, 150, 150]\n",
      "reed98 - 10 acc 55.4404 \\pm 3.0389 itr 150 sd 0\n",
      "Roman-empire - 10 [0.7410871867278503, 0.7502647370278857, 0.7432050829509355, 0.7462054359336393, 0.7386163078009178] [150, 150, 150, 150, 150]\n",
      "Roman-empire - 10 acc 74.3876 \\pm 0.4053 itr 150 sd 0\n",
      "Actor - 10 [0.30855263157894736, 0.29407894736842105, 0.3013157894736842, 0.29473684210526313, 0.3] [150, 150, 150, 150, 150]\n",
      "Actor - 10 acc 29.9737 \\pm 0.5240 itr 150 sd 0\n",
      "Minesweeper - 10 [0.834, 0.8244, 0.8236, 0.8208, 0.8128] [150, 150, 150, 150, 150]\n",
      "Minesweeper - 10 acc 82.3120 \\pm 0.6814 itr 150 sd 0\n",
      "Tolokers - 10 [0.7826530612244897, 0.7918367346938775, 0.7884353741496599, 0.7894557823129251, 0.7819727891156463] [150, 150, 150, 150, 150]\n",
      "Tolokers - 10 acc 78.6871 \\pm 0.3888 itr 150 sd 0\n",
      "Runtime:  813.2603590488434\n"
     ]
    }
   ],
   "source": [
    "def ablation(num_run = 1):\n",
    "    \n",
    "    #SYN_NAME = random.randint(0,1000)\n",
    "\n",
    "    ALL_DATASETs= [\n",
    "        'Wisconsin',\n",
    "        'reed98',        \n",
    "        'Roman-empire',\n",
    "        'Actor',\n",
    "        'Minesweeper',        \n",
    "        'Tolokers'\n",
    "    ]\n",
    "    \n",
    "#     ALL_DATASETs= [\"Cora\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/AGSGS-GNN-NS-2Ablation.txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        \n",
    "        random_state = 10\n",
    "        \n",
    "        print(DATASET_NAME,\"-\",random_state, end=' ')\n",
    "        \n",
    "        \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "#             d = 100\n",
    "#             h =0.05\n",
    "#             train=0.3\n",
    "#             balance=True\n",
    "#             h2 = 0.25\n",
    "#             ratio = 0.50\n",
    "                                    \n",
    "# #             global data_filename_extension\n",
    "# #             data_filename_extension = str(d)+str(h)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "# #             data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "# #             if os.path.exists(data_filename):\n",
    "# #                 data = torch.load(data_filename)                \n",
    "# #                 print(\"loaded \"+data_filename)\n",
    "# #             else:\n",
    "# #                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False, balance=balance)\n",
    "# # #                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False)\n",
    "# #                 torch.save(data,data_filename)\n",
    "# #                 print(\"saved \"+data_filename)\n",
    "        \n",
    "#             global data_filename_extension\n",
    "#             data_filename_extension = str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "#             data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "#             if os.path.exists(data_filename):\n",
    "#                 data = torch.load(data_filename)                \n",
    "#                 print(\"loaded \"+data_filename)\n",
    "#             else:\n",
    "#                 data = generate_synthetic2homophily(data, d=d, h1=h, h2=h2, ratio=ratio, train=train, random_state=random_state, log=False, balance=balance)                 \n",
    "#                 torch.save(data,data_filename)\n",
    "#                 print(\"saved \"+data_filename)\n",
    "    \n",
    "            ##Sparsifiy\n",
    "            #data = random_sparsify(data, 13, log = True)\n",
    "#             data = sparsify(data, log = True, method = 'submodular', metric= 'cosine')\n",
    "                        \n",
    "#             data1 = sparsify(copy.deepcopy(data), log = True, method = 'submodular', metric= 'cosine')\n",
    "#             data = sparsify(data, log = True, method = 'nn', metric= 'cosine')                         \n",
    "#             data.edge_index = torch.cat((data.edge_index, data1.edge_index), dim=1)\n",
    "            \n",
    "            #optional for making undirected graph\n",
    "            (row, col) = data.edge_index\n",
    "            data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "            data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "            \n",
    "            if args.log_info:\n",
    "                print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "                print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "                print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "                print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 20\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon','cornell5','penn94','johnshopkins55','amherst41']:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "\n",
    "#             accuracy, itr = 0,0           \n",
    "            accuracy, itr, _ = GSAINTperformance(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs,train_neighbors=[8,4],test_neighbors=[8,4])\n",
    "            \n",
    "            #print(mdl)\n",
    "    \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        print(accs, itrs)\n",
    "        print(DATASET_NAME,\"-\",random_state, end=' ')\n",
    "        print(f'acc {np.mean(accs)*100:0.4f} \\pm {np.std(accs)*100:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs)*100:0.4f} \\pm {np.std(accs)*10:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "    return \n",
    "\n",
    "st_time = time.time()\n",
    "ablation(num_run=5)\n",
    "en_time = time.time()\n",
    "\n",
    "print(\"Runtime: \", en_time-st_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
