{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "# device = torch.device(\"cpu\")  \n",
    "# print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "    DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "elif os.uname()[1].find('unimodular')==0:\n",
    "    DIR='/scratch2/das90/Dataset/'\n",
    "elif os.uname()[1].find('Siddharthas')==0:\n",
    "    DIR='/Users/siddharthashankardas/Purdue/Dataset/'  \n",
    "else:\n",
    "    DIR='./Dataset/'\n",
    "    \n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR=DIR+'RESULTS/'\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Data directory: \", DIR)\n",
    "print(\"Result directory:\", RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Cora\", \"CiteSeer\", \"PubMed\", \"Reddit\", \"Flickr\", \n",
    "#\"Yelp\", \"AmazonProducts\",\"Reddit2\",\" OGB_MAG\"\n",
    "#\"Fake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME='Cora' \n",
    "\n",
    "def get_data(DATASET_NAME='Cora'):\n",
    "\n",
    "    if DATASET_NAME in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "        dataset = Planetoid(root=DIR+'Planetoid', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "\n",
    "    elif DATASET_NAME == \"Reddit2\":\n",
    "        dataset = Reddit2(root=DIR+'Reddit2', transform=NormalizeFeatures())\n",
    "\n",
    "    elif DATASET_NAME == \"Reddit\":\n",
    "        dataset = Reddit(root=DIR+'Reddit', transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME == \"Flickr\":\n",
    "        dataset = Flickr(root=DIR+'Flickr', transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"Yelp\":\n",
    "        dataset = Yelp(root=DIR+'Yelp', transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"AmazonProducts\":\n",
    "        dataset = AmazonProducts(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "\n",
    "    else:    \n",
    "        raise Exception('dataset not found')\n",
    "\n",
    "    print()\n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('======================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Number of features: {dataset.num_features}')\n",
    "    print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "    data = dataset[0]  # Get the first graph object.\n",
    "    \n",
    "    data.y=F.one_hot(data.y, num_classes=dataset.num_classes)\n",
    "\n",
    "    print()\n",
    "    print(data)\n",
    "    print('===========================================================================================================')\n",
    "\n",
    "    # Gather some statistics about the graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "    \n",
    "    return data, dataset\n",
    "\n",
    "#data, dataset = get_data(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers=2):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                #x = F.dropout(x, p=0.5, training=self.training)\n",
    "                x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, device, subgraph_loader):\n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, epochs=100, train_neighbors=[25,10]):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    #criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    print(\"Train neighbors: \", train_neighbors)\n",
    "    \n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "#     train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
    "#                                    sizes=[25, 10], batch_size=1024,\n",
    "#                                    shuffle=True, num_workers=0)\n",
    "    \n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
    "                                   sizes=train_neighbors, batch_size=1024,\n",
    "                                   shuffle=True, num_workers=0)    \n",
    "    \n",
    "    subgraph_loader = NeighborSampler(data.edge_index, node_idx=None,\n",
    "                                          sizes=[-1], batch_size=2048,\n",
    "                                          shuffle=False, num_workers=6)\n",
    "    \n",
    "    x, y = data.x.to(device), data.y.to(device)\n",
    "    data.train_mask.to(device)\n",
    "    data.val_mask.to(device)\n",
    "    data.test_mask.to(device)\n",
    "    \n",
    "    \n",
    "    best_acc=0\n",
    "    best_sk=0\n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        pbar = tqdm(total=train_idx.size(0))\n",
    "        pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        total_loss = total_correct = 0\n",
    "        total_sample=0\n",
    "        total_acc = 0 \n",
    "        model.train()\n",
    "        for batch_size, n_id, adjs in train_loader:\n",
    "            adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x[n_id], adjs)\n",
    "            #loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "            loss = criterion(out, y[n_id[:batch_size]])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            total_loss += float(loss)\n",
    "            \n",
    "            \n",
    "#             tolerance=0.10\n",
    "#             bin_out = torch.sigmoid(out)\n",
    "            \n",
    "#             print(bin_out)\n",
    "            \n",
    "#             threshold = torch.max(bin_out,dim=1).values.view(-1,1)-tolerance            \n",
    "#             bin_out=(bin_out>=threshold).type(torch.long)\n",
    "            \n",
    "#             print(bin_out)\n",
    "#             print(threshold)\n",
    "\n",
    "#             total_acc+=accuracy_score(y[n_id[:batch_size]].detach().cpu().numpy(),bin_out.detach().cpu().numpy())*batch_size\n",
    "            \n",
    "            bin_out = torch.sigmoid(out)\n",
    "            total_acc+=label_ranking_average_precision_score(y[n_id[:batch_size]].detach().cpu().numpy(),bin_out.detach().cpu().numpy())*batch_size\n",
    "            \n",
    "            total_sample+=batch_size\n",
    "            \n",
    "            total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]].argmax(dim=-1)).sum())\n",
    "            \n",
    "            #total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        approx_acc = total_correct / train_idx.size(0)\n",
    "        \n",
    "        print(\"Multi-Accuracy: \", total_acc/total_sample)\n",
    "        print(f'Epoch: {epoch:03d}, Training Loss: {loss:.4f}, Training Accuracy: {approx_acc:.4f}')\n",
    "                \n",
    "        ####EVALUATION\n",
    "        if epoch>0 and epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.inference(x, device, subgraph_loader)\n",
    "            #res = out.argmax(dim=-1) == data.y\n",
    "            res = out.argmax(dim=-1) == data.y.argmax(dim=-1)\n",
    "            train_acc = int(res[data.train_mask].sum()) / int(data.train_mask.sum())\n",
    "            val_acc = int(res[data.val_mask].sum()) / int(data.val_mask.sum())\n",
    "            test_acc = int(res[data.test_mask].sum()) / int(data.test_mask.sum())            \n",
    "            \n",
    "            \n",
    "#             #tolerance=0.10\n",
    "#             bin_out = torch.sigmoid(out)\n",
    "#             threshold = torch.max(bin_out,dim=1).values.view(-1,1)-tolerance            \n",
    "#             bin_out=(bin_out>=threshold).type(torch.long)\n",
    "#             #print(bin_out[data.train_mask].detach().cpu().numpy())\n",
    "            \n",
    "#             sk_train=accuracy_score(y[data.train_mask].detach().cpu().numpy(),bin_out[data.train_mask].detach().cpu().numpy())\n",
    "#             sk_val=accuracy_score(y[data.val_mask].detach().cpu().numpy(),bin_out[data.val_mask].detach().cpu().numpy())\n",
    "#             sk_test=accuracy_score(y[data.test_mask].detach().cpu().numpy(),bin_out[data.test_mask].detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "            bin_out = torch.sigmoid(out)\n",
    "            sk_train=label_ranking_average_precision_score(y[data.train_mask].detach().cpu().numpy(),bin_out[data.train_mask].detach().cpu().numpy())\n",
    "            sk_val=label_ranking_average_precision_score(y[data.val_mask].detach().cpu().numpy(),bin_out[data.val_mask].detach().cpu().numpy())\n",
    "            sk_test=label_ranking_average_precision_score(y[data.test_mask].detach().cpu().numpy(),bin_out[data.test_mask].detach().cpu().numpy())\n",
    "\n",
    "            \n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "        \n",
    "            print(f'Epoch: {epoch:03d}, Mult-Train: {sk_train:.4f}, Mult-Val: {sk_val:.4f}, Mult-Test: {sk_test:.4f}')\n",
    "        \n",
    "            if test_acc>best_acc:\n",
    "                best_acc=test_acc\n",
    "                \n",
    "            if sk_test>best_sk:\n",
    "                best_sk=sk_test\n",
    "\n",
    "    print (\"Best Test Accuracy, \",best_acc)\n",
    "    print (\"Best Test Sk Accuracy, \",best_sk)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GSAGEperformance(data, dataset, epochs=20, train_neighbors=[25,10]):\n",
    "    model = SAGE(dataset.num_features, dataset.num_classes, hidden_channels=256).to(device)        \n",
    "    print(model)\n",
    "    \n",
    "    train(model, data, epochs, train_neighbors)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, dataset = get_data(DATASET_NAME)\n",
    "# GSAGEperformance(data, dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708, 7], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "===========================================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "SAGE(\n",
      "  (convs): ModuleList(\n",
      "    (0): SAGEConv(1433, 256)\n",
      "    (1): SAGEConv(256, 7)\n",
      "  )\n",
      ")\n",
      "Train neighbors:  [25, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01: 100%|██████████| 140/140 [00:00<00:00, 468.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.3721938775510207\n",
      "Epoch: 001, Training Loss: 0.4116, Training Accuracy: 0.1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 140/140 [00:00<00:00, 6369.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.6002551020408166\n",
      "Epoch: 002, Training Loss: 0.4089, Training Accuracy: 0.4286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 140/140 [00:00<00:00, 6422.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.831190476190476\n",
      "Epoch: 003, Training Loss: 0.4060, Training Accuracy: 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 140/140 [00:00<00:00, 6421.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9357142857142855\n",
      "Epoch: 004, Training Loss: 0.4023, Training Accuracy: 0.8857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 140/140 [00:00<00:00, 6609.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9299999999999999\n",
      "Epoch: 005, Training Loss: 0.3971, Training Accuracy: 0.8786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 7740.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.3971, Train: 0.9500, Val: 0.7100, Test: 0.6930\n",
      "Epoch: 005, Mult-Train: 0.9750, Mult-Val: 0.8208, Mult-Test: 0.8141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 140/140 [00:00<00:00, 5874.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9505952380952379\n",
      "Epoch: 006, Training Loss: 0.3920, Training Accuracy: 0.9143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 140/140 [00:00<00:00, 6161.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9678571428571429\n",
      "Epoch: 007, Training Loss: 0.3842, Training Accuracy: 0.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 140/140 [00:00<00:00, 6363.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9714285714285714\n",
      "Epoch: 008, Training Loss: 0.3761, Training Accuracy: 0.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 140/140 [00:00<00:00, 6585.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9726190476190477\n",
      "Epoch: 009, Training Loss: 0.3673, Training Accuracy: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 140/140 [00:00<00:00, 6572.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9559523809523808\n",
      "Epoch: 010, Training Loss: 0.3584, Training Accuracy: 0.9214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 7991.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.3584, Train: 0.9929, Val: 0.7440, Test: 0.7590\n",
      "Epoch: 010, Mult-Train: 0.9964, Mult-Val: 0.8439, Mult-Test: 0.8559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 140/140 [00:00<00:00, 6361.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9738095238095237\n",
      "Epoch: 011, Training Loss: 0.3451, Training Accuracy: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 140/140 [00:00<00:00, 6707.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.975\n",
      "Epoch: 012, Training Loss: 0.3341, Training Accuracy: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 140/140 [00:00<00:00, 6724.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9595238095238096\n",
      "Epoch: 013, Training Loss: 0.3218, Training Accuracy: 0.9214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 140/140 [00:00<00:00, 6673.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9845238095238094\n",
      "Epoch: 014, Training Loss: 0.3063, Training Accuracy: 0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 140/140 [00:00<00:00, 6650.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9791666666666665\n",
      "Epoch: 015, Training Loss: 0.2945, Training Accuracy: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 7960.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss: 0.2945, Train: 0.9929, Val: 0.7620, Test: 0.7740\n",
      "Epoch: 015, Mult-Train: 0.9964, Mult-Val: 0.8537, Mult-Test: 0.8648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 140/140 [00:00<00:00, 6251.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9797619047619047\n",
      "Epoch: 016, Training Loss: 0.2791, Training Accuracy: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 140/140 [00:00<00:00, 6606.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9964285714285714\n",
      "Epoch: 017, Training Loss: 0.2653, Training Accuracy: 0.9929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 140/140 [00:00<00:00, 6642.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9809523809523808\n",
      "Epoch: 018, Training Loss: 0.2546, Training Accuracy: 0.9643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 140/140 [00:00<00:00, 6521.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9857142857142858\n",
      "Epoch: 019, Training Loss: 0.2411, Training Accuracy: 0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 140/140 [00:00<00:00, 6634.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Accuracy:  0.9892857142857143\n",
      "Epoch: 020, Training Loss: 0.2279, Training Accuracy: 0.9786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5416/5416 [00:00<00:00, 7554.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.2279, Train: 0.9929, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 020, Mult-Train: 0.9964, Mult-Val: 0.8608, Mult-Test: 0.8700\n",
      "Best Test Accuracy,  0.782\n",
      "Best Test Sk Accuracy,  0.8700357142857149\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    data, dataset = get_data(DATASET_NAME)    \n",
    "    GSAGEperformance(data, dataset, epochs=20)\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = torch.empty(3, 5).uniform_(0, 1) \n",
    "# y=torch.bernoulli(out)\n",
    "\n",
    "# print(out)\n",
    "# print(y)\n",
    "\n",
    "# sort_out = torch.sort(out,dim=1, descending=True)\n",
    "# sort_out = sort_out.values\n",
    "# print(sort_out)\n",
    "# result = sort_out[:, :-1] - sort_out[:, 1:]\n",
    "# print(result)\n",
    "\n",
    "# torch.max(result, dim=1)\n",
    "\n",
    "# tolerance=0.01\n",
    "# threshold = torch.max(out,dim=1).values.view(-1,1)-tolerance\n",
    "# print(threshold)\n",
    "\n",
    "# (out>=threshold).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My py38cu11 Kernel)",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
