{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44415f5a",
   "metadata": {},
   "source": [
    "## Get Cuda and Processor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ac319c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../Submodular')\n",
    "\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()\n",
    "\n",
    "NUM_PROCESSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "413bed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cpu count:  32\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "782d8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data,generate_synthetic\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d11e7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--recompute', type=bool, default=False)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d0c7f",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ae0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SYNTHETIC = True\n",
    "seed = 123\n",
    "\n",
    "data_filename_extension = \"\"\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f2deccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f832",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbd4df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, ChebConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from ipynb.fs.full.SpatialConv import SpatialConv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007e8b8",
   "metadata": {},
   "source": [
    "### GNN option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5744498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNNconv = SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ac584ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import GCNConv, GATConv, GINConv, SAGEConv\n",
    "GNNconv2 = GATConv\n",
    "\n",
    "class GNNother(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=256):\n",
    "        super().__init__()        \n",
    "        ##GNN layer\n",
    "        global GNNconv2\n",
    "        \n",
    "        if(GNNconv2==GINConv):\n",
    "            self.MLP1 = nn.Linear(num_features,hidden_channels)\n",
    "            self.MLP2 = nn.Linear(hidden_channels,num_classes)\n",
    "            self.conv1 = GNNconv2(self.MLP1)\n",
    "            self.conv2 = GNNconv2(self.MLP2)                \n",
    "        else:        \n",
    "            self.conv1 = GNNconv2(num_features, hidden_channels)\n",
    "            self.conv2 = GNNconv2(hidden_channels,num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        \n",
    "        #x = x.log_softmax(dim=-1)\n",
    "        #x = x.relu()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class GNNGAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads, edge_dim=1)  # TODO\n",
    "        self.conv2 = GATConv(hidden_channels*heads, num_classes, heads=1, concat=True, edge_dim=1)  # TODO\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd0e048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNHomophily(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "             \n",
    "        self.conv1 = GNNconv(num_features, hidden_channels)\n",
    "        #self.conv2 = GNNconv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = GNNconv(hidden_channels,num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GNNHeterophily(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "             \n",
    "        self.conv1 = ChebConv(num_features, hidden_channels, K=2, normalization='sym')\n",
    "        #self.conv2 = GNNconv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = ChebConv(hidden_channels,num_classes, K=2, normalization='sym')\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "#         x = self.conv2(x, edge_index, edge_weight)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class AGSGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=16, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        hidden = int(hidden_channels/2)        \n",
    "        \n",
    "        ####################\n",
    "#         self.gnn1 = GNNHomophily(num_features, hidden, hidden_channels)\n",
    "# #         self.gnn2 = GNNHomophily(num_features, hidden, hidden_channels)\n",
    "        \n",
    "# #         self.gnn1 = GNNHeterophily(num_features, hidden, hidden_channels)\n",
    "#         self.gnn2 = GNNHeterophily(num_features, hidden, hidden_channels)\n",
    "        \n",
    "#         ####################\n",
    "#         self.gnn1 = GNNother(num_features, hidden, hidden_channels)\n",
    "#         self.gnn2 = GNNother(num_features, hidden, hidden_channels)        \n",
    "\n",
    "#         ####################\n",
    "        self.gnn1 = GNNHomophily(num_features, num_classes, hidden_channels)\n",
    "#         self.gnn1 = GNNHeterophily(num_features, num_classes, hidden_channels)\n",
    "        #################\n",
    "    \n",
    "        self.p = dropout\n",
    "        self.com_lin = nn.Linear(hidden*2, num_classes)\n",
    "        \n",
    "        \n",
    "#         self.T = 2        \n",
    "#         self.layer_norm_a1 =  nn.LayerNorm(num_classes)\n",
    "#         self.layer_norm_s1 =  nn.LayerNorm(num_classes)\n",
    "        \n",
    "#         self.alpha_a1 = nn.Linear(num_classes, 1)\n",
    "#         self.alpha_s1 = nn.Linear(num_classes, 1)\n",
    "#         self.w1 = nn.Linear(self.T, self.T)\n",
    "        \n",
    "        #self.reset_parameters()\n",
    "            \n",
    "#     def reset_parameters(self):\n",
    "#         std_att = 1. / math.sqrt(self.w1.weight.size(1))\n",
    "#         std_att_vec = 1. / math.sqrt( self.alpha_a1.weight.size(1))\n",
    "        \n",
    "#         self.alpha_s1.weight.data.uniform_(-std_att, std_att)\n",
    "#         self.alpha_i1.weight.data.uniform_(-std_att, std_att)\n",
    "        \n",
    "#         self.layer_norm_a1.reset_parameters()\n",
    "#         self.layer_norm_s1.reset_parameters()        \n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "        #out = model(batch_data.x, batch_data.edge_index, batch_data.weight)\n",
    "        #out = model(batch_data.x, batch_data.edge_index, batch_data.edge_weight)\n",
    "        #out = model(batch_data.x, batch_data.edge_index)\n",
    "        \n",
    "        x1 = self.gnn1(batch_data[0].x, batch_data[0].edge_index)\n",
    "        return x1        \n",
    "        \n",
    "        x2 = self.gnn2(batch_data[1].x, batch_data[1].edge_index)\n",
    "        #return x2\n",
    "        \n",
    "        a1 = F.relu(x1)\n",
    "        #a1 = self.layer_norm_a1(a1)\n",
    "        a1 = F.dropout(a1, p=self.p, training=self.training)\n",
    "        \n",
    "        s1 = F.relu(x2)\n",
    "        #s1 = self.layer_norm_s1(s1)\n",
    "        s1 = F.dropout(s1, p=self.p, training=self.training)\n",
    "        \n",
    "        used = batch_data[0].batch_size\n",
    "        \n",
    "        x = torch.cat([a1[:used,:], s1[:used,:]], dim=-1)\n",
    "        x = self.com_lin(x)\n",
    "        \n",
    "        \n",
    "#         ala1 = torch.sigmoid(self.alpha_a1(a1))\n",
    "#         als1 = torch.sigmoid(self.alpha_s1(s1))        \n",
    "        \n",
    "#         alpha1 = F.softmax(self.w1(torch.cat([ala1, als1],dim=-1)/self.T), dim=1)                \n",
    "#         x = torch.mm(torch.diag(alpha1[:,0]),a1) + torch.mm(torch.diag(alpha1[:,1]),s1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b17b8e",
   "metadata": {},
   "source": [
    "## GNN Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e11e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborSampler, NeighborLoader\n",
    "from ipynb.fs.full.AGSNodeSampler import WeightedNeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ac415e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):    \n",
    "    if args.log_info:    \n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()    \n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "            batch_data = [b.to(device) for b in batch_data]\n",
    "            used = batch_data[0].batch_size\n",
    "            \n",
    "            out = model(batch_data)\n",
    "                   \n",
    "            out=out[:used,:]\n",
    "            pred = out.argmax(dim=1)            \n",
    "\n",
    "            y_true.append(batch_data[0].y[:used].detach().cpu().numpy())\n",
    "            y_pred.append(pred.detach().cpu().numpy())\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(used)\n",
    "              \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    #acc = f1_score(y_true, y_pred, average='micro')\n",
    "                    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85fa0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(DATASET_NAME, model, data, epochs=100, train_neighbors=[-1,10], test_neighbors=[-1,10]):\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "        print(\"Test neighbors: \", test_neighbors)\n",
    "        \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    if data.y.ndim == 1:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    batch_size=1024         \n",
    "#     batch_size=512         \n",
    "    worker = 8\n",
    "    \n",
    "    if data.num_nodes>=50000:\n",
    "        worker = 8\n",
    "#     else:\n",
    "#         worker = min(8,int(sum(data.train_mask)/batch_size))\n",
    "        \n",
    "    if args.log_info:\n",
    "        print(\"Worker: \", worker)\n",
    "        \n",
    "    weight_func=['knn','random']; \n",
    "#     weight_func=['knn','submodular']; \n",
    "#     weight_func=['random', 'random'];  worker = 0;\n",
    "#     weight_func=['link-nn', 'link-sub'];  worker = 2;\n",
    "    params={\n",
    "        'knn':{'metric':'cosine'},\n",
    "        'submodular':{'metric':'cosine'},\n",
    "        'link-nn':{'value':'min'},\n",
    "        'link-sub':{'value':'max'},\n",
    "        'apricot':{'sub_func':'coverage','metric':'cosine'}\n",
    "    }    \n",
    "    \n",
    "    global data_filename_extension        \n",
    "#     sampler_dir = DIR+'AGSGNNstruc/'+DATASET_NAME+data_filename_extension\n",
    "    sampler_dir = DIR+'AGSGNNstrucCorrect/'+DATASET_NAME+data_filename_extension\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(sampler_dir)\n",
    "    \n",
    "    \n",
    "#     if not os.path.exists(sampler_dir):\n",
    "#         os.makedirs(sampler_dir)\n",
    "    \n",
    "    start = time.time()    \n",
    "#     loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "#                               batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False, \n",
    "#                               weight_func=weight_func, params=params, log=args.log_info,\n",
    "#                                     directed=True, replace = False,\n",
    "#                                     save_dir = sampler_dir,recompute = args.recompute)\n",
    "    \n",
    "    loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=worker, drop_last=False, \n",
    "                              weight_func=weight_func, params=params, log=True,\n",
    "                                    directed=True, replace = False,\n",
    "                                    save_dir = sampler_dir,recompute = args.recompute)\n",
    "\n",
    "    train_loader = WeightedNeighborLoader(data, input_nodes=data.train_mask,num_neighbors=train_neighbors, \n",
    "                              batch_size=batch_size, shuffle=False, num_workers=worker, drop_last=False, \n",
    "                              weight_func=weight_func, params=params, log=args.log_info,\n",
    "                                          directed=True, replace = False,\n",
    "                                          save_dir = sampler_dir,recompute = False)\n",
    "    \n",
    "    val_loader = WeightedNeighborLoader(data, input_nodes=data.val_mask,num_neighbors=test_neighbors, \n",
    "                              batch_size=batch_size, shuffle=False, num_workers=min(8,int(sum(data.val_mask)/batch_size)), drop_last=False, \n",
    "                              weight_func=weight_func, params=params,log=args.log_info, directed=True, replace = False,\n",
    "                                        save_dir = sampler_dir,recompute = False)\n",
    "    \n",
    "    test_loader = WeightedNeighborLoader(data, input_nodes=data.test_mask,num_neighbors=test_neighbors, \n",
    "                              batch_size=batch_size, shuffle=False, num_workers=min(8,int(sum(data.test_mask)/batch_size)), drop_last=False, \n",
    "                              weight_func=weight_func, params=params, log=args.log_info, directed=True, replace = False,\n",
    "                                         save_dir = sampler_dir,recompute = False)\n",
    "    \n",
    "    top_k_accs = []    \n",
    "    best_acc=0  \n",
    "    \n",
    "    train_losses=[]\n",
    "    val_accuracies=[]\n",
    "    train_accuracies=[]\n",
    "    test_accuracies=[]\n",
    "    \n",
    "    num_iteration = epochs\n",
    "    \n",
    "    end = time.time()\n",
    "    if args.log_info:\n",
    "        print(\"Total initialization time: \", end-start)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=int(sum(data.train_mask)))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = total_examples = 0\n",
    "        \n",
    "        for i,batch_data in enumerate(loader):            \n",
    "            #print(batch_data)\n",
    "            \n",
    "            batch_data = [b.to(device) for b in batch_data]\n",
    "            used = batch_data[0].batch_size #int(sum(batch_data.train_mask))       \n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            out = model(batch_data)\n",
    "            #out = F.log_softmax(out, dim=1)                 \n",
    "            #loss = F.nll_loss(out[batch_data[0].train_mask], batch_data[0].y[batch_data[0].train_mask])\n",
    "            #loss = F.cross_entropy(out[:used], batch_data[0].y[:used])\n",
    "            loss = criterion(out[:used], batch_data[0].y[:used])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            total_loss += loss.item() * used\n",
    "            total_examples += used\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(used)\n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "        \n",
    "        loss=total_loss / total_examples\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        #print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}', end = ', ')                \n",
    "        \n",
    "        if args.log_info:\n",
    "            train_acc=test(model, train_loader,data.train_mask,'Train')            \n",
    "            train_accuracies.append(train_acc.item())        \n",
    "        else:\n",
    "            train_acc = 0 ; train_accuracies.append(train_acc)\n",
    "        \n",
    "        if args.log_info:\n",
    "            val_acc = test(model, val_loader,data.val_mask,'Validation')\n",
    "            val_accuracies.append(val_acc.item())\n",
    "        else:\n",
    "            val_acc = 0 ; val_accuracies.append(val_acc)\n",
    "    \n",
    "        if epoch%10==0:\n",
    "            test_acc = test(model, test_loader,data.test_mask,'Test')\n",
    "            test_accuracies.append(test_acc.item())\n",
    "        else:\n",
    "            test_acc = 0\n",
    "            test_accuracies.append(test_acc)\n",
    "            \n",
    "        \n",
    "        #print(f'Epoch: {epoch:03d}, Test: {test_acc:.4f}')\n",
    "        \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        #print(f'Epoch: {epoch:03d}, Std dev: {std_dev:.4f}')\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "        if epoch>=5 and std_dev<=1e-3:\n",
    "            num_iteration = epoch\n",
    "            \n",
    "            if args.log_info:                \n",
    "                print(\"Iteration for convergence: \", epoch)\n",
    "            break\n",
    "        \n",
    "    if args.log_info:\n",
    "        #save_plot([val_accuracies], labels=['Validation'], name='Plots/Validation', yname='Accuracy', xname='Epoch')    \n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Results/AGSNSVal', yname='Accuracy', xname='Epoch')\n",
    "        \n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "        \n",
    "    best_acc = max(test_accuracies)\n",
    "    \n",
    "    end = time.time()\n",
    "    if args.log_info:\n",
    "        print(\"Total epoch time: \", end-start)    \n",
    "    \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34bee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=1, train_neighbors=[-1,-1], test_neighbors=[-1,-1]):        \n",
    "    \n",
    "    model = AGSGNN(data.x.shape[1], num_classes, hidden_channels=256).to(device)\n",
    "    \n",
    "    if args.log_info: print(model)    \n",
    "    \n",
    "    best_acc, num_iteration = train(DATASET_NAME, model, data, epochs, train_neighbors=train_neighbors, test_neighbors=test_neighbors)    \n",
    "    \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9140fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_feature(data):    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    edges = data.edge_index.t()\n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    return adj_mat\n",
    "\n",
    "# adj_feature(data)\n",
    "# data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e76eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import add_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "586728b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# args.log_info = True\n",
    "# DATASET_NAME = 'Cora'\n",
    "# data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=1); print(\"\")\n",
    "# print(data)\n",
    "\n",
    "# # (row, col) = data.edge_index\n",
    "# # data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "# # data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "# # print(data)\n",
    "\n",
    "# args.recompute = True\n",
    "\n",
    "\n",
    "# if len(data.y.shape) > 1:\n",
    "#     data.y = data.y.argmax(dim=1)        \n",
    "#     num_classes = torch.max(data.y).item()+1\n",
    "# else:\n",
    "#     num_classes = dataset.num_classes\n",
    "\n",
    "# if num_classes!= torch.max(data.y)+1:\n",
    "#     num_classes = torch.max(data.y).item()+1\n",
    "    \n",
    "# # data.edge_index, _ = add_self_loops(data.edge_index)            \n",
    "# # data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "# # if args.log_info == True:\n",
    "# #     print(data.x.shape)\n",
    "\n",
    "    \n",
    "# # if DATASET_NAME in ['Cornell', 'cornell5']:\n",
    "# #     data.edge_index, _ = add_self_loops(data.edge_index)            \n",
    "    \n",
    "# # if DATASET_NAME in ['Squirrel', 'Chameleon', 'amherst41',\n",
    "# #                     'Cornell','cornell5', 'johnshopkins55']:\n",
    "# #     data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "# #     if args.log_info == True:\n",
    "# #         print(data.x.shape)\n",
    "\n",
    "\n",
    "# best_acc, num_iteration, _ =  AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=150, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f89d3",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7a2677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        \"Cornell\",\"Texas\",\"Wisconsin\",\n",
    "        \"reed98\",\"amherst41\",\n",
    "        \"penn94\",\"Roman-empire\",\"cornell5\",\"Squirrel\",\"johnshopkins55\",\n",
    "        \"AmazonProducts\",\n",
    "        \"Actor\",\"Minesweeper\",\"Questions\",\"Chameleon\",\n",
    "        \"Tolokers\",\"Flickr\",\n",
    "        \"Yelp\",\"Amazon-ratings\",\"genius\",\"cora\",\"CiteSeer\",\n",
    "        \"dblp\",\"Computers\",\"PubMed\",\"pubmed\",\"Reddit\",\n",
    "        \"cora_ml\",\"Cora\",\"Reddit2\",\"CS\",\"Photo\",\"Physics\",\"citeseer\"\n",
    "    ]     \n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        'cornell5',\n",
    "    ]\n",
    "\n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/AGSGNN-NS-2.txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "            #optional for making undirected graph\n",
    "            (row, col) = data.edge_index\n",
    "            data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "            data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 20\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon','cornell5','penn94','johnshopkins55'\n",
    "                               \"amherst41\"]:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "                              \n",
    "            accuracy, itr, _ = AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cab8c",
   "metadata": {},
   "source": [
    "## View Learned Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "361d4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':    \n",
    "    \n",
    "#     n=7\n",
    "#     x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "#     y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "#     edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "#     edge_index = edge_index-1\n",
    "    \n",
    "#     mask = torch.zeros(n, dtype=torch.bool)\n",
    "#     mask[[1,3]] = True\n",
    "    \n",
    "#     test_data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = mask, val_mask = mask)    \n",
    "#     print(test_data)\n",
    "    \n",
    "    \n",
    "#     None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83ca9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48f76e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# #X = model(data.x.to(device),data.edge_index.to(device), data.weight.to(device))\n",
    "# X = model(data.x.to(device),data.edge_index.to(device))\n",
    "# X = X.detach().to('cpu')\n",
    "# y = data.y.to('cpu')\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1e750fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Create a t-SNE model with 2 components and a perplexity of 30\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=42, learning_rate='auto', init='random')\n",
    "\n",
    "# # Fit and transform the data to the 2D t-SNE space\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# # Plot the data in the 2D t-SNE space, colored by class\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6a5b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sparsify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc7c75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.SubmodularWeights import SubModularWeightFacilityFaster\n",
    "from ipynb.fs.full.KNNWeights import KNNWeight\n",
    "# from ipynb.fs.full.PretrainedLink import LinkPred, LinkNN, LinkSub\n",
    "from ipynb.fs.full.PretrainedLinkFast import get_link_weight, LinkNN, LinkSub\n",
    "from ipynb.fs.full.RandomSparse import RandomSparse\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import copy\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a64131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify(data, log = True, method = 'NN', metric= None):\n",
    "    data.to('cpu')    \n",
    "    \n",
    "    if metric is None:\n",
    "        metric = 'cosine'\n",
    "    \n",
    "    if method == 'nn':\n",
    "        submodular_weight = KNNWeight(data, metric=metric, log=log)                \n",
    "        data.weight = submodular_weight.compute_weights()        \n",
    "\n",
    "    elif method == 'submodular':\n",
    "        submodular_weight = SubModularWeightFacilityFaster(data, metric=metric, log=log)\n",
    "        data.weight = submodular_weight.compute_weights()        \n",
    "    \n",
    "    elif method == 'link-nn':    \n",
    "        submodular_weight = LinkPred(data, selfloop = True, log=log)\n",
    "        data.weight = submodular_weight.compute_weights()        \n",
    "        nn_weight = LinkNN(data, value='min', log=log) #min favor similar ones, max disimilar\n",
    "        data.weight = nn_weight.compute_weights()\n",
    "    elif method == 'link-sub':    \n",
    "        nn_weight = LinkSub(data, value='max', selfloop = True, log=log) #min favor similar ones, max disimilar    \n",
    "        data.weight = nn_weight.compute_weights()\n",
    "    else:\n",
    "        raise 'Not implemented error'\n",
    "    \n",
    "    cp_data= copy.deepcopy(data)\n",
    "    G = to_networkx(cp_data, to_undirected=False, edge_attrs=['weight'])\n",
    "    to_remove = [(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] < 0.7 ]\n",
    "    G.remove_edges_from(to_remove)\n",
    "    updated_data = from_networkx(G)\n",
    "    \n",
    "    updated_data = from_networkx(G, group_edge_attrs=['weight'])\n",
    "    updated_data.weight = updated_data.edge_attr.view(-1)\n",
    "\n",
    "    row, col = updated_data.edge_index\n",
    "    updated_data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "    updated_data.weight = torch.cat((updated_data.weight, updated_data.weight),dim=0)\n",
    "\n",
    "    \n",
    "    #if args.log_info:\n",
    "    if True:\n",
    "        print(updated_data)\n",
    "        print(\"Node Homophily:\", homophily(updated_data.edge_index, data.y, method='node'))\n",
    "        print(\"Edge Homophily:\", homophily(updated_data.edge_index, data.y, method='edge'))\n",
    "        print(\"Edge_insensitive Homophily:\", homophily(updated_data.edge_index, data.y, method='edge_insensitive'))    \n",
    "        print(\"Degree: \", updated_data.num_edges / updated_data.num_nodes)\n",
    "\n",
    "    data.edge_index = updated_data.edge_index\n",
    "    data.edge_weight = updated_data.weight\n",
    "    data.weight = None\n",
    "\n",
    "    return data\n",
    "\n",
    "# LOG_INFO = True\n",
    "# data = sparsify(data, log = False)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "295e15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sparsify(data, K, log = False):    \n",
    "    rand_sparse = RandomSparse(data, K = K, log = log)\n",
    "    edge_index = rand_sparse.sparse()\n",
    "    row, col = edge_index\n",
    "    data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "        print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "        print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "        print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfb29939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_homophily(data, h = 0.1, d = 11, log = False):\n",
    "    data.to('cpu')\n",
    "    N = data.num_nodes\n",
    "    E = data.num_edges\n",
    "    adj = SparseTensor(\n",
    "        row=data.edge_index[0], col=data.edge_index[1],\n",
    "        value=torch.arange(E, device=data.edge_index.device),\n",
    "        sparse_sizes=(N, N))\n",
    "    \n",
    "    edge_index=[]\n",
    "    \n",
    "#     h = 0.1\n",
    "#     d = 11\n",
    "\n",
    "    match = int(round(d*h))\n",
    "    unmatch = int(round(d*(1-h)))\n",
    "    #print(match,unmatch)\n",
    "    \n",
    "    for u in range(N):                \n",
    "        row, col, e_index = adj[u,:].coo()   \n",
    "        \n",
    "        cur_y = data.y[u]\n",
    "        neighbors = data.y[col]\n",
    "        #print(cur_y, neighbors)\n",
    "        \n",
    "        match_indexs = torch.nonzero(neighbors == cur_y).squeeze()\n",
    "        other_indexs = torch.nonzero(neighbors != cur_y).squeeze()\n",
    "        \n",
    "        #print(match_indexs, other_indexs)\n",
    "        \n",
    "        if match_indexs.dim()>0:\n",
    "            m_sel = match_indexs[np.random.choice(len(match_indexs), size=min(match,len(match_indexs)), replace = False)]\n",
    "        else:\n",
    "            m_sel = torch.LongTensor([])\n",
    "        if other_indexs.dim()>0:\n",
    "            um_sel = other_indexs[np.random.choice(len(other_indexs), size=min(unmatch, len(other_indexs)), replace = False)]\n",
    "        else:\n",
    "            um_sel = torch.LongTensor([])\n",
    "            \n",
    "        \n",
    "        #print(m_sel, um_sel)\n",
    "        \n",
    "        indexs = torch.cat((m_sel,um_sel),dim=0)\n",
    "    \n",
    "        e_index = e_index[indexs]            \n",
    "        edge_index.extend(e_index)\n",
    "        \n",
    "        #break        \n",
    "            \n",
    "    edge_index = data.edge_index[:,edge_index]\n",
    "    row, col = edge_index\n",
    "    data.edge_index = torch.stack((torch.cat((row, col),dim=0), torch.cat((col, row),dim=0)),dim=0)\n",
    "    \n",
    "    if log:\n",
    "        print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "        print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "        print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "        print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# data = modify_homophily(data, h=0.15, d=11, log = True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df9032d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hetero():\n",
    "    d = 42\n",
    "    for h in np.array(range(0,21))/20:\n",
    "        DATASET_NAME = 'squirrel'\n",
    "        data, dataset = get_data(DATASET_NAME, log=False)\n",
    "        data = generate_synthetic(data, d=d, h = h, train=0.6, random_state=1, log=False, balance = False)\n",
    "        num_classes = dataset.num_classes\n",
    "        \n",
    "        print('d ', d, ' h', h, end=' ')\n",
    "        count, score = test_uniformity(data, num_classes, log=False)\n",
    "        print(count, score, end = ' ')\n",
    "        total_en, en_score = total_entropy(data, num_classes, log=False)\n",
    "        print(total_en, en_score, end = ' ')\n",
    "        \n",
    "        print('sparse', end = ' ')\n",
    "        data = sparsify(data, log=False)\n",
    "        \n",
    "        count, score = test_uniformity(data, num_classes, log=False)\n",
    "        print(count, score, end = ' ')\n",
    "        total_en, en_score = total_entropy(data, num_classes, log=False)\n",
    "        print(total_en, en_score, end = ' ')\n",
    "        \n",
    "        print(\"Nh \", homophily(data.edge_index, data.y, method='node'), end = ' ')\n",
    "        print(\"Eh \", homophily(data.edge_index, data.y, method='edge'), end = ' ')\n",
    "        print(\"EiH \", homophily(data.edge_index, data.y, method='edge_insensitive'), end = ' ')    \n",
    "        \n",
    "#         print(\"Ha \", agg_homophily(data, 'affinity'), end = ' ')\n",
    "#         print(\"Hl \", agg_homophily(data, 'laplacian'), end =' ')\n",
    "        \n",
    "        print(\"D \", data.num_edges / data.num_nodes, end = '\\n')\n",
    "\n",
    "\n",
    "# LOG_INFO = False\n",
    "# test_hetero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "505da1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import generate_synthetic2homophily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f746ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reed98 - 10 Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "[0.6528497409326425, 0.6528497409326425, 0.6424870466321243] [107, 117, 195]\n",
      "reed98 - 10 acc 64.9396 sd 0.4885 itr 139 sd 39\n",
      "amherst41 - 10 Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "[0.7762863534675615, 0.7785234899328859, 0.785234899328859] [61, 59, 50]\n",
      "amherst41 - 10 acc 78.0015 sd 0.3802 itr 56 sd 4\n",
      "cornell5 - 10 Metric:  cosine\n",
      "Pool Size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 18660/18660 [00:22<00:00, 843.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  knncosine\n",
      "Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "[0.7722400857449089, 0.7805466237942122, 0.7751875669882101] [28, 48, 40]\n",
      "cornell5 - 10 acc 77.5991 sd 0.3438 itr 38 sd 8\n",
      "Squirrel - 10 Metric:  cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 5201/5201 [00:07<00:00, 716.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  knncosine\n",
      "Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "[0.6839577329490875, 0.6791546589817483, 0.654178674351585] [76, 82, 87]\n",
      "Squirrel - 10 acc 67.2430 sd 1.3054 itr 81 sd 4\n",
      "johnshopkins55 - 10 Metric:  cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 5180/5180 [00:07<00:00, 663.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  knncosine\n",
      "Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "[0.7847490347490348, 0.7722007722007722, 0.7673745173745173] [59, 50, 56]\n",
      "johnshopkins55 - 10 acc 77.4775 sd 0.7323 itr 55 sd 3\n",
      "Chameleon - 10 Metric:  cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nodes: 100%|██████████| 2277/2277 [00:01<00:00, 1696.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights  knncosine\n",
      "Loading weights  knncosine\n",
      "Loading weights  knncosine\n",
      "[0.7192982456140351, 0.7456140350877193, 0.7039473684210527] [134, 130, 141]\n",
      "Chameleon - 10 acc 72.2953 sd 1.7206 itr 135 sd 4\n",
      "Runtime:  2593.0342602729797\n"
     ]
    }
   ],
   "source": [
    "def ablation(num_run = 1):\n",
    "    \n",
    "    #SYN_NAME = random.randint(0,1000)\n",
    "\n",
    "#     ALL_DATASETs= [\n",
    "#         'Wisconsin',\n",
    "#         'reed98',        \n",
    "#         'Roman-empire',\n",
    "#         'Actor',\n",
    "#         'Minesweeper',        \n",
    "#         'Tolokers'\n",
    "#     ]\n",
    "\n",
    "    ALL_DATASETs= [\n",
    "        \"reed98\",\n",
    "        \"amherst41\",\n",
    "#         \"penn94\",\n",
    "        \"cornell5\",\n",
    "        \"Squirrel\",\n",
    "        \"johnshopkins55\",\n",
    "        \"Chameleon\",\n",
    "#         \"Tolokers\",\n",
    "#         \"Flickr\",\n",
    "        \n",
    "#         \"Computers\",\n",
    "#         \"Photo\",\n",
    "#         \"Physics\",\n",
    "        \n",
    "#         \"AmazonProducts\",\n",
    "#         \"Yelp\",\n",
    "#         'pokec',\n",
    "#         'twitch-gamer',\n",
    "#         'wiki',        \n",
    "        \n",
    "#         \"Reddit\",\n",
    "#         \"Reddit2\",\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "#     ALL_DATASETs= [\"Cora\"]\n",
    "    \n",
    "    args.log_info = False    \n",
    "    \n",
    "    filename = \"Results/AGSGNN-NS-2Ablation.txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        \n",
    "        random_state = 10\n",
    "        #args.recompute = True\n",
    "        \n",
    "        print(DATASET_NAME,\"-\",random_state, end=' ')\n",
    "        \n",
    "        \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "            d = 100\n",
    "            h =0.50\n",
    "            train=0.1\n",
    "            balance=True\n",
    "            h2 = 0.25\n",
    "            ratio = 0.50\n",
    "                                    \n",
    "#             global data_filename_extension\n",
    "#             data_filename_extension = str(d)+str(h)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "#             data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "#             if os.path.exists(data_filename):\n",
    "#                 data = torch.load(data_filename)                \n",
    "#                 print(\"loaded \"+data_filename)\n",
    "#             else:\n",
    "#                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False, balance=balance)\n",
    "# #                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False)\n",
    "#                 torch.save(data,data_filename)\n",
    "#                 print(\"saved \"+data_filename)\n",
    "        \n",
    "#             global data_filename_extension\n",
    "#             data_filename_extension = str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "#             data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "#             if os.path.exists(data_filename):\n",
    "#                 data = torch.load(data_filename)                \n",
    "#                 print(\"loaded \"+data_filename)\n",
    "#             else:\n",
    "#                 data = generate_synthetic2homophily(data, d=d, h1=h, h2=h2, ratio=ratio, train=train, random_state=random_state, log=False, balance=balance)                 \n",
    "#                 torch.save(data,data_filename)\n",
    "#                 print(\"saved \"+data_filename)\n",
    "    \n",
    "            ##Sparsifiy\n",
    "            #data = random_sparsify(data, 13, log = True)\n",
    "#             data = sparsify(data, log = True, method = 'submodular', metric= 'cosine')\n",
    "                        \n",
    "#             data1 = sparsify(copy.deepcopy(data), log = True, method = 'submodular', metric= 'cosine')\n",
    "#             data = sparsify(data, log = True, method = 'nn', metric= 'cosine')                         \n",
    "#             data.edge_index = torch.cat((data.edge_index, data1.edge_index), dim=1)\n",
    "            \n",
    "            #optional for making undirected graph\n",
    "            (row, col) = data.edge_index\n",
    "            data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "            data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "            \n",
    "            if args.log_info:\n",
    "                print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "                print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "                print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "                print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 500\n",
    "            else:\n",
    "                max_epochs = 20\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon','cornell5','penn94','johnshopkins55','amherst41']:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "\n",
    "#             accuracy, itr = 0,0\n",
    "            \n",
    "#             accuracy, itr, mdl = AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs, train_neighbors=[25,25], test_neighbors=[25,25])        \n",
    "            accuracy, itr, mdl = AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])            \n",
    "#             accuracy, itr, mdl = AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs, train_neighbors=[4,4], test_neighbors=[4,4])            \n",
    "#             accuracy, itr, mdl = AGSNSperformanceSampler(DATASET_NAME, data, dataset, num_classes, epochs=max_epochs, train_neighbors=[-1,-1], test_neighbors=[-1,-1])\n",
    "            \n",
    "            #print(mdl)\n",
    "            #args.recompute = False\n",
    "    \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        print(accs, itrs)\n",
    "        print(DATASET_NAME,\"-\",random_state, end=' ')\n",
    "        print(f'acc {np.mean(accs)*100:0.4f} sd {np.std(accs)*100:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs)*100:0.4f} sd {np.std(accs)*10:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "    return \n",
    "\n",
    "st_time = time.time()\n",
    "ablation(num_run=3)\n",
    "en_time = time.time()\n",
    "\n",
    "print(\"Runtime: \", en_time-st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ca0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef0592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
