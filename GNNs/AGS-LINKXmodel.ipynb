{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03da4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc906b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f3ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0509ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "# from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8186ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" adapted from https://github.com/CUAI/CorrectAndSmooth/blob/master/gen_models.py \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout=.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        if num_layers == 1:\n",
    "            # just linear layer i.e. logistic regression\n",
    "            self.lins.append(nn.Linear(in_channels, out_channels))\n",
    "        else:\n",
    "            self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
    "            self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, b_data, input_tensor=False):\n",
    "        if not input_tensor:\n",
    "            x = b_data.x.shape[1]\n",
    "        else:\n",
    "            x = b_data\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = F.relu(x, inplace=True)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cedd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINKX(nn.Module):\t\n",
    "    \"\"\" our LINKX method with skip connections \n",
    "        a = MLP_1(A), x = MLP_2(X), MLP_3(sigma(W_1[a, x] + a + x))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, num_nodes, dropout=.5, cache=False, inner_activation=False, inner_dropout=False, init_layers_A=1, init_layers_X=1):\n",
    "        super(LINKX, self).__init__()\t\n",
    "        self.mlpA = MLP(num_nodes, hidden_channels, hidden_channels, init_layers_A, dropout=0)\n",
    "        self.mlpX = MLP(in_channels, hidden_channels, hidden_channels, init_layers_X, dropout=0)\n",
    "        self.W = nn.Linear(2*hidden_channels, hidden_channels)\n",
    "        self.mlp_final = MLP(hidden_channels, hidden_channels, out_channels, num_layers, dropout=dropout)\n",
    "        self.in_channels = in_channels\n",
    "        self.num_nodes = num_nodes\n",
    "        self.A = None\n",
    "        self.inner_activation = inner_activation\n",
    "        self.inner_dropout = inner_dropout\n",
    "\n",
    "    def reset_parameters(self):\t\n",
    "        self.mlpA.reset_parameters()\t\n",
    "        self.mlpX.reset_parameters()\n",
    "        self.W.reset_parameters()\n",
    "        self.mlp_final.reset_parameters()\t\n",
    "\n",
    "    def forward(self, b_data):\t\n",
    "        \n",
    "        m = b_data.num_nodes\n",
    "        feat_dim = b_data.x.shape[1]\n",
    "        row, col = b_data.edge_index\n",
    "        \n",
    "        row = row-row.min()\n",
    "        A = SparseTensor(row=row, col=col,\t\n",
    "                 sparse_sizes=(m, self.num_nodes)\n",
    "                        ).to_torch_sparse_coo_tensor()\n",
    "\n",
    "        xA = self.mlpA(A, input_tensor=True)\n",
    "        xX = self.mlpX(b_data.x, input_tensor=True)\n",
    "        x = torch.cat((xA, xX), axis=-1)\n",
    "        x = self.W(x)\n",
    "        if self.inner_dropout:\n",
    "            x = F.dropout(x)\n",
    "        if self.inner_activation:\n",
    "            x = F.relu(x)\n",
    "        x = F.relu(x + xA + xX)\n",
    "        x = self.mlp_final(x, input_tensor=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c811317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  34  E  156  d  4.588235294117647 0.8020520210266113 0.7564102411270142 0.6170591711997986 -0.4756128787994385 \n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = 'karate'\n",
    "data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022144fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = LINKX(in_channels=data.x.shape[1], hidden_channels=32, out_channels=dataset.num_classes, num_layers=2, num_nodes = data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760f6e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8602, -2.3305,  1.9020,  1.7530],\n",
       "        [-0.0415, -0.8655,  0.3054, -1.3157],\n",
       "        [ 0.9592, -0.1012,  0.1361, -0.2043],\n",
       "        [ 0.1871,  0.2289, -0.2385, -0.5092],\n",
       "        [-0.2558, -0.4815, -0.5436, -0.3338],\n",
       "        [ 1.2181,  0.5065, -0.2723, -0.5030],\n",
       "        [-1.4196, -1.9565, -1.3435,  0.5172],\n",
       "        [-0.2967, -1.6790, -1.2853,  0.3231],\n",
       "        [ 0.3240,  0.2654, -0.3580, -0.7607],\n",
       "        [ 0.2989,  0.8665,  0.0217, -0.3542],\n",
       "        [ 0.4382, -0.4370, -1.1081,  0.7084],\n",
       "        [-0.6875,  0.6078,  0.0684,  0.2254],\n",
       "        [ 0.4677,  0.3808, -0.2784,  0.1706],\n",
       "        [-1.1885, -1.6461, -0.8335, -1.4923],\n",
       "        [ 0.5087, -0.4047, -0.5542,  0.3314],\n",
       "        [ 0.5350, -0.6899, -0.8723,  0.9766],\n",
       "        [ 0.6135, -0.3675,  0.2930, -0.5557],\n",
       "        [ 0.4203,  0.6643, -0.1018, -0.4959],\n",
       "        [ 0.3971,  0.6886, -1.1383, -0.6583],\n",
       "        [ 0.8777,  0.2066,  1.2963,  0.3344],\n",
       "        [ 0.2656, -0.0056, -0.1563, -0.1857],\n",
       "        [-0.1354, -0.4392,  0.1960,  0.0806],\n",
       "        [ 0.3741, -0.3650, -0.7831, -0.2885],\n",
       "        [ 1.2445,  0.3621, -1.0864, -0.3557],\n",
       "        [-0.3291, -0.5210,  0.2634, -0.4816],\n",
       "        [-0.1817, -0.2375, -1.1937,  0.3369],\n",
       "        [-0.4172, -1.1493, -0.0930,  0.7287],\n",
       "        [-0.2157,  1.0966, -0.8549, -0.0325],\n",
       "        [-0.0251, -0.5724, -0.4711,  0.7177],\n",
       "        [-0.0312, -0.4163, -0.1860, -0.0913],\n",
       "        [ 0.9957,  0.1041, -0.6365, -0.5256],\n",
       "        [ 0.0256, -0.7988, -1.3647, -1.5432],\n",
       "        [-0.2148, -0.2162, -0.2325,  0.5773],\n",
       "        [-0.4204, -0.3626,  0.7919,  1.6319]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec4cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
