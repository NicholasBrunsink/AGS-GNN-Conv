{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cpu count:  20\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)\n",
    "\n",
    "DIR='./Dataset/'\n",
    "\n",
    "if os.uname()[1].find('gilbreth')==0: ##if not darwin(mac/locallaptop)\n",
    "    DIR='/scratch/gilbreth/das90/Dataset/'\n",
    "elif os.uname()[1].find('unimodular')==0:\n",
    "    DIR='/scratch2/das90/Dataset/'\n",
    "elif os.uname()[1].find('Siddharthas')==0:\n",
    "    DIR='/Users/siddharthashankardas/Purdue/Dataset/'  \n",
    "else:\n",
    "    DIR='./Dataset/'\n",
    "    \n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR=DIR+'RESULTS/'\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Data directory: \", DIR)\n",
    "print(\"Result directory:\", RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random        \n",
    "random.seed(12345)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.datasets import Reddit, Reddit2, Flickr, Yelp, AmazonProducts, PPI,  OGB_MAG,  FakeDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "# # old = np.load\n",
    "# # np.load = lambda *a,**k: old(*a,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Cora\", \"CiteSeer\", \"PubMed\", \"Reddit\", \"Flickr\", \n",
    "#\"Yelp\", \"AmazonProducts\",\"Reddit2\",\" OGB_MAG\"\n",
    "#\"Fake\"\n",
    "#\"Moon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "===========================================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME='Cora' \n",
    "\n",
    "def get_data(DATASET_NAME='Cora'):\n",
    "\n",
    "    if DATASET_NAME in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "        dataset = Planetoid(root=DIR+'Planetoid', name=DATASET_NAME, transform=NormalizeFeatures())\n",
    "\n",
    "    elif DATASET_NAME == \"Reddit2\":\n",
    "        dataset = Reddit2(root=DIR+'Reddit2', transform=NormalizeFeatures())\n",
    "\n",
    "    elif DATASET_NAME == \"Reddit\":\n",
    "        dataset = Reddit(root=DIR+'Reddit', transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME == \"Flickr\":\n",
    "        dataset = Flickr(root=DIR+'Flickr', transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"Yelp\":\n",
    "        dataset = Yelp(root=DIR+'Yelp', transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"AmazonProducts\":\n",
    "        dataset = AmazonProducts(root=DIR+'AmazonProducts', transform=NormalizeFeatures())\n",
    "    \n",
    "    elif DATASET_NAME == \"OGB_MAG\":\n",
    "        dataset = OGB_MAG(root=DIR+'OGB_MAG', preprocess='metapath2vec', transform=NormalizeFeatures())\n",
    "        \n",
    "    elif DATASET_NAME == \"Fake\":\n",
    "        import torch        \n",
    "        \n",
    "        dataset = FakeDataset(num_graphs = 1, \n",
    "                              avg_num_nodes = 1000, \n",
    "                              avg_degree = 10, \n",
    "                              num_channels = 64, \n",
    "                              edge_dim = 0, \n",
    "                              num_classes = 10, \n",
    "                              task = 'auto', \n",
    "                              is_undirected = True,                               \n",
    "                              transform=NormalizeFeatures())\n",
    "        \n",
    "        data = dataset[0]\n",
    "        train=0.3;val=0.3;test=1-train-val\n",
    "\n",
    "        l=len(data.y)\n",
    "        indexs=list(range(l))\n",
    "        random.shuffle(indexs)\n",
    "\n",
    "        train_index=indexs[:int(train*l)]\n",
    "        val_index=indexs[int(train*l):int(train*l)+int(val*l)]\n",
    "        test_index=indexs[int(train*l)+int(val*l):]\n",
    "\n",
    "        data.train_mask=torch.zeros(l, dtype=torch.bool)\n",
    "        data.train_mask[train_index]=True\n",
    "        data.val_mask=torch.zeros(l, dtype=torch.bool)\n",
    "        data.val_mask[val_index]=True\n",
    "        data.test_mask=torch.zeros(l, dtype=torch.bool)\n",
    "        data.test_mask[test_index]=True\n",
    "        \n",
    "        dataset.root=DIR+'FakeDataset'\n",
    "        \n",
    "        print(data)\n",
    "        \n",
    "        return data, dataset\n",
    "        \n",
    "    \n",
    "    elif DATASET_NAME=='Moon':\n",
    "        import ipynb.fs.full.utils.MoonGraph as GMoon        \n",
    "        dataset = GMoon.MoonDataset(n_samples=1000, degree=10)    \n",
    "        G, data =dataset[0]    \n",
    "        #GMoon.drawMoon(G,data)\n",
    "   \n",
    "    else:    \n",
    "        raise Exception('dataset not found')\n",
    "\n",
    "    \n",
    "    if DATASET_NAME == \"OGB_MAG\":\n",
    "        print(f'Dataset: {dataset}:')\n",
    "        print('======================')\n",
    "        print(f'Number of graphs: {len(dataset)}')\n",
    "        \n",
    "        data = dataset[0]\n",
    "    \n",
    "    else:\n",
    "        print()\n",
    "        print(f'Dataset: {dataset}:')\n",
    "        print('======================')\n",
    "        print(f'Number of graphs: {len(dataset)}')\n",
    "        print(f'Number of features: {dataset.num_features}')\n",
    "        print(f'Number of classes: {dataset.num_classes}')\n",
    "        \n",
    "        if DATASET_NAME in ['Moon']:\n",
    "            None\n",
    "        else:\n",
    "            data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "        print()\n",
    "        print(data)\n",
    "        print('===========================================================================================================')\n",
    "\n",
    "        # Gather some statistics about the graph.\n",
    "        print(f'Number of nodes: {data.num_nodes}')\n",
    "        print(f'Number of edges: {data.num_edges}')\n",
    "        print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "        print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "        print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "        print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "        print(f'Has self-loops: {data.has_self_loops()}')\n",
    "        print(f'Is undirected: {data.is_undirected()}')\n",
    "    \n",
    "    return data, dataset\n",
    "\n",
    "data, dataset = get_data(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # restore np.load for future normal usage\n",
    "# # np.load = old\n",
    "# # del(old)\n",
    "# np.load.__defaults__=(None, False, True, 'ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IgnoreStructure = False\n",
    "\n",
    "if IgnoreStructure==True:\n",
    "    nodeid=list(range(len(data.y)))\n",
    "    data.edge_index=torch.tensor([nodeid,nodeid],dtype=torch.long)\n",
    "    #print(data.edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Graph construction\n",
    "\n",
    "[Cora dataset drawing](https://graphsandnetworks.com/the-cora-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create graph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "from ipynb.fs.full.utils.GNNutils import save_gephi_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Saved graph...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if DATASET_NAME in ['Moon','Fake']:\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "else:\n",
    "    G_fillename=DIR+DATASET_NAME+\".gpickle\"\n",
    "\n",
    "    if os.path.exists(G_fillename)==False:\n",
    "        print(\"Graph is not found, creating it....\")\n",
    "        G = to_networkx(data, to_undirected=True)\n",
    "        nx.write_gpickle(G, G_fillename)\n",
    "        print(\"Done\")\n",
    "    else:\n",
    "        print(\"Loading Saved graph...\")\n",
    "        G = nx.read_gpickle(G_fillename)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save_gephi_graph(G, data.y, RESULTS_DIR+DATASET_NAME+'_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=['ExactERSampling',\n",
    "         'LocalERSampling',\n",
    "         'SubmodularSparsificationLocal',\n",
    "         'SubmodularSparsifierGlobal',\n",
    "         'SpanningTree',\n",
    "         'TSpanner',         \n",
    "         'RandomGlobal',\n",
    "         'RandomLocal',\n",
    "         'LocalKNN',\n",
    "         'SpectralSubmodularGlobal',\n",
    "         'SpectralSubmodularLocal',\n",
    "         'GSmain'\n",
    "        ]\n",
    "\n",
    "method_name = 'LocalKNN' \n",
    "STOverlay = False\n",
    "IgnoreFeature = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Optimizer\n",
    "\n",
    "['random', 'modular', 'naive', 'lazy', 'approximate-lazy', 'two-stage', 'stochastic', 'sample', 'greedi', 'bidirectional', 'sieve']\n",
    "\n",
    "#### Available distance metric\n",
    "\n",
    "['euclidean', 'l2', 'l1', 'manhattan', 'cityblock', 'braycurtis', 'canberra', 'chebyshev', 'correlation', 'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule', 'wminkowski', 'nan_euclidean', 'haversine'], or 'precomputed', or a callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IgnoreFeature==True:\n",
    "    print(data.x.shape)\n",
    "    p = torch.empty(data.x.shape).uniform_(0, 1)    \n",
    "    x = torch.bernoulli(p)\n",
    "    #x = torch.ones(data.x.shape)\n",
    "    data.x = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "sparse_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cpu count:  20\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019861459732055664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9050fb7c1d6e4a7fb27365536c75d93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7186\n"
     ]
    }
   ],
   "source": [
    "if method_name=='ExactERSampling':\n",
    "    from ipynb.fs.full.SpectralSparsifier import  TrueERSparsify    \n",
    "    \n",
    "    sparserW, nz = TrueERSparsify(G, epsilon=0.90)\n",
    "    H = nx.from_scipy_sparse_matrix(sparserW)\n",
    "    \n",
    "    ledge_index=np.array([sparserW.tocoo().row, sparserW.tocoo().col])\n",
    "    edge_weight=sparserW.tocoo().data\n",
    "    \n",
    "elif method_name=='LocalERSampling':\n",
    "    from ipynb.fs.full.SpectralSparsifier import  LocalERSparsify, LocalERfast    \n",
    "    \n",
    "    #sparserW, nz = LocalERSparsify(G, 0.70, computation='parallel', window=NUM_PROCESSORS*1000)\n",
    "    sparserW, nz = LocalERSparsify(G, 0.70, computation='linear', window=NUM_PROCESSORS*1000)\n",
    "    #sparserW, nz = LocalERfast(G, 0.70, computation='parallel', window=NUM_PROCESSORS*100)\n",
    "    \n",
    "    H = nx.from_scipy_sparse_matrix(sparserW)\n",
    "    \n",
    "    ledge_index=np.array([sparserW.tocoo().row, sparserW.tocoo().col])\n",
    "    edge_weight=sparserW.tocoo().data\n",
    "    \n",
    "elif method_name=='SubmodularSparsificationLocal':\n",
    "    from ipynb.fs.full.SubmodularSparsifierLocal import  LocalKSparsify, LocalKSparsifyParallel\n",
    "    from ipynb.fs.full.SubmodularSparsifierLocal import LocalKSparsifyParallelCustom,LocalKSparsifyFacilityLocation\n",
    "    \n",
    "    H=LocalKSparsifyParallelCustom(G, data.x.numpy(), K=1, window=NUM_PROCESSORS*100, njobs=NUM_PROCESSORS)\n",
    "    #H=LocalKSparsify(G, data.x.numpy(), K=1)\n",
    "    #H=LocalKSparsifyFacilityLocation(G, data.x.numpy(), K=1,window=NUM_PROCESSORS*100, njobs=NUM_PROCESSORS,metric='cosine')\n",
    "    \n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='SubmodularSparsifierGlobal':\n",
    "    from ipynb.fs.full.SubmodularSparsifierGlobal import  GlobalSparsifier \n",
    "    \n",
    "    H=GlobalSparsifier(G, data.x.numpy(), 10000, optimizer='stochastic')\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='SpectralSubmodularGlobal':\n",
    "    from ipynb.fs.full.SpectralSubmodularGlobal import  GlobalSubSpecSparsifier \n",
    "    \n",
    "    H=GlobalSubSpecSparsifier(G, data.x.numpy(), 2500, optimizer='approximate-lazy')\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='SpectralSubmodularLocal':\n",
    "    from ipynb.fs.full.SpectralSubmodularLocal import  LocalKSparsify, LocalKSparsifySpectral\n",
    "    \n",
    "    H=LocalKSparsify(G, data.x.numpy(), K=2)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='GSmain':\n",
    "    from ipynb.fs.full.GraphSparsiferMain import LocalKSparsifySpectral, SpectralSubmodularK \n",
    "    \n",
    "    #H=LocalKSparsifySpectral(G, data.x.numpy(), K=2)\n",
    "    H=SpectralSubmodularK(G, data.x.numpy(), K=3)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "    \n",
    "elif method_name=='SpanningTree':\n",
    "    from ipynb.fs.full.SpanningTreeSparsifier import  STSparsify \n",
    "    \n",
    "    H=STSparsify(G)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='TSpanner':\n",
    "    from ipynb.fs.full.SpanningTreeSparsifier import  TSpannerSparsify \n",
    "    \n",
    "    H=TSpannerSparsify(G, stretch=20)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='RandomGlobal':\n",
    "    from ipynb.fs.full.SpanningTreeSparsifier import  RandomWieghtedSparsify \n",
    "    \n",
    "    H=RandomWieghtedSparsify(G,0.95)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "elif method_name=='RandomLocal':\n",
    "    from ipynb.fs.full.SpanningTreeSparsifier import  RandomKLocal, RandomKLocalLinear\n",
    "    \n",
    "    H=RandomKLocalLinear(G, data.x.numpy(), K=1)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "    \n",
    "elif method_name=='LocalKNN':\n",
    "    from ipynb.fs.full.SpanningTreeSparsifier import  LocalKNN, LocalKNNParallel\n",
    "    \n",
    "    H = LocalKNN(G, data.x.numpy(), K=2)\n",
    "    #H = LocalKNNParallel(G, data.x.numpy(), K=5, window=NUM_PROCESSORS*100, njobs=NUM_PROCESSORS)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"The sparsification algorithm is not defined\")\n",
    "    ledge_index=[[],[]]\n",
    "    edge_weight=None\n",
    "    H=nx.Graph()\n",
    "    \n",
    "print(len(ledge_index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.5909264087677002\n"
     ]
    }
   ],
   "source": [
    "sparse_end = time.time()\n",
    "print (\"Time elapsed:\", sparse_end - sparse_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOverlay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STOverlay==True:\n",
    "    from ipynb.fs.full.SpanningTreeSparsifier import  STSparsify \n",
    "        \n",
    "    ST_H=STSparsify(G)    \n",
    "    H = nx.compose(H,ST_H)\n",
    "    \n",
    "    edges=np.array(list(H.edges))\n",
    "    ledge_index=[list(edges[:,0]),list(edges[:,1])]\n",
    "    ledge_index[0].extend(list(edges[:,1]))\n",
    "    ledge_index[1].extend(list(edges[:,0]))\n",
    "    edge_weight=None\n",
    "    \n",
    "    print(len(ledge_index[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_NAME in ['Moon','Fake']:\n",
    "    import copy\n",
    "    ldata=copy.deepcopy(data)\n",
    "else:\n",
    "    ldata=dataset[0]\n",
    "\n",
    "\n",
    "if IgnoreFeature==True:\n",
    "    ldata.x = x\n",
    "    \n",
    "if IgnoreStructure==True:\n",
    "    nodeid=list(range(len(data.y)))\n",
    "    ledge_index=[nodeid,nodeid]\n",
    "    edge_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge weights not updated\n"
     ]
    }
   ],
   "source": [
    "ldata.edge_index=torch.tensor(ledge_index, dtype=torch.long)\n",
    "\n",
    "if edge_weight is None:\n",
    "    print(\"Edge weights not updated\")\n",
    "else:\n",
    "    ldata.edge_weight=torch.tensor(edge_weight, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_fillename=DIR+DATASET_NAME+\"_sub_spectral.gpickle\"\n",
    "# nx.write_gpickle(H, H_fillename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save_gephi_graph(H, ldata.y, RESULTS_DIR+DATASET_NAME+\"-\"+method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nodes=list(G.nodes())\n",
    "\n",
    "# for u in nodes[:100]:\n",
    "#     V=list(G.neighbors(u))\n",
    "#     print(data.y[u])\n",
    "#     print(data.y[V])\n",
    "#     print(\"-\"*50)  \n",
    "    \n",
    "#     print(\"Sparse..\")\n",
    "    \n",
    "#     V=list(H.neighbors(u))\n",
    "#     print(data.y[u])\n",
    "#     print(data.y[V])\n",
    "#     print(\"*\"*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_NAME=='Moon':\n",
    "    import ipynb.fs.full.utils.MoonGraph as GMoon        \n",
    "#     GMoon.drawMoon(G,data)\n",
    "#     GMoon.drawMoon(H,data)\n",
    "    \n",
    "    GMoon.draw_blobs_data(G,data)\n",
    "    GMoon.draw_blobs_data(H,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct hybrid graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7186])\n",
      "torch.Size([2, 10556])\n"
     ]
    }
   ],
   "source": [
    "print(ldata.edge_index.shape)\n",
    "print(data.edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### GNN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n",
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n",
      "Data directory:  /scratch/gilbreth/das90/Dataset/\n",
      "Result directory: /scratch/gilbreth/das90/Dataset/RESULTS/\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.GNNalgorithms import GCNperformance, GATperformance\n",
    "from ipynb.fs.full.GCNGATsampler import GCNperformanceSampler, GATperformanceSampler\n",
    "from ipynb.fs.full.GSAGE import GSAGEperformance\n",
    "from ipynb.fs.full.GSAGEmultilabel import GSAGEperformance as GSAGEperformanceMultiLabel\n",
    "from ipynb.fs.full.GSAINT import GSAINTperformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSAINTperformance(data, dataset, epochs=30,train_method='Sample',inference_method='Sample',\n",
    "#                   train_neighbors=[-1,10],\n",
    "#                   test_neighbors=[-1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSAINTperformance(ldata, dataset, epochs=30,train_method='Sample',inference_method='Sample',\n",
    "#                   train_neighbors=[-1,-1],\n",
    "#                   test_neighbors=[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSAGEperformance(data, dataset, epochs=50, train_neighbors=[25,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSAGEperformance(ldata, dataset, epochs=50, train_neighbors=[25, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSAGEperformanceMultiLabel(data, dataset, epochs=20, train_neighbors=[25,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSAGEperformanceMultiLabel(ldata, dataset, epochs=20, train_neighbors=[25,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 256)\n",
      "  (conv2): GCNConv(256, 7)\n",
      ")\n",
      "Epoch: 001, Loss: 1.9447, Val: 0.3720, Test: 0.3820\n",
      "Epoch: 002, Loss: 1.9220, Val: 0.3740, Test: 0.3930\n",
      "Epoch: 003, Loss: 1.8934, Val: 0.4200, Test: 0.4190\n",
      "Epoch: 004, Loss: 1.8559, Val: 0.4200, Test: 0.4290\n",
      "Epoch: 005, Loss: 1.8135, Val: 0.4660, Test: 0.4740\n",
      "Epoch: 006, Loss: 1.7646, Val: 0.5240, Test: 0.5420\n",
      "Epoch: 007, Loss: 1.7154, Val: 0.5960, Test: 0.6090\n",
      "Epoch: 008, Loss: 1.6547, Val: 0.6560, Test: 0.6640\n",
      "Epoch: 009, Loss: 1.5888, Val: 0.6880, Test: 0.7170\n",
      "Epoch: 010, Loss: 1.5195, Val: 0.7120, Test: 0.7460\n",
      "Epoch: 011, Loss: 1.4485, Val: 0.7400, Test: 0.7630\n",
      "Epoch: 012, Loss: 1.3815, Val: 0.7580, Test: 0.7750\n",
      "Epoch: 013, Loss: 1.2946, Val: 0.7720, Test: 0.7820\n",
      "Epoch: 014, Loss: 1.2068, Val: 0.7720, Test: 0.7850\n",
      "Epoch: 015, Loss: 1.1322, Val: 0.7840, Test: 0.7870\n",
      "Epoch: 016, Loss: 1.0754, Val: 0.7840, Test: 0.7930\n",
      "Epoch: 017, Loss: 0.9883, Val: 0.7840, Test: 0.8010\n",
      "Epoch: 018, Loss: 0.9034, Val: 0.7860, Test: 0.8020\n",
      "Epoch: 019, Loss: 0.8536, Val: 0.7880, Test: 0.8000\n",
      "Epoch: 020, Loss: 0.7760, Val: 0.7960, Test: 0.8090\n",
      "Epoch: 021, Loss: 0.7245, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 022, Loss: 0.6712, Val: 0.7980, Test: 0.8140\n",
      "Epoch: 023, Loss: 0.6037, Val: 0.8000, Test: 0.8150\n",
      "Epoch: 024, Loss: 0.5856, Val: 0.7980, Test: 0.8170\n",
      "Epoch: 025, Loss: 0.5369, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 026, Loss: 0.4997, Val: 0.8060, Test: 0.8190\n",
      "Epoch: 027, Loss: 0.4714, Val: 0.8100, Test: 0.8250\n",
      "Epoch: 028, Loss: 0.4386, Val: 0.8080, Test: 0.8260\n",
      "Epoch: 029, Loss: 0.4229, Val: 0.8000, Test: 0.8240\n",
      "Epoch: 030, Loss: 0.3948, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 031, Loss: 0.3775, Val: 0.7980, Test: 0.8170\n",
      "Epoch: 032, Loss: 0.3644, Val: 0.7980, Test: 0.8200\n",
      "Epoch: 033, Loss: 0.3467, Val: 0.8000, Test: 0.8200\n",
      "Epoch: 034, Loss: 0.3334, Val: 0.8000, Test: 0.8240\n",
      "Epoch: 035, Loss: 0.3310, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 036, Loss: 0.3096, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 037, Loss: 0.2906, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 038, Loss: 0.2987, Val: 0.7960, Test: 0.8140\n",
      "Epoch: 039, Loss: 0.2849, Val: 0.7900, Test: 0.8120\n",
      "Epoch: 040, Loss: 0.2904, Val: 0.7860, Test: 0.8130\n",
      "Epoch: 041, Loss: 0.2875, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 042, Loss: 0.2741, Val: 0.7980, Test: 0.8190\n",
      "Epoch: 043, Loss: 0.2707, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 044, Loss: 0.2717, Val: 0.7980, Test: 0.8110\n",
      "Epoch: 045, Loss: 0.2624, Val: 0.7940, Test: 0.8040\n",
      "Epoch: 046, Loss: 0.2442, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 047, Loss: 0.2372, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 048, Loss: 0.2400, Val: 0.7900, Test: 0.8130\n",
      "Epoch: 049, Loss: 0.2437, Val: 0.7940, Test: 0.8110\n",
      "Epoch: 050, Loss: 0.2257, Val: 0.8020, Test: 0.8110\n",
      "Epoch: 051, Loss: 0.2324, Val: 0.8000, Test: 0.8050\n",
      "Epoch: 052, Loss: 0.2151, Val: 0.7980, Test: 0.8000\n",
      "Epoch: 053, Loss: 0.2201, Val: 0.7980, Test: 0.7970\n",
      "Epoch: 054, Loss: 0.2160, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 055, Loss: 0.2193, Val: 0.7980, Test: 0.8070\n",
      "Epoch: 056, Loss: 0.2213, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 057, Loss: 0.2104, Val: 0.7960, Test: 0.8110\n",
      "Epoch: 058, Loss: 0.2063, Val: 0.8020, Test: 0.8030\n",
      "Epoch: 059, Loss: 0.2132, Val: 0.7980, Test: 0.7960\n",
      "Epoch: 060, Loss: 0.2198, Val: 0.7980, Test: 0.7930\n",
      "Epoch: 061, Loss: 0.2117, Val: 0.7920, Test: 0.8000\n",
      "Epoch: 062, Loss: 0.2047, Val: 0.7980, Test: 0.8060\n",
      "Epoch: 063, Loss: 0.1917, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 064, Loss: 0.1961, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 065, Loss: 0.1924, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 066, Loss: 0.1984, Val: 0.7960, Test: 0.8030\n",
      "Epoch: 067, Loss: 0.1907, Val: 0.7920, Test: 0.8080\n",
      "Epoch: 068, Loss: 0.1814, Val: 0.7980, Test: 0.8070\n",
      "Epoch: 069, Loss: 0.1804, Val: 0.7960, Test: 0.8040\n",
      "Epoch: 070, Loss: 0.1843, Val: 0.7900, Test: 0.8050\n",
      "Epoch: 071, Loss: 0.1871, Val: 0.7880, Test: 0.7980\n",
      "Epoch: 072, Loss: 0.1728, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 073, Loss: 0.1766, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 074, Loss: 0.1881, Val: 0.7940, Test: 0.8080\n",
      "Epoch: 075, Loss: 0.1795, Val: 0.7960, Test: 0.8090\n",
      "Epoch: 076, Loss: 0.1628, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 077, Loss: 0.1712, Val: 0.7900, Test: 0.8060\n",
      "Epoch: 078, Loss: 0.1755, Val: 0.8040, Test: 0.8040\n",
      "Epoch: 079, Loss: 0.1755, Val: 0.8060, Test: 0.8050\n",
      "Epoch: 080, Loss: 0.1676, Val: 0.7920, Test: 0.8070\n",
      "Epoch: 081, Loss: 0.1790, Val: 0.7860, Test: 0.8050\n",
      "Epoch: 082, Loss: 0.1649, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 083, Loss: 0.1748, Val: 0.7940, Test: 0.8020\n",
      "Epoch: 084, Loss: 0.1740, Val: 0.7920, Test: 0.8090\n",
      "Epoch: 085, Loss: 0.1693, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 086, Loss: 0.1769, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 087, Loss: 0.1613, Val: 0.7940, Test: 0.8040\n",
      "Epoch: 088, Loss: 0.1606, Val: 0.7980, Test: 0.7960\n",
      "Epoch: 089, Loss: 0.1633, Val: 0.7920, Test: 0.8020\n",
      "Epoch: 090, Loss: 0.1527, Val: 0.7900, Test: 0.8070\n",
      "Epoch: 091, Loss: 0.1692, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 092, Loss: 0.1615, Val: 0.7960, Test: 0.8140\n",
      "Epoch: 093, Loss: 0.1616, Val: 0.7980, Test: 0.8080\n",
      "Epoch: 094, Loss: 0.1573, Val: 0.7900, Test: 0.8030\n",
      "Epoch: 095, Loss: 0.1501, Val: 0.7940, Test: 0.7970\n",
      "Epoch: 096, Loss: 0.1563, Val: 0.7920, Test: 0.7980\n",
      "Epoch: 097, Loss: 0.1532, Val: 0.7960, Test: 0.8060\n",
      "Epoch: 098, Loss: 0.1460, Val: 0.7980, Test: 0.8170\n",
      "Epoch: 099, Loss: 0.1523, Val: 0.7980, Test: 0.8120\n",
      "Epoch: 100, Loss: 0.1547, Val: 0.7880, Test: 0.8100\n",
      "Best Test Accuracy,  0.826\n",
      "GCN Test Accuracy: 0.8100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCNperformance(data, dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 256)\n",
      "  (conv2): GCNConv(256, 7)\n",
      ")\n",
      "Epoch: 001, Loss: 1.9450, Val: 0.2200, Test: 0.2460\n",
      "Epoch: 002, Loss: 1.9195, Val: 0.2200, Test: 0.2420\n",
      "Epoch: 003, Loss: 1.8874, Val: 0.2420, Test: 0.2690\n",
      "Epoch: 004, Loss: 1.8472, Val: 0.2900, Test: 0.3140\n",
      "Epoch: 005, Loss: 1.7986, Val: 0.3720, Test: 0.4050\n",
      "Epoch: 006, Loss: 1.7464, Val: 0.4460, Test: 0.4810\n",
      "Epoch: 007, Loss: 1.6911, Val: 0.5340, Test: 0.5370\n",
      "Epoch: 008, Loss: 1.6260, Val: 0.6180, Test: 0.6130\n",
      "Epoch: 009, Loss: 1.5524, Val: 0.6800, Test: 0.6890\n",
      "Epoch: 010, Loss: 1.4807, Val: 0.7080, Test: 0.7250\n",
      "Epoch: 011, Loss: 1.3963, Val: 0.7400, Test: 0.7550\n",
      "Epoch: 012, Loss: 1.3271, Val: 0.7580, Test: 0.7600\n",
      "Epoch: 013, Loss: 1.2340, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 014, Loss: 1.1427, Val: 0.7680, Test: 0.7610\n",
      "Epoch: 015, Loss: 1.0582, Val: 0.7720, Test: 0.7630\n",
      "Epoch: 016, Loss: 0.9962, Val: 0.7820, Test: 0.7670\n",
      "Epoch: 017, Loss: 0.9090, Val: 0.7920, Test: 0.7750\n",
      "Epoch: 018, Loss: 0.8238, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 019, Loss: 0.7767, Val: 0.8080, Test: 0.7780\n",
      "Epoch: 020, Loss: 0.6996, Val: 0.8140, Test: 0.7850\n",
      "Epoch: 021, Loss: 0.6451, Val: 0.8160, Test: 0.7860\n",
      "Epoch: 022, Loss: 0.5880, Val: 0.8200, Test: 0.7880\n",
      "Epoch: 023, Loss: 0.5257, Val: 0.8140, Test: 0.7880\n",
      "Epoch: 024, Loss: 0.5122, Val: 0.8120, Test: 0.7840\n",
      "Epoch: 025, Loss: 0.4660, Val: 0.8140, Test: 0.7860\n",
      "Epoch: 026, Loss: 0.4303, Val: 0.8120, Test: 0.7800\n",
      "Epoch: 027, Loss: 0.4069, Val: 0.8100, Test: 0.7840\n",
      "Epoch: 028, Loss: 0.3759, Val: 0.8120, Test: 0.7860\n",
      "Epoch: 029, Loss: 0.3578, Val: 0.8100, Test: 0.7910\n",
      "Epoch: 030, Loss: 0.3312, Val: 0.8060, Test: 0.7860\n",
      "Epoch: 031, Loss: 0.3218, Val: 0.8060, Test: 0.7840\n",
      "Epoch: 032, Loss: 0.3064, Val: 0.8000, Test: 0.7800\n",
      "Epoch: 033, Loss: 0.2954, Val: 0.7980, Test: 0.7730\n",
      "Epoch: 034, Loss: 0.2796, Val: 0.8000, Test: 0.7730\n",
      "Epoch: 035, Loss: 0.2807, Val: 0.7920, Test: 0.7850\n",
      "Epoch: 036, Loss: 0.2572, Val: 0.7940, Test: 0.7820\n",
      "Epoch: 037, Loss: 0.2400, Val: 0.7920, Test: 0.7840\n",
      "Epoch: 038, Loss: 0.2581, Val: 0.7940, Test: 0.7830\n",
      "Epoch: 039, Loss: 0.2339, Val: 0.7900, Test: 0.7790\n",
      "Epoch: 040, Loss: 0.2455, Val: 0.7940, Test: 0.7750\n",
      "Epoch: 041, Loss: 0.2434, Val: 0.7980, Test: 0.7750\n",
      "Epoch: 042, Loss: 0.2291, Val: 0.7980, Test: 0.7800\n",
      "Epoch: 043, Loss: 0.2299, Val: 0.8000, Test: 0.7830\n",
      "Epoch: 044, Loss: 0.2311, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 045, Loss: 0.2197, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 046, Loss: 0.2126, Val: 0.7860, Test: 0.7810\n",
      "Epoch: 047, Loss: 0.2027, Val: 0.7820, Test: 0.7780\n",
      "Epoch: 048, Loss: 0.1987, Val: 0.7880, Test: 0.7790\n",
      "Epoch: 049, Loss: 0.2119, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 050, Loss: 0.1950, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 051, Loss: 0.1995, Val: 0.7960, Test: 0.7810\n",
      "Epoch: 052, Loss: 0.1890, Val: 0.7880, Test: 0.7800\n",
      "Epoch: 053, Loss: 0.1942, Val: 0.7920, Test: 0.7770\n",
      "Epoch: 054, Loss: 0.1910, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 055, Loss: 0.1849, Val: 0.8000, Test: 0.7820\n",
      "Epoch: 056, Loss: 0.1850, Val: 0.8020, Test: 0.7830\n",
      "Epoch: 057, Loss: 0.1760, Val: 0.7980, Test: 0.7830\n",
      "Epoch: 058, Loss: 0.1777, Val: 0.7940, Test: 0.7810\n",
      "Epoch: 059, Loss: 0.1753, Val: 0.7900, Test: 0.7820\n",
      "Epoch: 060, Loss: 0.1806, Val: 0.7920, Test: 0.7750\n",
      "Epoch: 061, Loss: 0.1814, Val: 0.7900, Test: 0.7740\n",
      "Epoch: 062, Loss: 0.1789, Val: 0.7900, Test: 0.7720\n",
      "Epoch: 063, Loss: 0.1638, Val: 0.7920, Test: 0.7850\n",
      "Epoch: 064, Loss: 0.1648, Val: 0.7960, Test: 0.7860\n",
      "Epoch: 065, Loss: 0.1655, Val: 0.7960, Test: 0.7870\n",
      "Epoch: 066, Loss: 0.1743, Val: 0.7820, Test: 0.7850\n",
      "Epoch: 067, Loss: 0.1645, Val: 0.7780, Test: 0.7790\n",
      "Epoch: 068, Loss: 0.1561, Val: 0.7840, Test: 0.7810\n",
      "Epoch: 069, Loss: 0.1511, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 070, Loss: 0.1597, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 071, Loss: 0.1580, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 072, Loss: 0.1487, Val: 0.7860, Test: 0.7780\n",
      "Epoch: 073, Loss: 0.1527, Val: 0.7860, Test: 0.7780\n",
      "Epoch: 074, Loss: 0.1589, Val: 0.7900, Test: 0.7780\n",
      "Epoch: 075, Loss: 0.1526, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 076, Loss: 0.1440, Val: 0.7820, Test: 0.7800\n",
      "Epoch: 077, Loss: 0.1434, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 078, Loss: 0.1543, Val: 0.8000, Test: 0.7840\n",
      "Epoch: 079, Loss: 0.1493, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 080, Loss: 0.1455, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 081, Loss: 0.1517, Val: 0.7800, Test: 0.7780\n",
      "Epoch: 082, Loss: 0.1373, Val: 0.7720, Test: 0.7740\n",
      "Epoch: 083, Loss: 0.1497, Val: 0.7740, Test: 0.7820\n",
      "Epoch: 084, Loss: 0.1514, Val: 0.7880, Test: 0.7880\n",
      "Epoch: 085, Loss: 0.1439, Val: 0.8020, Test: 0.7940\n",
      "Epoch: 086, Loss: 0.1508, Val: 0.8000, Test: 0.7840\n",
      "Epoch: 087, Loss: 0.1384, Val: 0.7840, Test: 0.7730\n",
      "Epoch: 088, Loss: 0.1346, Val: 0.7800, Test: 0.7660\n",
      "Epoch: 089, Loss: 0.1380, Val: 0.7780, Test: 0.7730\n",
      "Epoch: 090, Loss: 0.1297, Val: 0.7820, Test: 0.7850\n",
      "Epoch: 091, Loss: 0.1472, Val: 0.8000, Test: 0.7920\n",
      "Epoch: 092, Loss: 0.1398, Val: 0.8020, Test: 0.7870\n",
      "Epoch: 093, Loss: 0.1405, Val: 0.7960, Test: 0.7810\n",
      "Epoch: 094, Loss: 0.1325, Val: 0.7960, Test: 0.7800\n",
      "Epoch: 095, Loss: 0.1287, Val: 0.7840, Test: 0.7750\n",
      "Epoch: 096, Loss: 0.1314, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 097, Loss: 0.1314, Val: 0.7880, Test: 0.7870\n",
      "Epoch: 098, Loss: 0.1258, Val: 0.7880, Test: 0.7880\n",
      "Epoch: 099, Loss: 0.1294, Val: 0.8020, Test: 0.7880\n",
      "Epoch: 100, Loss: 0.1292, Val: 0.7940, Test: 0.7820\n",
      "Best Test Accuracy,  0.794\n",
      "GCN Test Accuracy: 0.7820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.782"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCNperformance(ldata, dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNperformanceSampler(data, dataset, epochs=100, train_neighbors=[-1,10],test_neighbors=[-1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNperformanceSampler(ldata, dataset, epochs=100, train_neighbors=[-1,10],test_neighbors=[-1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldata.edge_weight=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATperformance(data, dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATperformance(ldata, dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATperformanceSampler(data, dataset, epochs=100, train_neighbors=[2,2,2],test_neighbors=[2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATperformanceSampler(ldata, dataset, epochs=100, train_neighbors=[-1,-1,-1],test_neighbors=[-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
