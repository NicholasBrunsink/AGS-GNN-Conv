{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed5cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../Submodular')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d69a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c940d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd7aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    #parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('--use_normalization', action='store_true')    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc20b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import LINKXDataset\n",
    "from torch_geometric.nn import LINKX\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feffa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)    \n",
    "    data = data.to(device)\n",
    "    \n",
    "    train_losses=[]\n",
    "    best_acc = 0 \n",
    "    num_iteration = epochs\n",
    "    \n",
    "    for epoch in range(1,epochs+1):        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        mask = data.train_mask  # Use the first set of the five masks.\n",
    "        loss = F.cross_entropy(out[mask], data.y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()                            \n",
    "        \n",
    "        total_loss = loss.item()\n",
    "        train_losses.append(total_loss)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            accs = []\n",
    "            model.eval()\n",
    "            pred = model(data.x, data.edge_index).argmax(dim=-1)\n",
    "            for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "                mask = mask # Use the first set of the five masks.\n",
    "                accs.append(int((pred[mask] == data.y[mask]).sum()) / int(mask.sum()))        \n",
    "                \n",
    "        train_acc, val_acc, test_acc = accs[0], accs[1], accs[2]\n",
    "        \n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "        \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "                \n",
    "#         if epoch>=5 and std_dev<=1e-3:\n",
    "#             num_iteration = epoch\n",
    "            \n",
    "#             if args.log_info:                \n",
    "#                 print(\"Iteration for convergence: \", epoch)\n",
    "#             break\n",
    "                \n",
    "    return best_acc, num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f08f5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train'):\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            out = model(batch_data.x, batch_data.edge_index,batch_data.edge_weight)\n",
    "            out=out[:batch_data.batch_size,:]\n",
    "            pred = out.argmax(dim=-1)            \n",
    "            correct = pred.eq(batch_data.y[:batch_data.batch_size].to(device))\n",
    "\n",
    "            total_correct+=correct.sum()\n",
    "            total_examples+=batch_data.batch_size\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_data.batch_size)\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "\n",
    "    return total_correct.item()/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d85e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LINKXperformanceSampler(data, dataset, num_classes, epochs=1, train_neighbors=[8,4], test_neighbors=[8,4]):        \n",
    "    model = LINKX(data.num_nodes, data.num_features, hidden_channels=128,\n",
    "              out_channels= num_classes, num_layers=1,\n",
    "              num_edge_layers=1, num_node_layers=1, dropout=0.5).to(device)\n",
    "\n",
    "    if args.log_info:\n",
    "        print(model) \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, epochs)        \n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "558cc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['link', 'gcn', 'mlp', 'cs', 'sgc', \n",
    "           'gprgnn', 'appnp', 'gat', 'lp', \n",
    "           'mixhop','gcnjk','gatjk','h2gcn',\n",
    "           'link_concat','linkx','gcn2']\n",
    "\n",
    "#others = ['gsage','gsaint','acmgcn','clustergcn','gcn','gin','gat','linkx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "303b0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.log_info = True\n",
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "# print(data)\n",
    "# best_acc, num_iteration, _ = LINKXperformanceSampler(data, dataset, dataset.num_classes, epochs=10, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817f9ab",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adfb1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        \"Cornell\",\n",
    "        \"Texas\",\n",
    "        \"Wisconsin\",\n",
    "        \"reed98\",\n",
    "        \"amherst41\",\n",
    "        \"penn94\",\n",
    "        \"Roman-empire\",\n",
    "        \"cornell5\",\n",
    "        \"Squirrel\",\n",
    "        \"johnshopkins55\",\n",
    "        \"AmazonProducts\",\n",
    "        \"Actor\",\n",
    "        \"Minesweeper\",\n",
    "        \"Questions\",\n",
    "        \"Chameleon\",\n",
    "        \"Tolokers\",\n",
    "        \"Flickr\",\n",
    "        \"Yelp\",\n",
    "        \"Amazon-ratings\",\n",
    "        \"genius\",\n",
    "        \"cora\",\n",
    "        \"CiteSeer\",\n",
    "        \"dblp\",\n",
    "        \"Computers\",\n",
    "        \"PubMed\",\n",
    "        \"pubmed\",\n",
    "        \"Reddit\",\n",
    "        \"cora_ml\",\n",
    "        \"Cora\",\n",
    "        \"Reddit2\",\n",
    "        \"CS\",\n",
    "        \"Photo\",\n",
    "        \"Physics\",\n",
    "        \"citeseer\"\n",
    "    ]\n",
    " \n",
    "    \n",
    "#     ALL_DATASETs= [\"karate\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        \n",
    "        result_file = open(\"Results/LINKX.txt\",'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "            if data.num_nodes>100000:\n",
    "                accs.append(-1)\n",
    "                itrs.append(-1)\n",
    "                break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                              \n",
    "            accuracy, itr, _ = LINKXperformanceSampler(data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f28ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ipynb.fs.full.Dataset import generate_synthetic2homophily\n",
    "import torch_geometric.utils.homophily as homophily\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb2487dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora - 10 loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "loaded /scratch/gilbreth/das90/Dataset/AGSGNNstruc/Cora1000.050.250.50.310True\n",
      "Node Homophily: 0.14723201096057892\n",
      "Edge Homophily: 0.14734117686748505\n",
      "Edge_insensitive Homophily: 0.005595000926405191\n",
      "Degree:  191.96349206349205\n",
      "[0.3416289592760181, 0.32805429864253394, 0.34841628959276016, 0.3416289592760181, 0.3506787330316742] [250, 250, 250, 250, 250]\n",
      "acc 34.2081 \\pm 0.7889 itr 250 sd 0\n",
      "Runtime:  5.181512832641602\n"
     ]
    }
   ],
   "source": [
    "def ablation(num_run = 1):\n",
    "    \n",
    "    #SYN_NAME = random.randint(0,1000)\n",
    "\n",
    "    ALL_DATASETs= [\n",
    "        \"Tolokers\",\"Computers\",\"Photo\"\n",
    "    ]\n",
    "    \n",
    "    ALL_DATASETs= [\"Cora\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/LINKX-GCNablation.txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        \n",
    "        random_state = 10\n",
    "        \n",
    "        print(DATASET_NAME,\"-\",random_state, end=' ')\n",
    "        \n",
    "        \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i)   \n",
    "            \n",
    "            d = 100\n",
    "            h =0.05\n",
    "            train=0.3\n",
    "            balance=True\n",
    "            h2 = 0.25\n",
    "            ratio = 0.50\n",
    "                                    \n",
    "#             global data_filename_extension\n",
    "#             data_filename_extension = str(d)+str(h)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "#             data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "#             if os.path.exists(data_filename):\n",
    "#                 data = torch.load(data_filename)                \n",
    "#                 print(\"loaded \"+data_filename)\n",
    "#             else:\n",
    "#                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False, balance=balance)\n",
    "# #                 data = generate_synthetic(data, d=d, h=h, train=train, random_state=random_state, log=False)\n",
    "#                 torch.save(data,data_filename)\n",
    "#                 print(\"saved \"+data_filename)\n",
    "        \n",
    "            global data_filename_extension\n",
    "            data_filename_extension = str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)+'.weight'            \n",
    "            data_filename = DIR+'AGSGNNstruc/'+DATASET_NAME+str(d)+str(h)+str(h2)+str(ratio)+str(train)+str(random_state)+str(balance)\n",
    "            \n",
    "            if os.path.exists(data_filename):\n",
    "                data = torch.load(data_filename)                \n",
    "                print(\"loaded \"+data_filename)\n",
    "            else:\n",
    "                data = generate_synthetic2homophily(data, d=d, h1=h, h2=h2, ratio=ratio, train=train, random_state=random_state, log=False, balance=balance)                 \n",
    "                torch.save(data,data_filename)\n",
    "                print(\"saved \"+data_filename)\n",
    "    \n",
    "            ##Sparsifiy\n",
    "            #data = random_sparsify(data, 13, log = True)\n",
    "#             data = sparsify(data, log = True, method = 'submodular', metric= 'cosine')\n",
    "                        \n",
    "#             data1 = sparsify(copy.deepcopy(data), log = True, method = 'submodular', metric= 'cosine')\n",
    "#             data = sparsify(data, log = True, method = 'nn', metric= 'cosine')                         \n",
    "#             data.edge_index = torch.cat((data.edge_index, data1.edge_index), dim=1)\n",
    "            \n",
    "            #optional for making undirected graph\n",
    "            (row, col) = data.edge_index\n",
    "            data.edge_index = torch.stack((torch.cat((row, col),dim=0),torch.cat((col, row),dim=0)),dim=0)\n",
    "            data.edge_index = torch_geometric.utils.coalesce(data.edge_index)\n",
    "            \n",
    "            if True:\n",
    "                print(\"Node Homophily:\", homophily(data.edge_index, data.y, method='node'))\n",
    "                print(\"Edge Homophily:\", homophily(data.edge_index, data.y, method='edge'))\n",
    "                print(\"Edge_insensitive Homophily:\", homophily(data.edge_index, data.y, method='edge_insensitive'))    \n",
    "                print(\"Degree: \", data.num_edges / data.num_nodes)\n",
    "\n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 250\n",
    "            else:\n",
    "                max_epochs = 20\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon','cornell5','penn94','johnshopkins55','amherst41']:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "                              \n",
    "            accuracy, itr, _ = LINKXperformanceSampler(data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4], test_neighbors=[8,4])\n",
    "\n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs)*100:0.4f} \\pm {np.std(accs)*100:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs)*100:0.4f} \\pm {np.std(accs)*10:0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "    return \n",
    "\n",
    "st_time = time.time()\n",
    "ablation(num_run=5)\n",
    "en_time = time.time()\n",
    "\n",
    "print(\"Runtime: \", en_time-st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a496f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311cu117pyg200",
   "language": "python",
   "name": "py311cu117pyg200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
