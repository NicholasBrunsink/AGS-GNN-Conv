{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44415f5a",
   "metadata": {},
   "source": [
    "# AGS-GNN Graph Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ac319c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Submodular')\n",
    "\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "782d8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11e7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--recompute', type=bool, default=False)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_true', default=True)    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38ae0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f832",
   "metadata": {},
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbd4df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv, ChebConv\n",
    "from torch_geometric.nn import GraphConv, TransformerConv\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ipynb.fs.full.SpatialConv import SpatialConv\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb2011",
   "metadata": {},
   "source": [
    "## Homophilic GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2402dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomophilicNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels, end_hidden):\n",
    "        super().__init__()\n",
    "        in_channels = num_features\n",
    "        out_channels = num_classes\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "#         self.lin = torch.nn.Linear(3 * hidden_channels, end_hidden)\n",
    "#         self.lin2 = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, end_hidden)\n",
    "        self.lin2 = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
    "\n",
    "\n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "#         self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "#         x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "#         x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "#         x = torch.cat([x1, x2, x3], dim=-1)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        \n",
    "        c1 = self.lin(x)\n",
    "        c2 = self.lin2(x)\n",
    "        \n",
    "        return F.relu(c1), c2.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525b2fa",
   "metadata": {},
   "source": [
    "## Heterophilic GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3777acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace this part with ACMGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad18eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterophilicNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels, end_hidden):\n",
    "        super().__init__()\n",
    "        in_channels = num_features\n",
    "        out_channels = num_classes\n",
    "        self.conv1 = ChebConv(in_channels, hidden_channels, K=1, normalization='sym')\n",
    "        self.conv2 = ChebConv(hidden_channels, hidden_channels, K=1, normalization='sym')\n",
    "#         self.conv3 = ChebConv(hidden_channels, hidden_channels, K=1, normalization='sym')\n",
    "#         self.lin = torch.nn.Linear(3 * hidden_channels, end_hidden)\n",
    "#         self.lin2 = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "\n",
    "        self.lin = torch.nn.Linear(2 * hidden_channels, end_hidden)\n",
    "        self.lin2 = torch.nn.Linear(2 * hidden_channels, out_channels)\n",
    "\n",
    "\n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "#         self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "#         x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "#         x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "#         x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        \n",
    "        c1 = self.lin(x)\n",
    "        c2 = self.lin2(x)\n",
    "        \n",
    "        return F.relu(c1), c2.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ef962",
   "metadata": {},
   "source": [
    "## Combination Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b24c3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_classes, hidden_channels=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        hidden = int(hidden_channels/2)        \n",
    "        self.gnn1 = HomophilicNet(num_features, num_classes, hidden_channels, hidden)\n",
    "#         self.gnn2 = HeterophilicNet(num_features, num_classes, hidden_channels, hidden)\n",
    "        self.gnn2 = HomophilicNet(num_features, num_classes, hidden_channels, hidden)\n",
    "\n",
    "        self.p = dropout\n",
    "        self.com_lin = nn.Linear(hidden*2, num_classes)      \n",
    "        \n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "        #out = model(batch_data.x, batch_data.edge_index, batch_data.edge_weight)\n",
    "        #out = model(batch_data.x, batch_data.edge_index)\n",
    "        \n",
    "        x1, x1c1 = self.gnn1(batch_data[0].x, batch_data[0].edge_index, batch_data[0].weight)\n",
    "        x2, x2c2 = self.gnn2(batch_data[1].x, batch_data[1].edge_index, batch_data[1].weight)\n",
    "\n",
    "#         x1 = self.gnn1(batch_data[0].x, batch_data[0].edge_index)\n",
    "#         x2 = self.gnn2(batch_data[1].x, batch_data[1].edge_index)\n",
    "\n",
    "        a1 = F.relu(x1)\n",
    "        a1 = F.dropout(a1, p=self.p, training=self.training)\n",
    "        \n",
    "        s1 = F.relu(x2)        \n",
    "        s1 = F.dropout(s1, p=self.p, training=self.training)\n",
    "        \n",
    "        batch_size = batch_data[0].batch_size        \n",
    "        x = torch.cat([a1[:batch_size,:], s1[:batch_size,:]], dim=-1)\n",
    "        x = self.com_lin(x)\n",
    "        \n",
    "        #return x\n",
    "    \n",
    "        return x.log_softmax(dim=-1), x1c1, x2c2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b17b8e",
   "metadata": {},
   "source": [
    "## GNN Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e11e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.a2AGS_Graph_Sampler import AGSGraphSampler\n",
    "from torch_geometric.loader import NeighborSampler, NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83409b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def prediction(y_pred_seed, y_pred_hm, y_pred_ht):\n",
    "    \n",
    "    all_tensors = [y_pred_seed, y_pred_hm, y_pred_ht]\n",
    "    final_predictions = []\n",
    "\n",
    "    for i in range(len(y_pred_seed)):\n",
    "        values_at_index = [tensor[i].item() for tensor in all_tensors]\n",
    "        counter = Counter(values_at_index)\n",
    "        most_common_values = counter.most_common()\n",
    "\n",
    "        # Check if there's a tie\n",
    "        if len(most_common_values) > 1 and most_common_values[0][1] == most_common_values[1][1]:\n",
    "            #selected_value = random.choice([value for value, count in most_common_values[:2]])\n",
    "            selected_value = y_pred_seed[i].item()\n",
    "        else:\n",
    "            selected_value = most_common_values[0][0]\n",
    "\n",
    "        final_predictions.append(selected_value)\n",
    "            \n",
    "    return torch.LongTensor(final_predictions)\n",
    "\n",
    "y_pred_seed = torch.tensor([0, 1, 2, 2, 1])\n",
    "y_pred_hm = torch.tensor([2, 1, 1, 0, 2])\n",
    "y_pred_ht = torch.tensor([0, 2, 1, 1, 0])\n",
    "\n",
    "\n",
    "# prediction(y_pred_seed,y_pred_hm,y_pred_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ac415e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, mask, name='Train', channel='all'):\n",
    "    if args.log_info:\n",
    "        pbar = tqdm(total=sum(mask).item())\n",
    "        pbar.set_description(f'Evaluating {name}')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct=0\n",
    "    total_examples=0\n",
    "    \n",
    "    with torch.no_grad():                  \n",
    "    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "            batch_data = [b_data.to(device) for b_data in batch_data]\n",
    "            batch_size = batch_data[0].batch_size\n",
    "            batch_data[0].weight = batch_data[0].edge_norm * batch_data[0].edge_weight\n",
    "            batch_data[1].weight = batch_data[1].edge_norm * batch_data[1].edge_weight\n",
    "            \n",
    "            out, out1, out2 = model(batch_data)\n",
    "                \n",
    "            #print(out.shape, out1.shape, out2.shape)\n",
    "            \n",
    "            y_pred_seed = out[:batch_size].argmax(dim=-1).cpu()\n",
    "            y_pred_hm = out1[:batch_size].argmax(dim=-1).cpu()\n",
    "            y_pred_ht = out2[:batch_size].argmax(dim=-1).cpu()\n",
    "            \n",
    "            #print(y_pred_seed.shape,y_pred_hm.shape, y_pred_ht.shape)\n",
    "            \n",
    "            \n",
    "            y_true = batch_data[0].y[:batch_size]\n",
    "            \n",
    "            if name == 'Train':\n",
    "                t_mask = batch_data[0].train_mask[:batch_size]\n",
    "            elif name == 'Validation':\n",
    "                t_mask = batch_data[0].val_mask[:batch_size]\n",
    "            else:\n",
    "                t_mask = batch_data[0].test_mask[:batch_size]\n",
    "\n",
    "            if channel=='sd':\n",
    "                y_pred = y_pred_seed\n",
    "            elif channel=='hm':\n",
    "                y_pred = y_pred_hm\n",
    "            elif channel=='ht':\n",
    "                y_pred = y_pred_ht\n",
    "            else:\n",
    "                y_pred = prediction(y_pred_seed, y_pred_hm, y_pred_ht)\n",
    "\n",
    "            #print(y_pred)                \n",
    "            correct = y_pred.eq(y_true.cpu())\n",
    "            total_correct+= correct[t_mask].sum().item()\n",
    "            \n",
    "            items = t_mask.sum().item()\n",
    "            total_examples+= items           \n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(items)\n",
    "    \n",
    "    if args.log_info:\n",
    "        pbar.close()\n",
    "\n",
    "    return total_correct/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85fa0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/GraphSAINT/GraphSAINT/issues/11\n",
    "    \n",
    "def train(DATASET_NAME,model, data, dataset, epochs=1, channel='all'): #'all', 'hm', 'ht', 'sd'\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    \n",
    "    if data.y.ndim == 1:\n",
    "        criterion = torch.nn.CrossEntropyLoss() #regular logits as output\n",
    "#         criterion = torch.nn.NLLLoss() ## if log softmax used as activation\n",
    "    else:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()     #multillabel\n",
    "    \n",
    "    row, col = data.edge_index\n",
    "    data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree.\n",
    "    \n",
    "    sampler_dir = DIR+'AGSGSAINTCHEB/'+DATASET_NAME\n",
    "    if not os.path.exists(sampler_dir):\n",
    "        os.makedirs(sampler_dir)\n",
    "        \n",
    "    batch_size= min(1024, data.num_nodes)\n",
    "    #num_steps=math.ceil(sum (data.train_mask)/batch_size)    \n",
    "    num_steps=math.ceil(data.num_nodes/batch_size)\n",
    "    num_workers = 8  if data.num_nodes>50000 else 0\n",
    "    \n",
    "    sample_func =['wrw', 'wrw']\n",
    "    weight_func =[\n",
    "        {'exact':False,'weight':'fastlink'}, #exact for exact size to the batch\n",
    "        {'exact':False,'weight':'fastlink'}\n",
    "    ]\n",
    "\n",
    "    params={'knn':{'metric':'cosine'},\n",
    "            'submodular':{'metric':'cosine'},\n",
    "            'link-nn':{'value':'min'},\n",
    "            'link-sub':{'value':'max'},\n",
    "            'disjoint':{'value':'mst'},\n",
    "           }\n",
    "    \n",
    "    loader = AGSGraphSampler(\n",
    "        data, batch_size=batch_size, walk_length=2, num_steps=num_steps, sample_coverage=100,\n",
    "        num_workers=num_workers,log=args.log_info,save_dir=sampler_dir,recompute = args.recompute, shuffle = False,\n",
    "        sample_func = sample_func, weight_func=weight_func, params=params)\n",
    "        \n",
    "    \n",
    "    #for evaluation\n",
    "    train_num_steps = int(torch.ceil(sum(data.train_mask)/batch_size))\n",
    "    val_num_steps = int(torch.ceil(sum(data.val_mask)/batch_size))\n",
    "    test_num_steps = int(torch.ceil(sum(data.test_mask)/batch_size))\n",
    "    \n",
    "    train_loader = AGSGraphSampler(\n",
    "        data, input_nodes = data.train_mask, batch_size=batch_size, walk_length=2, num_steps=train_num_steps, \n",
    "        sample_coverage=100,num_workers=num_workers,log=args.log_info,save_dir=sampler_dir,\n",
    "        recompute = False, shuffle = False,sample_func = sample_func, weight_func=weight_func, params=params)\n",
    "    \n",
    "    \n",
    "    val_loader = AGSGraphSampler(\n",
    "        data, input_nodes = data.val_mask, batch_size=batch_size, walk_length=2, num_steps=val_num_steps, \n",
    "        sample_coverage=100,num_workers=num_workers,log=args.log_info,save_dir=sampler_dir,\n",
    "        recompute = False, shuffle = False,sample_func = sample_func, weight_func=weight_func, params=params)\n",
    "        \n",
    "    test_loader = AGSGraphSampler(\n",
    "        data, input_nodes = data.test_mask, batch_size=batch_size, walk_length=2, num_steps=test_num_steps, \n",
    "        sample_coverage=100,num_workers=num_workers,log=args.log_info,save_dir=sampler_dir,\n",
    "        recompute = False, shuffle = False,sample_func = sample_func, weight_func=weight_func, params=params)\n",
    "            \n",
    "    best_acc=0    \n",
    "    train_losses = []; val_accuracies = []; train_accuracies = []; test_accuracies = []    \n",
    "    max_iteration = epochs    \n",
    "    \n",
    "    th_node = 10000\n",
    "    \n",
    "    if data.num_nodes<th_node:  \n",
    "        test_data = data.clone()\n",
    "        test_data.weight = loader.edge_norm * test_data.edge_weight\n",
    "        test_data.seed_node=torch.arange(test_data.num_nodes)  \n",
    "        test_data.batch_size=test_data.seed_node.shape[0]\n",
    "        all_data = [test_data.to(device), test_data.to(device)]    \n",
    "        \n",
    "        if args.log_info:\n",
    "            print(all_data)\n",
    "        \n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=batch_size*num_steps)\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        model.train()\n",
    "        #if model.__class__.__name__ == 'Net':\n",
    "            #model.set_aggr('add' if args.use_normalization else 'mean')\n",
    "        model.gnn1.set_aggr('add' if args.use_normalization else 'mean')\n",
    "        model.gnn2.set_aggr('add' if args.use_normalization else 'mean')\n",
    "\n",
    "        total_loss = total_examples = 0\n",
    "        total_loss_seed = total_loss_hm = total_loss_ht = 0\n",
    "        total_seed_example = total_hm_example = total_ht_example = 0\n",
    "                    \n",
    "        for i,batch_data in enumerate(loader):\n",
    "            \n",
    "            #print(batch_data);\n",
    "            #print(\"*\"*50)            \n",
    "            \n",
    "            batch_data = [b_data.to(device) for b_data in batch_data]\n",
    "                    \n",
    "            batch_size = batch_data[0].batch_size\n",
    "            mask = batch_data[0].train_mask[:batch_size]\n",
    "            y_true = batch_data[0].y[:batch_size]\n",
    "            \n",
    "            if torch.sum(mask) == 0:\n",
    "                if args.log_info:\n",
    "                    print(\"no training mask in seed node\")\n",
    "#                 continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if args.use_normalization:                 \n",
    "                \n",
    "                batch_data[0].weight = batch_data[0].edge_norm * batch_data[0].edge_weight\n",
    "                batch_data[1].weight = batch_data[1].edge_norm * batch_data[1].edge_weight\n",
    "                out, out1, out2 = model(batch_data)                            \n",
    "                #print(out.shape, out1.shape, out2.shape)\n",
    "                \n",
    "                                \n",
    "#                 print(out[:batch_size][mask].shape, y_true[mask].shape)                \n",
    "#                 print(loss.shape)\n",
    "#                 print(batch_data[0].node_norm.shape)\n",
    "#                 print(batch_data[0].node_norm[:batch_size].shape)\n",
    "#                 loss = criterion(out, batch_data.y, reduction='none')\n",
    "\n",
    "                #--------- loss computation seed nodes --------- #                \n",
    "                loss_seed = F.nll_loss(out[:batch_size][mask], y_true[mask], reduction='none')\n",
    "                loss_seed = (loss_seed * batch_data[0].node_norm[:batch_size][mask]).sum()\n",
    "        \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "            \n",
    "                loss_hm = F.nll_loss(out1[batch_data[0].train_mask], batch_data[0].y[batch_data[0].train_mask], reduction='none')\n",
    "                loss_hm = (loss_hm * batch_data[0].node_norm[batch_data[0].train_mask]).sum()\n",
    "                \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "                loss_ht = F.nll_loss(out2[batch_data[1].train_mask], batch_data[1].y[batch_data[1].train_mask], reduction='none')\n",
    "                loss_ht = (loss_ht * batch_data[1].node_norm[batch_data[1].train_mask]).sum()\n",
    "                \n",
    "                if channel=='sd':loss = loss_seed\n",
    "                elif channel=='hm':loss = loss_hm\n",
    "                elif channel=='ht':loss = loss_ht\n",
    "                else:loss = loss_seed + loss_hm + loss_ht                \n",
    "\n",
    "            else:\n",
    "                batch_data[0].weight = None\n",
    "                batch_data[1].weight = None\n",
    "                    \n",
    "                out, out1, out2 = model(batch_data)                            \n",
    "                #print(out.shape, out1.shape, out2.shape)\n",
    "                \n",
    "                mask = batch_data[0].train_mask[:batch_size]\n",
    "                y_true = batch_data[0].y[:batch_size]\n",
    "                \n",
    "                #--------- loss computation seed nodes --------- #                \n",
    "                loss_seed = F.nll_loss(out[:batch_size][mask], y_true[mask])\n",
    "                \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "                loss_hm = F.nll_loss(out1[batch_data[0].train_mask], batch_data[0].y[batch_data[0].train_mask])\n",
    "                \n",
    "                #--------- loss computation homophily nodes --------- #\n",
    "                loss_ht = F.nll_loss(out2[batch_data[1].train_mask], batch_data[1].y[batch_data[1].train_mask])\n",
    "                #loss = criterion(out[batch_data.train_mask], batch_data.y[batch_data.train_mask])\n",
    "                \n",
    "                if channel=='sd':loss = loss_seed\n",
    "                elif channel=='hm':loss = loss_hm\n",
    "                elif channel=='ht':loss = loss_ht\n",
    "                else:loss = loss_seed + loss_hm + loss_ht\n",
    "                \n",
    "                \n",
    "                        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss_seed+= loss_seed.item()*batch_size\n",
    "            total_loss_hm+= loss_hm.item()*sum(batch_data[0].train_mask).item()\n",
    "            total_loss_ht+= loss_ht.item()*sum(batch_data[1].train_mask).item()\n",
    "            \n",
    "            total_seed_example+= sum(mask).item()\n",
    "            total_hm_example+=sum(batch_data[0].train_mask).item()\n",
    "            total_ht_example+=sum(batch_data[1].train_mask).item()\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_size)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "            \n",
    "        total_loss_seed /= max(total_seed_example,1)\n",
    "        total_loss_hm /= max(total_hm_example, 1)\n",
    "        total_loss_ht /= max(total_ht_example, 1)\n",
    "        \n",
    "        total_loss=total_loss_seed+total_loss_hm+total_loss_ht\n",
    "        total_examples = total_seed_example + total_hm_example + total_ht_example\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Loss:{total_loss:0.4f} seed:{total_loss_seed:0.4f} hm:{total_loss_hm:0.4f} ht:{total_loss_ht:0.4f}')\n",
    "            print(f'Example:{total_examples:1d} seed:{total_seed_example:1d} hm:{total_hm_example:1d} ht:{total_ht_example:1d}')\n",
    "        \n",
    "        if channel=='sd':train_losses.append(total_loss_seed)\n",
    "        elif channel=='hm':train_losses.append(total_loss_hm)\n",
    "        elif channel=='ht':train_losses.append(total_loss_ht)\n",
    "        else:train_losses.append(total_loss)\n",
    "                        \n",
    "        accs = [0,0,0]\n",
    "        \n",
    "        model.eval()\n",
    "#         model.gnn1.set_aggr('mean')\n",
    "#         model.gnn2.set_aggr('mean')\n",
    "        model.gnn1.set_aggr('add' if args.use_normalization else 'mean')\n",
    "        model.gnn2.set_aggr('add' if args.use_normalization else 'mean')\n",
    "        \n",
    "        \n",
    "        if data.num_nodes<th_node:\n",
    "                        \n",
    "            with torch.no_grad():\n",
    "                out, out1, out2 = model(all_data)\n",
    "                \n",
    "                #print(out.shape, out1.shape, out2.shape)\n",
    "                y_pred_seed = out.argmax(dim=-1).cpu()\n",
    "                y_pred_hm = out1.argmax(dim=-1).cpu()\n",
    "                y_pred_ht = out2.argmax(dim=-1).cpu()\n",
    "                \n",
    "                if channel=='sd':\n",
    "                    y_pred = y_pred_seed\n",
    "                elif channel=='hm':\n",
    "                    y_pred = y_pred_hm\n",
    "                elif channel=='ht':\n",
    "                    y_pred = y_pred_ht\n",
    "                else:\n",
    "                    y_pred = prediction(y_pred_seed, y_pred_hm, y_pred_ht)\n",
    "                \n",
    "                #print(y_pred)                \n",
    "                correct = y_pred.eq(data.y.cpu())\n",
    "                accs = []\n",
    "                for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "                    accs.append(correct[mask].sum().item() / mask.sum().item()) \n",
    "                                                    \n",
    "        else:\n",
    "            accs[0] = test(model, train_loader, data.train_mask, name='Train', channel=channel)\n",
    "            accs[1] = test(model, val_loader, data.val_mask, name='Validation', channel=channel)\n",
    "            accs[2] = test(model, test_loader, data.test_mask, name='Test', channel=channel)\n",
    "            \n",
    "        train_accuracies.append(accs[0])\n",
    "        val_accuracies.append(accs[1])\n",
    "        test_accuracies.append(accs[2])\n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {accs[0]:.4f}, Val: {accs[1]:.4f}, Test: {accs[2]:.4f}, Std dev: {std_dev:.4f}')\n",
    "        \n",
    "        if epoch>=5 and std_dev<=1e-4:\n",
    "            if args.log_info:\n",
    "                print(\"Iteration for convergence: \", epoch)\n",
    "            max_iteration = epoch\n",
    "            break\n",
    "    \n",
    "    if args.log_info:\n",
    "        save_plot([train_losses, train_accuracies, val_accuracies, test_accuracies], labels=['Loss','Train','Validation','Test'], name='Results/AGSGSValidation', yname='Accuracy', xname='Epoch')\n",
    "        print (\"Best Validation Accuracy, \",max(val_accuracies))\n",
    "        print (\"Best Test Accuracy, \",max(test_accuracies))\n",
    "    \n",
    "    if data.num_nodes<th_node:  \n",
    "        del all_data\n",
    "    \n",
    "    return max(test_accuracies), max_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34bee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGSGSperformanceSampler(DATASET_NAME,data, dataset, num_classes, epochs=1,channel='all'):\n",
    "    \n",
    "    model = AGSGNN(data.x.shape[1], num_classes, hidden_channels=256).to(device)            \n",
    "    \n",
    "    if args.log_info:\n",
    "        print(model)\n",
    "\n",
    "    itr, accuracy = train(DATASET_NAME, model, data, dataset, epochs, channel)\n",
    "    \n",
    "    return itr, accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ce2248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "def adj_feature(data):    \n",
    "    adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "    edges = data.edge_index.t()\n",
    "    adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "    adj_mat[edges[:,1], edges[:,0]] = 1\n",
    "    \n",
    "#     n_components = data.x.shape[1]\n",
    "    n_components = min(256, data.x.shape[1], data.num_nodes)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    x = svd.fit_transform(adj_mat)\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    x.shape    \n",
    "    \n",
    "    return x\n",
    "\n",
    "# x = adj_feature(data)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "586728b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N  2708  E  10556  d  3.8980797636632203 0.825157880783081 0.8099659085273743 0.7657181620597839 -0.06587088108062744 Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "AGSGNN(\n",
      "  (gnn1): HomophilicNet(\n",
      "    (conv1): GraphConv(1433, 256)\n",
      "    (conv2): GraphConv(256, 256)\n",
      "    (lin): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (lin2): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      "  (gnn2): HomophilicNet(\n",
      "    (conv1): GraphConv(1433, 256)\n",
      "    (conv2): GraphConv(256, 256)\n",
      "    (lin): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (lin2): Linear(in_features=512, out_features=7, bias=True)\n",
      "  )\n",
      "  (com_lin): Linear(in_features=256, out_features=7, bias=True)\n",
      ")\n",
      "loading saved norm\n",
      "Loading weights  fastlink\n",
      "loading saved norm\n",
      "Loading weights  fastlink\n",
      "loading saved norm\n",
      "Loading weights  fastlink\n",
      "loading saved norm\n",
      "Loading weights  fastlink\n",
      "[Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_weight=[10556], weight=[10556], seed_node=[2708], batch_size=2708), Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_weight=[10556], weight=[10556], seed_node=[2708], batch_size=2708)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01:  82%|████████▏ | 2520/3072 [00:00<00:00, 6443.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.1255 seed:1.0193 hm:0.0532 ht:0.0530\n",
      "Example:393 seed:131 hm:131 ht:131\n",
      "Epoch: 001, Train Loss: 0.1700, Train: 0.1643, Val: 0.1620, Test: 0.1490, Std dev: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: : 2545it [00:00, 4677.54it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.1157 seed:1.0121 hm:0.0518 ht:0.0518\n",
      "Example:390 seed:130 hm:130 ht:130\n",
      "Epoch: 002, Train Loss: 0.1729, Train: 0.2714, Val: 0.1700, Test: 0.1640, Std dev: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: : 2542it [00:00, 5836.46it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.0771 seed:0.9900 hm:0.0436 ht:0.0436\n",
      "Example:333 seed:111 hm:111 ht:111\n",
      "Epoch: 003, Train Loss: 0.1332, Train: 0.4786, Val: 0.2300, Test: 0.2310, Std dev: 0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|█████████▉| 2561/2571 [00:00<00:00, 5756.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.1135 seed:1.0091 hm:0.0522 ht:0.0522\n",
      "Example:390 seed:130 hm:130 ht:130\n",
      "Epoch: 004, Train Loss: 0.1855, Train: 0.4286, Val: 0.2140, Test: 0.2300, Std dev: 0.0184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: : 2578it [00:00, 5981.22it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.1548 seed:1.0287 hm:0.0630 ht:0.0631\n",
      "Example:480 seed:160 hm:160 ht:160\n",
      "Epoch: 005, Train Loss: 0.1948, Train: 0.4643, Val: 0.2460, Test: 0.2490, Std dev: 0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06:  99%|█████████▊| 2549/2583 [00:00<00:00, 5777.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.1320 seed:1.0216 hm:0.0552 ht:0.0552\n",
      "Example:420 seed:140 hm:140 ht:140\n",
      "Epoch: 006, Train Loss: 0.1623, Train: 0.5429, Val: 0.2280, Test: 0.2270, Std dev: 0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: : 2572it [00:00, 5819.83it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.0923 seed:0.9915 hm:0.0503 ht:0.0505\n",
      "Example:399 seed:133 hm:133 ht:133\n",
      "Epoch: 007, Train Loss: 0.1459, Train: 0.6286, Val: 0.2220, Test: 0.2340, Std dev: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08:  99%|█████████▊| 2565/2604 [00:00<00:00, 5058.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.1012 seed:0.9932 hm:0.0538 ht:0.0541\n",
      "Example:423 seed:141 hm:141 ht:141\n",
      "Epoch: 008, Train Loss: 0.1849, Train: 0.7500, Val: 0.3260, Test: 0.3290, Std dev: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|█████████▉| 2580/2589 [00:00<00:00, 6959.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.0986 seed:0.9913 hm:0.0535 ht:0.0538\n",
      "Example:423 seed:141 hm:141 ht:141\n",
      "Epoch: 009, Train Loss: 0.1875, Train: 0.7500, Val: 0.3500, Test: 0.3440, Std dev: 0.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: : 2567it [00:00, 6432.58it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.0302 seed:0.9416 hm:0.0441 ht:0.0445\n",
      "Example:363 seed:121 hm:121 ht:121\n",
      "Epoch: 010, Train Loss: 0.1076, Train: 0.7286, Val: 0.3520, Test: 0.3410, Std dev: 0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: : 2580it [00:00, 4433.27it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.0355 seed:0.9450 hm:0.0450 ht:0.0455\n",
      "Example:381 seed:127 hm:127 ht:127\n",
      "Epoch: 011, Train Loss: 0.1355, Train: 0.7714, Val: 0.3820, Test: 0.3830, Std dev: 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|█████████▉| 2570/2577 [00:00<00:00, 6595.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.9819 seed:0.8749 hm:0.0531 ht:0.0539\n",
      "Example:483 seed:161 hm:161 ht:161\n",
      "Epoch: 012, Train Loss: 0.1708, Train: 0.8357, Val: 0.5200, Test: 0.4960, Std dev: 0.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13:  99%|█████████▊| 2552/2589 [00:00<00:00, 5832.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.9094 seed:0.8204 hm:0.0438 ht:0.0453\n",
      "Example:414 seed:138 hm:138 ht:138\n",
      "Epoch: 013, Train Loss: 0.1118, Train: 0.8571, Val: 0.5260, Test: 0.5000, Std dev: 0.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|█████████▉| 2558/2565 [00:00<00:00, 6110.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.8069 seed:0.7253 hm:0.0403 ht:0.0413\n",
      "Example:414 seed:138 hm:138 ht:138\n",
      "Epoch: 014, Train Loss: 0.1145, Train: 0.8429, Val: 0.4720, Test: 0.4730, Std dev: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  99%|█████████▉| 2551/2574 [00:00<00:00, 5387.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.7502 seed:0.6678 hm:0.0407 ht:0.0417\n",
      "Example:444 seed:148 hm:148 ht:148\n",
      "Epoch: 015, Train Loss: 0.1168, Train: 0.8857, Val: 0.5380, Test: 0.5240, Std dev: 0.1060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|█████████▉| 2557/2559 [00:00<00:00, 5186.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.6713 seed:0.5979 hm:0.0357 ht:0.0377\n",
      "Example:420 seed:140 hm:140 ht:140\n",
      "Epoch: 016, Train Loss: 0.0998, Train: 0.9000, Val: 0.5760, Test: 0.5710, Std dev: 0.1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: : 2570it [00:00, 6695.15it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.5317 seed:0.4718 hm:0.0290 ht:0.0308\n",
      "Example:396 seed:132 hm:132 ht:132\n",
      "Epoch: 017, Train Loss: 0.0754, Train: 0.9143, Val: 0.5840, Test: 0.5750, Std dev: 0.1275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:  98%|█████████▊| 2571/2622 [00:00<00:00, 6534.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.4353 seed:0.3827 hm:0.0255 ht:0.0272\n",
      "Example:381 seed:127 hm:127 ht:127\n",
      "Epoch: 018, Train Loss: 0.0482, Train: 0.9357, Val: 0.5880, Test: 0.5720, Std dev: 0.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19:  99%|█████████▉| 2552/2568 [00:00<00:00, 3937.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.3640 seed:0.3150 hm:0.0235 ht:0.0256\n",
      "Example:393 seed:131 hm:131 ht:131\n",
      "Epoch: 019, Train Loss: 0.0546, Train: 0.9500, Val: 0.6200, Test: 0.6150, Std dev: 0.1434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20:  99%|█████████▉| 2551/2571 [00:00<00:00, 4950.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.2584 seed:0.2143 hm:0.0207 ht:0.0233\n",
      "Example:435 seed:145 hm:145 ht:145\n",
      "Epoch: 020, Train Loss: 0.0506, Train: 1.0000, Val: 0.7200, Test: 0.6960, Std dev: 0.1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 2553/2553 [00:00<00:00, 5539.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.2201 seed:0.1844 hm:0.0169 ht:0.0187\n",
      "Example:402 seed:134 hm:134 ht:134\n",
      "Epoch: 021, Train Loss: 0.0349, Train: 0.9929, Val: 0.7500, Test: 0.7510, Std dev: 0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: : 2560it [00:00, 4166.62it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.1483 seed:0.1222 hm:0.0123 ht:0.0138\n",
      "Example:372 seed:124 hm:124 ht:124\n",
      "Epoch: 022, Train Loss: 0.0327, Train: 0.9929, Val: 0.7600, Test: 0.7680, Std dev: 0.1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: : 2552it [00:00, 5948.53it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0949 seed:0.0750 hm:0.0092 ht:0.0107\n",
      "Example:345 seed:115 hm:115 ht:115\n",
      "Epoch: 023, Train Loss: 0.0300, Train: 1.0000, Val: 0.7760, Test: 0.7730, Std dev: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: : 2567it [00:00, 6738.33it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.1060 seed:0.0861 hm:0.0096 ht:0.0104\n",
      "Example:357 seed:119 hm:119 ht:119\n",
      "Epoch: 024, Train Loss: 0.0202, Train: 0.9929, Val: 0.7840, Test: 0.7790, Std dev: 0.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|█████████▉| 2529/2538 [00:00<00:00, 4881.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0703 seed:0.0538 hm:0.0079 ht:0.0087\n",
      "Example:363 seed:121 hm:121 ht:121\n",
      "Epoch: 025, Train Loss: 0.0197, Train: 1.0000, Val: 0.7920, Test: 0.7780, Std dev: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26:  99%|█████████▉| 2537/2562 [00:00<00:00, 6111.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0414 seed:0.0278 hm:0.0067 ht:0.0069\n",
      "Example:423 seed:141 hm:141 ht:141\n",
      "Epoch: 026, Train Loss: 0.0142, Train: 1.0000, Val: 0.7880, Test: 0.7790, Std dev: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: : 2564it [00:00, 4213.09it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0339 seed:0.0236 hm:0.0047 ht:0.0056\n",
      "Example:357 seed:119 hm:119 ht:119\n",
      "Epoch: 027, Train Loss: 0.0093, Train: 1.0000, Val: 0.7800, Test: 0.7780, Std dev: 0.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28:  99%|█████████▊| 2538/2571 [00:00<00:00, 3790.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0290 seed:0.0194 hm:0.0049 ht:0.0046\n",
      "Example:393 seed:131 hm:131 ht:131\n",
      "Epoch: 028, Train Loss: 0.0097, Train: 1.0000, Val: 0.7860, Test: 0.7790, Std dev: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 2574it [00:00, 4316.62it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0198 seed:0.0121 hm:0.0037 ht:0.0039\n",
      "Example:384 seed:128 hm:128 ht:128\n",
      "Epoch: 029, Train Loss: 0.0071, Train: 1.0000, Val: 0.7840, Test: 0.7850, Std dev: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|█████████▉| 2557/2565 [00:00<00:00, 4251.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0171 seed:0.0107 hm:0.0030 ht:0.0034\n",
      "Example:396 seed:132 hm:132 ht:132\n",
      "Epoch: 030, Train Loss: 0.0071, Train: 1.0000, Val: 0.7860, Test: 0.7910, Std dev: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|█████████▉| 2544/2547 [00:00<00:00, 6513.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0119 seed:0.0057 hm:0.0031 ht:0.0031\n",
      "Example:438 seed:146 hm:146 ht:146\n",
      "Epoch: 031, Train Loss: 0.0060, Train: 1.0000, Val: 0.7800, Test: 0.7900, Std dev: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32:  99%|█████████▉| 2557/2589 [00:00<00:00, 4233.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0149 seed:0.0102 hm:0.0022 ht:0.0026\n",
      "Example:360 seed:120 hm:120 ht:120\n",
      "Epoch: 032, Train Loss: 0.0045, Train: 1.0000, Val: 0.7880, Test: 0.7930, Std dev: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: : 2560it [00:00, 6169.76it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0066 seed:0.0036 hm:0.0014 ht:0.0016\n",
      "Example:411 seed:137 hm:137 ht:137\n",
      "Epoch: 033, Train Loss: 0.0022, Train: 1.0000, Val: 0.8000, Test: 0.7990, Std dev: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34:  99%|█████████▉| 2539/2559 [00:00<00:00, 6558.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0046 seed:0.0019 hm:0.0015 ht:0.0013\n",
      "Example:414 seed:138 hm:138 ht:138\n",
      "Epoch: 034, Train Loss: 0.0023, Train: 1.0000, Val: 0.8020, Test: 0.8010, Std dev: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: : 2556it [00:00, 5632.04it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0053 seed:0.0017 hm:0.0018 ht:0.0017\n",
      "Example:396 seed:132 hm:132 ht:132\n",
      "Epoch: 035, Train Loss: 0.0039, Train: 1.0000, Val: 0.8060, Test: 0.8000, Std dev: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: : 2581it [00:00, 6414.96it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0089 seed:0.0058 hm:0.0014 ht:0.0018\n",
      "Example:399 seed:133 hm:133 ht:133\n",
      "Epoch: 036, Train Loss: 0.0032, Train: 1.0000, Val: 0.7980, Test: 0.7990, Std dev: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37:  99%|█████████▊| 2583/2622 [00:00<00:00, 6417.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0038 seed:0.0015 hm:0.0012 ht:0.0011\n",
      "Example:357 seed:119 hm:119 ht:119\n",
      "Epoch: 037, Train Loss: 0.0025, Train: 1.0000, Val: 0.7920, Test: 0.7960, Std dev: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38:  99%|█████████▉| 2574/2589 [00:00<00:00, 4372.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0037 seed:0.0018 hm:0.0010 ht:0.0010\n",
      "Example:363 seed:121 hm:121 ht:121\n",
      "Epoch: 038, Train Loss: 0.0021, Train: 1.0000, Val: 0.7760, Test: 0.7920, Std dev: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|█████████▉| 2548/2550 [00:00<00:00, 5604.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0033 seed:0.0009 hm:0.0012 ht:0.0012\n",
      "Example:405 seed:135 hm:135 ht:135\n",
      "Epoch: 039, Train Loss: 0.0017, Train: 1.0000, Val: 0.7700, Test: 0.7900, Std dev: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: : 2590it [00:00, 4385.44it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0032 seed:0.0013 hm:0.0009 ht:0.0010\n",
      "Example:432 seed:144 hm:144 ht:144\n",
      "Epoch: 040, Train Loss: 0.0023, Train: 1.0000, Val: 0.7700, Test: 0.7840, Std dev: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: : 2571it [00:00, 6431.44it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0030 seed:0.0016 hm:0.0006 ht:0.0008\n",
      "Example:363 seed:121 hm:121 ht:121\n",
      "Epoch: 041, Train Loss: 0.0018, Train: 1.0000, Val: 0.7660, Test: 0.7780, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42:  99%|█████████▊| 2541/2574 [00:00<00:00, 5656.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0026 seed:0.0011 hm:0.0007 ht:0.0007\n",
      "Example:402 seed:134 hm:134 ht:134\n",
      "Epoch: 042, Train Loss: 0.0012, Train: 1.0000, Val: 0.7640, Test: 0.7740, Std dev: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: : 2573it [00:00, 6248.69it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0015 seed:0.0004 hm:0.0005 ht:0.0005\n",
      "Example:390 seed:130 hm:130 ht:130\n",
      "Epoch: 043, Train Loss: 0.0008, Train: 1.0000, Val: 0.7640, Test: 0.7740, Std dev: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44:  97%|█████████▋| 2537/2610 [00:00<00:00, 4554.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0026 seed:0.0010 hm:0.0008 ht:0.0008\n",
      "Example:423 seed:141 hm:141 ht:141\n",
      "Epoch: 044, Train Loss: 0.0015, Train: 1.0000, Val: 0.7640, Test: 0.7800, Std dev: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: : 2561it [00:00, 6798.78it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0024 seed:0.0010 hm:0.0007 ht:0.0007\n",
      "Example:435 seed:145 hm:145 ht:145\n",
      "Epoch: 045, Train Loss: 0.0015, Train: 1.0000, Val: 0.7780, Test: 0.7860, Std dev: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46:  97%|█████████▋| 2549/2625 [00:00<00:00, 4537.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0017 seed:0.0007 hm:0.0005 ht:0.0005\n",
      "Example:327 seed:109 hm:109 ht:109\n",
      "Epoch: 046, Train Loss: 0.0009, Train: 1.0000, Val: 0.7800, Test: 0.7940, Std dev: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: : 2571it [00:00, 4059.43it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0009 seed:0.0002 hm:0.0003 ht:0.0004\n",
      "Example:414 seed:138 hm:138 ht:138\n",
      "Epoch: 047, Train Loss: 0.0008, Train: 1.0000, Val: 0.7860, Test: 0.7940, Std dev: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: : 2574it [00:00, 6446.83it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0012 seed:0.0004 hm:0.0004 ht:0.0005\n",
      "Example:405 seed:135 hm:135 ht:135\n",
      "Epoch: 048, Train Loss: 0.0010, Train: 1.0000, Val: 0.7860, Test: 0.7940, Std dev: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49:  99%|█████████▉| 2566/2583 [00:00<00:00, 6554.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0013 seed:0.0005 hm:0.0004 ht:0.0004\n",
      "Example:378 seed:126 hm:126 ht:126\n",
      "Epoch: 049, Train Loss: 0.0005, Train: 1.0000, Val: 0.7860, Test: 0.7900, Std dev: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50:  99%|█████████▉| 2546/2577 [00:00<00:00, 5981.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0012 seed:0.0005 hm:0.0003 ht:0.0005\n",
      "Example:369 seed:123 hm:123 ht:123\n",
      "Epoch: 050, Train Loss: 0.0008, Train: 1.0000, Val: 0.7880, Test: 0.7920, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: : 2521it [00:00, 5105.65it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0011 seed:0.0003 hm:0.0004 ht:0.0004\n",
      "Example:387 seed:129 hm:129 ht:129\n",
      "Epoch: 051, Train Loss: 0.0005, Train: 1.0000, Val: 0.7860, Test: 0.7910, Std dev: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: : 2580it [00:00, 6826.05it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0016 seed:0.0007 hm:0.0004 ht:0.0005\n",
      "Example:438 seed:146 hm:146 ht:146\n",
      "Epoch: 052, Train Loss: 0.0013, Train: 1.0000, Val: 0.7860, Test: 0.7910, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53:  99%|█████████▊| 2543/2580 [00:00<00:00, 5584.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0009 seed:0.0003 hm:0.0003 ht:0.0003\n",
      "Example:381 seed:127 hm:127 ht:127\n",
      "Epoch: 053, Train Loss: 0.0005, Train: 1.0000, Val: 0.7880, Test: 0.7940, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: : 2573it [00:00, 3495.61it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0010 seed:0.0003 hm:0.0004 ht:0.0003\n",
      "Example:402 seed:134 hm:134 ht:134\n",
      "Epoch: 054, Train Loss: 0.0010, Train: 1.0000, Val: 0.7880, Test: 0.7940, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55:  99%|█████████▉| 2563/2589 [00:00<00:00, 4308.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0010 seed:0.0003 hm:0.0003 ht:0.0004\n",
      "Example:381 seed:127 hm:127 ht:127\n",
      "Epoch: 055, Train Loss: 0.0007, Train: 1.0000, Val: 0.7900, Test: 0.7940, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56:  99%|█████████▉| 2533/2553 [00:00<00:00, 3957.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0006 seed:0.0001 hm:0.0002 ht:0.0003\n",
      "Example:360 seed:120 hm:120 ht:120\n",
      "Epoch: 056, Train Loss: 0.0006, Train: 1.0000, Val: 0.7880, Test: 0.7930, Std dev: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57:  97%|█████████▋| 2528/2601 [00:00<00:00, 4601.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0007 seed:0.0002 hm:0.0003 ht:0.0003\n",
      "Example:399 seed:133 hm:133 ht:133\n",
      "Epoch: 057, Train Loss: 0.0005, Train: 1.0000, Val: 0.7860, Test: 0.7870, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: : 2578it [00:00, 4435.45it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0007 seed:0.0002 hm:0.0002 ht:0.0003\n",
      "Example:396 seed:132 hm:132 ht:132\n",
      "Epoch: 058, Train Loss: 0.0005, Train: 1.0000, Val: 0.7780, Test: 0.7840, Std dev: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59:  99%|█████████▉| 2579/2592 [00:00<00:00, 4745.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0008 seed:0.0002 hm:0.0002 ht:0.0004\n",
      "Example:435 seed:145 hm:145 ht:145\n",
      "Epoch: 059, Train Loss: 0.0006, Train: 1.0000, Val: 0.7780, Test: 0.7860, Std dev: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: : 2570it [00:00, 3846.37it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0006 seed:0.0001 hm:0.0002 ht:0.0002\n",
      "Example:429 seed:143 hm:143 ht:143\n",
      "Epoch: 060, Train Loss: 0.0003, Train: 1.0000, Val: 0.7820, Test: 0.7870, Std dev: 0.0001\n",
      "Iteration for convergence:  60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy,  0.806\n",
      "Best Test Accuracy,  0.801\n",
      "0.801 60\n"
     ]
    }
   ],
   "source": [
    "# args.log_info = True\n",
    "# args.recompute = False\n",
    "\n",
    "# DATASET_NAME = 'Cora'\n",
    "# data, dataset = get_data(DATASET_NAME,DIR=None, log=False, h_score=True, split_no=0)\n",
    "# print(data)\n",
    "\n",
    "# channel = 'all' #'all', 'hm', 'ht', 'sd'\n",
    "\n",
    "# if DATASET_NAME in ['Squirrel', 'Chameleon']:\n",
    "#     data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "#     if args.log_info == True:\n",
    "#         print(data.x.shape)\n",
    "    \n",
    "# best_acc, num_iteration, _ =  AGSGSperformanceSampler(DATASET_NAME, data, dataset, dataset.num_classes,epochs=150, channel=channel)\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72e3b1",
   "metadata": {},
   "source": [
    "# Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5dd76e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pokec "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         result_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     80\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[43mbatch_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime spent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m-\u001b[39mstart)\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mbatch_experiments\u001b[0;34m(num_run, channel)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlog_info \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 67\u001b[0m accuracy, itr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mAGSGSperformanceSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m accs\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     70\u001b[0m itrs\u001b[38;5;241m.\u001b[39mappend(itr)\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mAGSGSperformanceSampler\u001b[0;34m(DATASET_NAME, data, dataset, num_classes, epochs, channel)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlog_info:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m----> 8\u001b[0m itr, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itr, accuracy, model\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(DATASET_NAME, model, data, dataset, epochs, channel)\u001b[0m\n\u001b[1;32m    178\u001b[0m total_loss_ht\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_ht\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m\u001b[38;5;28msum\u001b[39m(batch_data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_mask)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    180\u001b[0m total_seed_example\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(mask)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 181\u001b[0m total_hm_example\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    182\u001b[0m total_ht_example\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(batch_data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_mask)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlog_info:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_experiments(num_run=1, channel='all'):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\"reed98\",\"amherst41\",\"genius\",\n",
    "#         \"AmazonProducts\",\n",
    "#         \"cornell5\",\"penn94\",\n",
    "#         \"johnshopkins55\",\n",
    "#         \"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "#         \"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "#         \"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",\n",
    "        'pokec',\n",
    "        'arxiv-year',\n",
    "        'snap-patents',\n",
    "        'twitch-gamer'\n",
    "    ]\n",
    "    \n",
    "    #ALL_DATASETs= [\"Cora\"]    \n",
    "\n",
    "    args.log_info = False\n",
    "    \n",
    "    filename = \"Results/AGSGNNsaintCheb-GS-\"+channel+\".txt\"\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        print(DATASET_NAME, end=' ')\n",
    "                \n",
    "        result_file = open(filename,'a+')        \n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        result_file.close()\n",
    "                \n",
    "        accs = []\n",
    "        itrs = []\n",
    "                \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state=i)   \n",
    "            \n",
    "#             if data.num_nodes>100000:\n",
    "#                 accs.append(-1)\n",
    "#                 itrs.append(-1)\n",
    "#                 break\n",
    "            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "                \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "                \n",
    "            if DATASET_NAME in ['Squirrel', 'Chameleon', \n",
    "                                'cornell5','penn94','johnshopkins55'\n",
    "                               ]:\n",
    "                data.x = torch.cat((data.x, adj_feature(data)), dim=1)\n",
    "                if args.log_info == True:\n",
    "                    print(data.x.shape)\n",
    "                              \n",
    "            accuracy, itr, _ = AGSGSperformanceSampler(DATASET_NAME, data, dataset, num_classes,epochs=max_epochs, channel=channel)\n",
    "            \n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)\n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file = open(filename,'a+')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "\n",
    "start = time.time()\n",
    "batch_experiments(num_run=1, channel='all')\n",
    "end = time.time()\n",
    "print(\"Time spent:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cab8c",
   "metadata": {},
   "source": [
    "## Visualize representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "361d4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    \n",
    "#     n=7\n",
    "#     x = torch.Tensor([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1],[0,1]])\n",
    "#     y = torch.LongTensor([0,0,0, 1, 1, 1, 1])\n",
    "#     edge_index = torch.LongTensor([[1,2],[1,4],[1,5],[2,1],[3,6],[3,7],[4,5],[4,1],[4,6],[4,7],[5,1],[5,4],[5,6],[6,3],[6,4],[6,5],[6,7],[7,3],[7,4],[7,6]]).T\n",
    "#     edge_index = edge_index-1\n",
    "    \n",
    "#     mask = torch.zeros(n, dtype=torch.bool)\n",
    "#     mask[[1,3]] = True\n",
    "    \n",
    "#     test_data = Data(x = x, y = y, edge_index = edge_index, train_mask = mask, test_mask = mask, val_mask = mask)    \n",
    "#     print(test_data)\n",
    "    \n",
    "    \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48f76e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AGS_layer(torch.nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, dropout=0.2):\n",
    "#         super().__init__()\n",
    "#         self.T = 3\n",
    "#         self.p = dropout\n",
    "#         #self.Aconv1 = GCNConv(input_channels, output_channels)        \n",
    "#         #self.Sconv1 = SpatialConv(input_channels, output_channels)\n",
    "        \n",
    "#         self.Aconv1 = GCNConv(input_channels, output_channels)\n",
    "#         self.Sconv1 = SpatialConv(input_channels, output_channels)\n",
    "        \n",
    "#         self.I1 = nn.Linear(input_channels, output_channels)\n",
    "        \n",
    "#         self.layer_norm_a1 =  nn.LayerNorm(output_channels)\n",
    "#         self.layer_norm_s1 =  nn.LayerNorm(output_channels)\n",
    "#         self.layer_norm_i1 =  nn.LayerNorm(output_channels)\n",
    "        \n",
    "#         self.alpha_a1 = nn.Linear(output_channels, 1)\n",
    "#         self.alpha_s1 = nn.Linear(output_channels, 1)\n",
    "#         self.alpha_i1 = nn.Linear(output_channels, 1)\n",
    "#         self.w1 = nn.Linear(3, 3)\n",
    "        \n",
    "#         #self.reset_parameters()\n",
    "            \n",
    "#     def reset_parameters(self):\n",
    "        \n",
    "#         stdv = 1. / math.sqrt(self.I1.weight.size(1))\n",
    "#         std_att = 1. / math.sqrt(self.w1.weight.size(1))\n",
    "#         std_att_vec = 1. / math.sqrt( self.alpha_a1.weight.size(1))\n",
    "        \n",
    "#         self.I1.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "#         self.alpha_a1.weight.data.uniform_(-std_att, std_att)\n",
    "#         self.alpha_s1.weight.data.uniform_(-std_att, std_att)\n",
    "#         self.alpha_i1.weight.data.uniform_(-std_att, std_att)\n",
    "        \n",
    "#         self.w1.weight.data.uniform_(-std_att_vec, std_att_vec)\n",
    "        \n",
    "#         self.layer_norm_a1.reset_parameters()\n",
    "#         self.layer_norm_s1.reset_parameters()\n",
    "#         self.layer_norm_i1.reset_parameters()\n",
    "        \n",
    "\n",
    "#     def forward(self, x0, edge_index, edge_weight=None):\n",
    "#         a1 = F.relu(self.Aconv1(x0, edge_index, edge_weight))\n",
    "#         a1 = self.layer_norm_a1(a1)\n",
    "#         a1 = F.dropout(a1, p=self.p, training=self.training)\n",
    "        \n",
    "#         s1 = F.relu(self.Sconv1(x0, edge_index, edge_weight))\n",
    "#         s1 = self.layer_norm_s1(s1)\n",
    "#         s1 = F.dropout(s1, p=self.p, training=self.training)\n",
    "\n",
    "#         i1 = F.relu(self.I1(x0))\n",
    "#         i1 = self.layer_norm_i1(i1)\n",
    "#         i1 = F.dropout(i1, p=self.p, training=self.training)\n",
    "        \n",
    "#         ala1 = torch.sigmoid(self.alpha_a1(a1))\n",
    "#         als1 = torch.sigmoid(self.alpha_s1(s1))\n",
    "#         ali1 = torch.sigmoid(self.alpha_i1(i1))        \n",
    "#         alpha1 = F.softmax(self.w1(torch.cat([ala1, als1, ali1],dim=-1)/self.T), dim=1)        \n",
    "        \n",
    "#         x1 = torch.mm(torch.diag(alpha1[:,0]),a1) + torch.mm(torch.diag(alpha1[:,1]),s1) + torch.mm(torch.diag(alpha1[:,2]),i1)                \n",
    "        \n",
    "#         return x1\n",
    "        \n",
    "# class AGS_GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_features, num_classes, hidden_channels=16, dropout=0.2):\n",
    "#         super().__init__()        \n",
    "#         self.num_classes = num_classes\n",
    "#         self.p = dropout\n",
    "        \n",
    "#         self.ags_layer1 = AGS_layer(num_features, hidden_channels)\n",
    "#         self.ags_layer2 = AGS_layer(hidden_channels, hidden_channels)\n",
    "#         #self.ags_layer2 = AGS_layer(hidden_channels, num_classes)\n",
    "                \n",
    "#         self.CombineW = nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "#         self.PredW = nn.Linear(1*hidden_channels, num_classes)\n",
    "        \n",
    "    \n",
    "#     def forward(self, x0, edge_index, edge_weight=None):\n",
    "        \n",
    "#         #x0 = F.dropout(x0, p=self.p, training=self.training)\n",
    "#         x1 = self.ags_layer1(x0, edge_index, edge_weight)\n",
    "#         x1 = F.dropout(x1, p=self.p, training=self.training)\n",
    "        \n",
    "#         x2 = self.ags_layer2(x1, edge_index, edge_weight)\n",
    "#         x2 = F.dropout(x2, p=self.p, training=self.training)        \n",
    "        \n",
    "#         #x = self.PredW(torch.cat([x1, x2], dim=-1))\n",
    "#         x = self.PredW(x2)\n",
    "         \n",
    "#         #return x\n",
    "#         return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
