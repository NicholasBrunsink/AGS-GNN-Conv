{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if not os.getcwd().endswith(\"Submodular\"):\n",
    "    sys.path.append('../Submodular')    \n",
    "\n",
    "import DeviceDir\n",
    "\n",
    "DIR, RESULTS_DIR = DeviceDir.get_directory()\n",
    "device, NUM_PROCESSORS = DeviceDir.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Dataset import get_data\n",
    "from ipynb.fs.full.Dataset import datasets as available_datasets\n",
    "from ipynb.fs.full.Utils import save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#set default arguments here\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--log_info', type=bool, default=True)\n",
    "    parser.add_argument('--pbar', type=bool, default=False)\n",
    "    parser.add_argument('--batch_size', type=int, default=2048)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--dataset', type=str, default=\"Cora\", choices=available_datasets)\n",
    "    parser.add_argument('--use_normalization', action='store_false', default=True)\n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    return args, dict_args\n",
    "\n",
    "args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                #x = F.dropout(x, p=0.5, training=self.training)\n",
    "                x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, device, subgraph_loader):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "            pbar.set_description('Evaluating')\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "                \n",
    "                if args.log_info:\n",
    "                    pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, epochs=100, train_neighbors=[25,10]):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    #criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    \n",
    "    if args.log_info:\n",
    "        print(\"Train neighbors: \", train_neighbors)\n",
    "    \n",
    "    train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "    \n",
    "    \n",
    "    num_workers = 0 \n",
    "    if data.num_nodes>100000:\n",
    "        num_workers=8\n",
    "    \n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
    "                                   sizes=train_neighbors, batch_size=1024,\n",
    "                                   shuffle=True, num_workers=0)    \n",
    "    \n",
    "    subgraph_loader = NeighborSampler(data.edge_index, node_idx=None,\n",
    "                                      sizes=[25], batch_size=2048,                                      \n",
    "                                      shuffle=False, num_workers=0)\n",
    "    \n",
    "    \n",
    "    x, y = data.x.to(device), data.y.to(device)\n",
    "    data.train_mask.to(device)\n",
    "    data.val_mask.to(device)\n",
    "    data.test_mask.to(device)\n",
    "    \n",
    "    best_acc=0    \n",
    "    num_iteration = epochs\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar = tqdm(total=train_idx.size(0))\n",
    "            pbar.set_description(f'Epoch {epoch:02d}')\n",
    "        \n",
    "        total_loss = total_correct = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_size, n_id, adjs in train_loader:\n",
    "            adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x[n_id], adjs)\n",
    "            #loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "            loss = criterion(out, y[n_id[:batch_size]])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "            total_loss += float(loss)\n",
    "            total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "            \n",
    "            if args.log_info:\n",
    "                pbar.update(batch_size)\n",
    "        \n",
    "        if args.log_info:\n",
    "            pbar.close()\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        approx_acc = total_correct / train_idx.size(0)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Training Loss: {loss:.4f}, Training Accuracy: {approx_acc:.4f}')\n",
    "                \n",
    "        ####EVALUATION        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model.inference(x, device, subgraph_loader)\n",
    "\n",
    "        res = out.argmax(dim=-1) == data.y\n",
    "        train_acc = int(res[data.train_mask].sum()) / int(data.train_mask.sum())\n",
    "        val_acc = int(res[data.val_mask].sum()) / int(data.val_mask.sum())\n",
    "        test_acc = int(res[data.test_mask].sum()) / int(data.test_mask.sum())\n",
    "\n",
    "        if test_acc>best_acc:\n",
    "            best_acc=test_acc\n",
    "            \n",
    "        std_dev = np.std(train_losses[-5:])\n",
    "        \n",
    "        if args.log_info:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Std dev: {std_dev:.4f}')\n",
    "\n",
    "        if epoch>=5 and std_dev<=1e-3:\n",
    "            num_iteration = epoch\n",
    "            \n",
    "            if args.log_info:                \n",
    "                print(\"Iteration for convergence: \", epoch)\n",
    "            break\n",
    "                \n",
    "    return best_acc, num_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GSAGEperformance(data, dataset, num_classes, epochs=20, train_neighbors=[25,10]):\n",
    "    model = SAGE(dataset.num_features, num_classes, hidden_channels=256).to(device)        \n",
    "    if args.log_info: \n",
    "        print(model)    \n",
    "    \n",
    "    best_acc, num_iteration = train(model, data, epochs, train_neighbors)\n",
    "    return best_acc, num_iteration, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.log_info = True\n",
    "# DATASET_NAME = 'karate'\n",
    "# data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=True, split_no=0); print(\"\")\n",
    "# print(data)\n",
    "# best_acc, num_iteration, _ = GSAGEperformance(data, dataset, dataset.num_classes, epochs=25, train_neighbors=[25,10])\n",
    "# print(best_acc, num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_experiments(num_run=1):\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "#         \"Roman-empire\",\"Texas\",\"Squirrel\",\"Chameleon\",\n",
    "#         \"Cornell\",\"Actor\",\"Wisconsin\",\"Flickr\",\"Amazon-ratings\",\"reed98\",\"amherst41\",\"genius\",\n",
    "        #\"AmazonProducts\",\n",
    "#         \"cornell5\",\"penn94\",\n",
    "#         \"johnshopkins55\",\n",
    "#         #\"Yelp\",\n",
    "#         \"cora\",\"Tolokers\",\"Minesweeper\",\n",
    "#         \"CiteSeer\",\"Computers\",\"PubMed\",\"pubmed\",\n",
    "#         #\"Reddit\",\n",
    "#         \"cora_ml\",\"dblp\",\n",
    "#         #\"Reddit2\",\n",
    "#         \"Cora\",\"CS\",\"Photo\",\"Questions\",\"Physics\",\"citeseer\",\n",
    "#         \"Reddit\", #remove this later\n",
    "#         \"Reddit2\",#remove this later\n",
    "#         \"Yelp\", #remove this later\n",
    "#         \"AmazonProducts\",#remove this later\n",
    "#         'pokec','arxiv-year','snap-patents','twitch-gamer'\n",
    "    ]\n",
    "    \n",
    "    ALL_DATASETs= [\"Reddit0.525\",\"Reddit0.425\",\"Reddit0.325\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        \n",
    "        result_file = open(\"Results/GSAGE.txt\",'a+')        \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state=i)            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 150\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "            \n",
    "            accuracy, itr, _ =  GSAGEperformance(data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4])\n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)        \n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f}, itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()\n",
    "                \n",
    "# batch_experiments(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':    \n",
    "    None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reed98 acc 0.6218 sd 0.0314 itr 81 sd 22\n",
      "amherst41 acc 0.6559 sd 0.0114 itr 151 sd 84\n",
      "penn94 acc 0.7570 sd 0.0030 itr 83 sd 28\n",
      "cornell5 acc 0.7022 sd 0.0052 itr 92 sd 27\n",
      "Squirrel acc 0.3812 sd 0.0094 itr 175 sd 68\n",
      "johnshopkins55 acc 0.6612 sd 0.0044 itr 500 sd 0\n",
      "Chameleon acc 0.5118 sd 0.0161 itr 500 sd 0\n",
      "Tolokers acc 0.7912 sd 0.0029 itr 34 sd 21\n",
      "Flickr acc 0.5083 sd 0.0023 itr 198 sd 83\n",
      "Computers acc 0.9112 sd 0.0076 itr 500 sd 0\n",
      "Photo acc 0.9612 sd 0.0043 itr 401 sd 166\n",
      "Physics acc 0.9674 sd 0.0028 itr 41 sd 15\n"
     ]
    }
   ],
   "source": [
    "def ablation(num_run = 1):\n",
    "    \n",
    "#     ALL_DATASETs= [\n",
    "#         'Wisconsin',\n",
    "#         'reed98',        \n",
    "#         'Roman-empire',\n",
    "#         'Actor',\n",
    "#         'Minesweeper',        \n",
    "#         'Tolokers'\n",
    "#     ]\n",
    "    \n",
    "    ALL_DATASETs= [\n",
    "        \"reed98\",\n",
    "        \"amherst41\",\n",
    "        \"penn94\",\n",
    "        \"cornell5\",\n",
    "        \"Squirrel\",\n",
    "        \"johnshopkins55\",\n",
    "        \"Chameleon\",\n",
    "        \"Tolokers\",\n",
    "        \"Flickr\",\n",
    "        \n",
    "        \"Computers\",\n",
    "        \"Photo\",\n",
    "        \"Physics\",\n",
    "        \n",
    "#         \"AmazonProducts\",\n",
    "#         \"Yelp\",\n",
    "#         'pokec',\n",
    "#         'twitch-gamer',\n",
    "#         'wiki',        \n",
    "        \n",
    "#         \"Reddit\",\n",
    "#         \"Reddit2\",\n",
    "    ]\n",
    "    \n",
    "    #ALL_DATASETs= [\"karate\"]\n",
    "    \n",
    "    args.log_info = False\n",
    "    \n",
    "    for DATASET_NAME in ALL_DATASETs:  \n",
    "        \n",
    "        result_file = open(\"Results/GSAGEablation.txt\",'a+')        \n",
    "        print(DATASET_NAME, end=' ')\n",
    "        result_file.write(f'{DATASET_NAME} ')\n",
    "        \n",
    "        accs = []\n",
    "        itrs = []\n",
    "        \n",
    "        for i in range(num_run):\n",
    "            data, dataset = get_data(DATASET_NAME, DIR=None, log=False, h_score=False, split_no=i, random_state=i)            \n",
    "            if len(data.y.shape) > 1:\n",
    "                data.y = data.y.argmax(dim=1)        \n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            else:\n",
    "                num_classes = dataset.num_classes\n",
    "            \n",
    "            if num_classes!= torch.max(data.y)+1:\n",
    "                num_classes = torch.max(data.y).item()+1\n",
    "            \n",
    "            if data.num_nodes<100000:\n",
    "                max_epochs = 500\n",
    "            else:\n",
    "                max_epochs = 50\n",
    "            \n",
    "            accuracy, itr, _ =  GSAGEperformance(data, dataset, num_classes, epochs=max_epochs, train_neighbors=[8,4])\n",
    "            accs.append(accuracy)\n",
    "            itrs.append(itr)\n",
    "            #print(itr, accuracy)\n",
    "                        \n",
    "        #print(accs, itrs)        \n",
    "        print(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f} itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}')\n",
    "        result_file.write(f'acc {np.mean(accs):0.4f} sd {np.std(accs):0.4f}, itr {int(np.mean(itrs)):d} sd {int(np.std(itrs)):d}\\n')\n",
    "        result_file.close()            \n",
    "    \n",
    "    return \n",
    "\n",
    "ablation(num_run=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38cu11",
   "language": "python",
   "name": "py38cu11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
